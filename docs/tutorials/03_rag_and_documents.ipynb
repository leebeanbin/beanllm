{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“š RAG & Document Processing - ë¬¸ì„œ ê¸°ë°˜ ì§€ì‹ ì‘ë‹µ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ beanllmì˜ RAG (Retrieval-Augmented Generation) ê¸°ëŠ¥ê³¼ ë¬¸ì„œ ì²˜ë¦¬ ê¸°ëŠ¥ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“‹ ëª©ì°¨\n",
        "\n",
        "- [1. ë¬¸ì„œ ë¡œë”©](#1-ë¬¸ì„œ-ë¡œë”©)\n",
        "- [2. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)](#2-í…ìŠ¤íŠ¸-ë¶„í• -chunking)\n",
        "- [3. RAGChain ê¸°ë³¸ ì‚¬ìš©](#3-ragchain-ê¸°ë³¸-ì‚¬ìš©)\n",
        "- [4. ê³ ê¸‰ RAG ê¸°ëŠ¥](#4-ê³ ê¸‰-rag-ê¸°ëŠ¥)\n",
        "- [5. ë²¡í„° ìŠ¤í† ì–´ ì„ íƒ](#5-ë²¡í„°-ìŠ¤í† ì–´-ì„ íƒ)\n",
        "- [6. ì‹¤ì „ ì˜ˆì œ](#6-ì‹¤ì „-ì˜ˆì œ)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•˜ë©´:\n",
        "- âœ… ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… í…ìŠ¤íŠ¸ë¥¼ ì ì ˆíˆ ë¶„í• í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… RAGChainì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ë°°ì¹˜ ì¿¼ë¦¬ì™€ ìŠ¤íŠ¸ë¦¬ë°ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ë‹¤ì–‘í•œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "- [01_setup_and_installation.ipynb](01_setup_and_installation.ipynb) ì™„ë£Œ\n",
        "- [02_core_client.ipynb](02_core_client.ipynb) ì™„ë£Œ\n",
        "- ìµœì†Œ í•˜ë‚˜ì˜ LLM Provider API í‚¤ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ë¬¸ì„œ ë¡œë”©\n",
        "\n",
        "beanllmì€ ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¡œë“œí•˜ë©´ `Document` ê°ì²´ë¡œ ë³€í™˜ë˜ì–´, ì´í›„ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **Document ê°ì²´**:\n",
        "> - `content`: ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ë‚´ìš©\n",
        "> - `metadata`: ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° (ì†ŒìŠ¤, í˜ì´ì§€ ë²ˆí˜¸ ë“±)\n",
        "> - ë‹¤ì–‘í•œ ë¡œë”ë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤\n",
        "\n",
        "### 1.1 í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©\n",
        "\n",
        "ê°€ì¥ ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©ì…ë‹ˆë‹¤. `.txt` íŒŒì¼ì„ ì½ì–´ì„œ `Document` ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©ì˜ íŠ¹ì§•**:\n",
        "> - ê°€ì¥ ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ë°©ë²•\n",
        "> - ì¸ì½”ë”© ìë™ ê°ì§€\n",
        "> - ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ê²½ë¡œ ìë™ ì €ì¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import TextLoader, Document\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================\n",
        "# í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ë°©ë²• 1: TextLoader ì‚¬ìš©\n",
        "print(\"\\n[ë°©ë²• 1] TextLoader ì‚¬ìš©:\")\n",
        "# loader = TextLoader(\"sample.txt\")\n",
        "# documents = loader.load()\n",
        "#\n",
        "# print(f\"  âœ… ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
        "# for i, doc in enumerate(documents, 1):\n",
        "#     print(f\"\\n  ğŸ“„ ë¬¸ì„œ {i}:\")\n",
        "#     print(f\"    - ë‚´ìš© ê¸¸ì´: {len(doc.content)}ì\")\n",
        "#     print(f\"    - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.content[:100]}...\")\n",
        "#     print(f\"    - ë©”íƒ€ë°ì´í„°: {doc.metadata}\")\n",
        "\n",
        "# ë°©ë²• 2: ì§ì ‘ Document ìƒì„±\n",
        "print(\"\\n[ë°©ë²• 2] ì§ì ‘ Document ìƒì„±:\")\n",
        "doc = Document(\n",
        "    content=\"Pythonì€ 1991ë…„ì— ë°œí‘œëœ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤. \"\n",
        "    \"ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤. \"\n",
        "    \"ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ë©°, íŠ¹íˆ ë°ì´í„° ê³¼í•™ê³¼ ì›¹ ê°œë°œì— ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.\",\n",
        "    metadata={\n",
        "        \"source\": \"manual\",\n",
        "        \"type\": \"introduction\",\n",
        "        \"language\": \"ko\",\n",
        "        \"created_at\": \"2025-01-01\",\n",
        "    },\n",
        ")\n",
        "\n",
        "print(\"  âœ… Document ìƒì„± ì™„ë£Œ!\")\n",
        "print(f\"  ğŸ“„ ë‚´ìš©: {doc.content}\")\n",
        "print(f\"  ğŸ“Š ë©”íƒ€ë°ì´í„°: {doc.metadata}\")\n",
        "\n",
        "# ë°©ë²• 3: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ íŒŒì¼ ì¼ê´„ ë¡œë”©\n",
        "print(\"\\n[ë°©ë²• 3] ì—¬ëŸ¬ í…ìŠ¤íŠ¸ íŒŒì¼ ì¼ê´„ ë¡œë”©:\")\n",
        "# text_files = [\"file1.txt\", \"file2.txt\", \"file3.txt\"]\n",
        "# all_documents = []\n",
        "#\n",
        "# for file_path in text_files:\n",
        "#     try:\n",
        "#         loader = TextLoader(file_path)\n",
        "#         docs = loader.load()\n",
        "#         all_documents.extend(docs)\n",
        "#         print(f\"  âœ… {file_path}: {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"  âŒ {file_path}: ë¡œë“œ ì‹¤íŒ¨ - {e}\")\n",
        "#\n",
        "# print(f\"\\n  ğŸ“Š ì´ ë¡œë“œëœ ë¬¸ì„œ: {len(all_documents)}ê°œ\")\n",
        "\n",
        "print(\"\\nğŸ’¡ í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© íŒ:\")\n",
        "print(\"  - ì¸ì½”ë”© ë¬¸ì œ ì‹œ encoding íŒŒë¼ë¯¸í„° ì§€ì •\")\n",
        "print(\"  - í° íŒŒì¼ì€ ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ ê³ ë ¤\")\n",
        "print(\"  - ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ ì •ë³´ ìë™ ì €ì¥\")\n",
        "print(\"  - ì—¬ëŸ¬ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ Document ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•© ê°€ëŠ¥\")\n",
        "\n",
        "# ============================================\n",
        "# Document ê°ì²´ í™œìš©\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Document ê°ì²´ í™œìš©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Document ì†ì„± í™•ì¸\n",
        "print(\"\\nğŸ“Š Document ê°ì²´ êµ¬ì¡°:\")\n",
        "print(f\"  - content: {type(doc.content).__name__} (ê¸¸ì´: {len(doc.content)}ì)\")\n",
        "print(f\"  - metadata: {type(doc.metadata).__name__} (í‚¤: {list(doc.metadata.keys())})\")\n",
        "\n",
        "# ë©”íƒ€ë°ì´í„° í™œìš©\n",
        "print(\"\\nğŸ’¡ ë©”íƒ€ë°ì´í„° í™œìš© ì˜ˆì‹œ:\")\n",
        "print(\"  - ë¬¸ì„œ ì¶œì²˜ ì¶”ì \")\n",
        "print(\"  - ë¬¸ì„œ í•„í„°ë§ (ì˜ˆ: íŠ¹ì • íƒ€ì…ë§Œ ì„ íƒ)\")\n",
        "print(\"  - ë¬¸ì„œ ê·¸ë£¹í™” (ì˜ˆ: ì†ŒìŠ¤ë³„ ê·¸ë£¹)\")\n",
        "print(\"  - ê²€ìƒ‰ ê²°ê³¼ì— ì¶œì²˜ í‘œì‹œ\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 PDF íŒŒì¼ ë¡œë”©\n",
        "\n",
        "PDF íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. beanllmì€ ì—¬ëŸ¬ PDF ì²˜ë¦¬ ì—”ì§„ì„ ì§€ì›í•˜ì—¬ ë‹¤ì–‘í•œ PDF í˜•ì‹ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **PDF ë¡œë”©ì˜ íŠ¹ì§•**:\n",
        "> - í…ìŠ¤íŠ¸ ì¶”ì¶œ: PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ Documentë¡œ ë³€í™˜\n",
        "> - í˜ì´ì§€ ë‹¨ìœ„ ì²˜ë¦¬: ê° í˜ì´ì§€ë¥¼ ë³„ë„ì˜ Documentë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "> - ë©”íƒ€ë°ì´í„° ë³´ì¡´: í˜ì´ì§€ ë²ˆí˜¸, íŒŒì¼ ê²½ë¡œ ë“± ë©”íƒ€ë°ì´í„° ìë™ ì €ì¥\n",
        "> - ì´ë¯¸ì§€ ë° í‘œ ì²˜ë¦¬: ê³ ê¸‰ PDF ì²˜ë¦¬ ì—”ì§„ ì‚¬ìš© ì‹œ ì´ë¯¸ì§€ì™€ í‘œë„ ì¶”ì¶œ ê°€ëŠ¥\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜ì‚¬í•­**:\n",
        "> - ìŠ¤ìº”ëœ PDFëŠ” OCRì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "> - ë³µì¡í•œ ë ˆì´ì•„ì›ƒì˜ PDFëŠ” ì¶”ê°€ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import PDFLoader\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================\n",
        "# PDF íŒŒì¼ ë¡œë”©\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PDF íŒŒì¼ ë¡œë”©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ë°©ë²• 1: ê¸°ë³¸ PDFLoader ì‚¬ìš©\n",
        "print(\"\\n[ë°©ë²• 1] ê¸°ë³¸ PDFLoader ì‚¬ìš©:\")\n",
        "# loader = PDFLoader(\"document.pdf\")\n",
        "# documents = loader.load()\n",
        "#\n",
        "# print(f\"  âœ… PDF ë¬¸ì„œ ë¡œë“œ: {len(documents)}ê°œ í˜ì´ì§€\")\n",
        "# for i, doc in enumerate(documents[:3], 1):  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ\n",
        "#     print(f\"\\n  ğŸ“„ í˜ì´ì§€ {i}:\")\n",
        "#     print(f\"    - ë‚´ìš© ê¸¸ì´: {len(doc.content)}ì\")\n",
        "#     print(f\"    - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.content[:150]}...\")\n",
        "#     print(f\"    - ë©”íƒ€ë°ì´í„°: {doc.metadata}\")\n",
        "\n",
        "# ë°©ë²• 2: í˜ì´ì§€ ë²”ìœ„ ì§€ì •\n",
        "print(\"\\n[ë°©ë²• 2] í˜ì´ì§€ ë²”ìœ„ ì§€ì •:\")\n",
        "# loader = PDFLoader(\"document.pdf\", page_range=(1, 5))  # 1-5í˜ì´ì§€ë§Œ\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ í˜ì´ì§€: {len(documents)}ê°œ\")\n",
        "\n",
        "# ë°©ë²• 3: ê³ ê¸‰ PDF ì²˜ë¦¬ (beanPDFLoader)\n",
        "print(\"\\n[ë°©ë²• 3] ê³ ê¸‰ PDF ì²˜ë¦¬ (beanPDFLoader):\")\n",
        "print(\"  from beanllm.domain.pdf import beanPDFLoader\")\n",
        "print(\"  \")\n",
        "print(\"  loader = beanPDFLoader(\")\n",
        "print(\"      source='document.pdf',\")\n",
        "print(\"      extract_images=True,  # ì´ë¯¸ì§€ ì¶”ì¶œ\")\n",
        "print(\"      extract_tables=True,  # í‘œ ì¶”ì¶œ\")\n",
        "print(\"      ocr_enabled=True      # OCR í™œì„±í™”\")\n",
        "print(\"  )\")\n",
        "print(\"  documents = loader.load()\")\n",
        "\n",
        "print(\"\\nğŸ’¡ PDF ë¡œë”© ì˜µì…˜:\")\n",
        "print(\"  - page_range: íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë§Œ ë¡œë“œ\")\n",
        "print(\"  - extract_images: ì´ë¯¸ì§€ ì¶”ì¶œ ì—¬ë¶€\")\n",
        "print(\"  - extract_tables: í‘œ ì¶”ì¶œ ì—¬ë¶€\")\n",
        "print(\"  - ocr_enabled: OCR í™œì„±í™” (ìŠ¤ìº”ëœ PDFìš©)\")\n",
        "\n",
        "# ============================================\n",
        "# PDF ë©”íƒ€ë°ì´í„° í™œìš©\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PDF ë©”íƒ€ë°ì´í„° í™œìš©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# PDF ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ì˜ˆì‹œ\n",
        "print(\"\\nğŸ“Š PDF Document ë©”íƒ€ë°ì´í„° êµ¬ì¡°:\")\n",
        "pdf_metadata_example = {\n",
        "    \"source\": \"document.pdf\",\n",
        "    \"page\": 1,\n",
        "    \"total_pages\": 10,\n",
        "    \"file_size\": 1024000,\n",
        "    \"created_at\": \"2025-01-01\",\n",
        "    \"author\": \"ì‘ì„±ì ì´ë¦„\",\n",
        "}\n",
        "\n",
        "print(\"  ì˜ˆì‹œ ë©”íƒ€ë°ì´í„°:\")\n",
        "for key, value in pdf_metadata_example.items():\n",
        "    print(f\"    - {key}: {value}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ë©”íƒ€ë°ì´í„° í™œìš©:\")\n",
        "print(\"  - í˜ì´ì§€ ë²ˆí˜¸ë¡œ íŠ¹ì • í˜ì´ì§€ ì°¸ì¡°\")\n",
        "print(\"  - ì¶œì²˜ ì¶”ì  (ì–´ë–¤ PDFì˜ ëª‡ í˜ì´ì§€ì¸ì§€)\")\n",
        "print(\"  - ë¬¸ì„œ í•„í„°ë§ (ì˜ˆ: íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë§Œ ì‚¬ìš©)\")\n",
        "print(\"  - ê²€ìƒ‰ ê²°ê³¼ì— í˜ì´ì§€ ë²ˆí˜¸ í‘œì‹œ\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… PDF íŒŒì¼ ë¡œë”© í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 ë””ë ‰í† ë¦¬ ë¡œë”©\n",
        "\n",
        "ë””ë ‰í† ë¦¬ ë‚´ì˜ ëª¨ë“  íŒŒì¼ì„ í•œ ë²ˆì— ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ë„ ì§€ì›í•˜ì—¬ ëŒ€ëŸ‰ì˜ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ë””ë ‰í† ë¦¬ ë¡œë”©ì˜ íŠ¹ì§•**:\n",
        "> - ì¬ê·€ì  ê²€ìƒ‰: í•˜ìœ„ ë””ë ‰í† ë¦¬ê¹Œì§€ ìë™ìœ¼ë¡œ ê²€ìƒ‰\n",
        "> - íŒŒì¼ íƒ€ì… í•„í„°ë§: glob íŒ¨í„´ìœ¼ë¡œ íŠ¹ì • íŒŒì¼ë§Œ ë¡œë“œ\n",
        "> - ë³‘ë ¬ ì²˜ë¦¬: ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ ì†ë„ í–¥ìƒ\n",
        "> - ë©”íƒ€ë°ì´í„° ìë™ ì €ì¥: íŒŒì¼ ê²½ë¡œ, íƒ€ì… ë“± ìë™ìœ¼ë¡œ ë©”íƒ€ë°ì´í„°ì— ì €ì¥\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜ì‚¬í•­**:\n",
        "> - í° ë””ë ‰í† ë¦¬ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "> - ë³‘ë ¬ ì²˜ë¦¬ëŠ” CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ì¡°ì •í•˜ì„¸ìš”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import DirectoryLoader\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================\n",
        "# ë””ë ‰í† ë¦¬ ë¡œë”©\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ë””ë ‰í† ë¦¬ ë¡œë”©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ë°©ë²• 1: ê¸°ë³¸ ë””ë ‰í† ë¦¬ ë¡œë”©\n",
        "print(\"\\n[ë°©ë²• 1] ê¸°ë³¸ ë””ë ‰í† ë¦¬ ë¡œë”©:\")\n",
        "# loader = DirectoryLoader(\n",
        "#     path=\"./docs\",\n",
        "#     glob=\"**/*.txt\",  # ëª¨ë“  .txt íŒŒì¼\n",
        "#     recursive=True,   # ì¬ê·€ì  ê²€ìƒ‰\n",
        "# )\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ\")\n",
        "\n",
        "# ë°©ë²• 2: ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”\n",
        "print(\"\\n[ë°©ë²• 2] ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”:\")\n",
        "# loader = DirectoryLoader(\n",
        "#     path=\"./docs\",\n",
        "#     glob=\"**/*.{txt,md,pdf}\",  # ì—¬ëŸ¬ íŒŒì¼ íƒ€ì…\n",
        "#     recursive=True,\n",
        "#     use_parallel=True,  # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”\n",
        "#     max_workers=4,      # ì›Œì»¤ ìˆ˜ (CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ì¡°ì •)\n",
        "# )\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ (ë³‘ë ¬ ì²˜ë¦¬)\")\n",
        "\n",
        "# ë°©ë²• 3: íŒŒì¼ íƒ€ì…ë³„ í•„í„°ë§\n",
        "print(\"\\n[ë°©ë²• 3] íŒŒì¼ íƒ€ì…ë³„ í•„í„°ë§:\")\n",
        "# # í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ\n",
        "# text_loader = DirectoryLoader(\n",
        "#     path=\"./docs\",\n",
        "#     glob=\"**/*.txt\",\n",
        "#     recursive=True\n",
        "# )\n",
        "# text_docs = text_loader.load()\n",
        "#\n",
        "# # PDF íŒŒì¼ë§Œ\n",
        "# pdf_loader = DirectoryLoader(\n",
        "#     path=\"./docs\",\n",
        "#     glob=\"**/*.pdf\",\n",
        "#     recursive=True\n",
        "# )\n",
        "# pdf_docs = pdf_loader.load()\n",
        "#\n",
        "# print(f\"  âœ… í…ìŠ¤íŠ¸ íŒŒì¼: {len(text_docs)}ê°œ\")\n",
        "# print(f\"  âœ… PDF íŒŒì¼: {len(pdf_docs)}ê°œ\")\n",
        "\n",
        "# ë°©ë²• 4: ì œì™¸ íŒ¨í„´ ì‚¬ìš©\n",
        "print(\"\\n[ë°©ë²• 4] ì œì™¸ íŒ¨í„´ ì‚¬ìš©:\")\n",
        "# loader = DirectoryLoader(\n",
        "#     path=\"./docs\",\n",
        "#     glob=\"**/*\",\n",
        "#     recursive=True,\n",
        "#     exclude=[\"**/node_modules/**\", \"**/.git/**\", \"**/__pycache__/**\"]  # ì œì™¸í•  íŒ¨í„´\n",
        "# )\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ (ì œì™¸ íŒ¨í„´ ì ìš©)\")\n",
        "\n",
        "print(\"\\nğŸ’¡ DirectoryLoader ì˜µì…˜:\")\n",
        "print(\"  - path: ë¡œë“œí•  ë””ë ‰í† ë¦¬ ê²½ë¡œ\")\n",
        "print(\"  - glob: íŒŒì¼ íŒ¨í„´ (ì˜ˆ: '**/*.txt', '**/*.{txt,md}')\")\n",
        "print(\"  - recursive: í•˜ìœ„ ë””ë ‰í† ë¦¬ ê²€ìƒ‰ ì—¬ë¶€\")\n",
        "print(\"  - use_parallel: ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”\")\n",
        "print(\"  - max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜\")\n",
        "print(\"  - exclude: ì œì™¸í•  íŒŒì¼ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸\")\n",
        "\n",
        "# ============================================\n",
        "# ë””ë ‰í† ë¦¬ ë¡œë”© ê²°ê³¼ ë¶„ì„\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ë””ë ‰í† ë¦¬ ë¡œë”© ê²°ê³¼ ë¶„ì„\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ (ì£¼ì„ í•´ì œ)\n",
        "# documents = DirectoryLoader(\"./docs\", glob=\"**/*\", recursive=True).load()\n",
        "#\n",
        "# print(f\"\\nğŸ“Š ë¡œë”© í†µê³„:\")\n",
        "# print(f\"  - ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ\")\n",
        "#\n",
        "# # íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜\n",
        "# file_types = {}\n",
        "# for doc in documents:\n",
        "#     source = doc.metadata.get(\"source\", \"unknown\")\n",
        "#     file_ext = Path(source).suffix.lower() if source != \"unknown\" else \"unknown\"\n",
        "#     file_types[file_ext] = file_types.get(file_ext, 0) + 1\n",
        "#\n",
        "# print(f\"\\nğŸ“ íŒŒì¼ íƒ€ì…ë³„ ë¶„ë¥˜:\")\n",
        "# for file_type, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):\n",
        "#     print(f\"  - {file_type}: {count}ê°œ\")\n",
        "#\n",
        "# # ë””ë ‰í† ë¦¬ë³„ ë¶„ë¥˜\n",
        "# directories = {}\n",
        "# for doc in documents:\n",
        "#     source = doc.metadata.get(\"source\", \"unknown\")\n",
        "#     if source != \"unknown\":\n",
        "#         dir_path = str(Path(source).parent)\n",
        "#         directories[dir_path] = directories.get(dir_path, 0) + 1\n",
        "#\n",
        "# print(f\"\\nğŸ“‚ ë””ë ‰í† ë¦¬ë³„ ë¶„ë¥˜ (ìƒìœ„ 5ê°œ):\")\n",
        "# for dir_path, count in sorted(directories.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
        "#     print(f\"  - {dir_path}: {count}ê°œ\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ë””ë ‰í† ë¦¬ ë¡œë”© í™œìš©:\")\n",
        "print(\"  - ëŒ€ëŸ‰ ë¬¸ì„œ ì¼ê´„ ì²˜ë¦¬\")\n",
        "print(\"  - ë¬¸ì„œ ì»¬ë ‰ì…˜ êµ¬ì¶•\")\n",
        "print(\"  - RAG ì‹œìŠ¤í…œ ë°ì´í„° ì¤€ë¹„\")\n",
        "print(\"  - ë¬¸ì„œ ë¶„ì„ ë° í†µê³„\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ë””ë ‰í† ë¦¬ ë¡œë”© í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹\n",
        "\n",
        "beanllmì€ ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤. ê° í˜•ì‹ì— ë§ëŠ” ë¡œë”ë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  Document ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ì§€ì›í•˜ëŠ” ë¬¸ì„œ í˜•ì‹**:\n",
        "> - **í…ìŠ¤íŠ¸ íŒŒì¼**: .txt, .md, .rst ë“±\n",
        "> - **PDF**: .pdf (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ ì¶”ì¶œ ê°€ëŠ¥)\n",
        "> - **CSV**: .csv (í…Œì´ë¸” ë°ì´í„°)\n",
        "> - **HTML**: .html, .htm (ì›¹ í˜ì´ì§€)\n",
        "> - **Markdown**: .md, .markdown\n",
        "> - **Jupyter Notebook**: .ipynb\n",
        "> - **Office ë¬¸ì„œ**: .docx, .xlsx, .pptx (ê³ ê¸‰ ë¡œë” ì‚¬ìš© ì‹œ)\n",
        "> - **ì´ë¯¸ì§€**: .jpg, .png ë“± (OCR ë˜ëŠ” Vision ëª¨ë¸ ì‚¬ìš© ì‹œ)\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜ì‚¬í•­**:\n",
        "> - ì¼ë¶€ í˜•ì‹ì€ ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "> - ë³µì¡í•œ ë ˆì´ì•„ì›ƒì˜ ë¬¸ì„œëŠ” ì¶”ê°€ ì²˜ë¦¬ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import TextLoader, PDFLoader, CSVLoader, HTMLLoader, MarkdownLoader, Document\n",
        "\n",
        "# ============================================\n",
        "# ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ë¡œë”©\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ë¡œë”©\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ì§€ì›í•˜ëŠ” ë¬¸ì„œ í˜•ì‹ ë° ë¡œë”\n",
        "supported_formats = {\n",
        "    \"í…ìŠ¤íŠ¸\": {\n",
        "        \"extensions\": [\".txt\", \".text\"],\n",
        "        \"loader\": \"TextLoader\",\n",
        "        \"description\": \"ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼\",\n",
        "        \"example\": 'TextLoader(\"file.txt\").load()',\n",
        "    },\n",
        "    \"PDF\": {\n",
        "        \"extensions\": [\".pdf\"],\n",
        "        \"loader\": \"PDFLoader, beanPDFLoader\",\n",
        "        \"description\": \"PDF ë¬¸ì„œ (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, í‘œ ì¶”ì¶œ ê°€ëŠ¥)\",\n",
        "        \"example\": 'PDFLoader(\"file.pdf\").load()',\n",
        "    },\n",
        "    \"CSV\": {\n",
        "        \"extensions\": [\".csv\"],\n",
        "        \"loader\": \"CSVLoader\",\n",
        "        \"description\": \"CSV í…Œì´ë¸” ë°ì´í„°\",\n",
        "        \"example\": 'CSVLoader(\"data.csv\").load()',\n",
        "    },\n",
        "    \"HTML\": {\n",
        "        \"extensions\": [\".html\", \".htm\"],\n",
        "        \"loader\": \"HTMLLoader\",\n",
        "        \"description\": \"HTML ì›¹ í˜ì´ì§€\",\n",
        "        \"example\": 'HTMLLoader(\"page.html\").load()',\n",
        "    },\n",
        "    \"Markdown\": {\n",
        "        \"extensions\": [\".md\", \".markdown\"],\n",
        "        \"loader\": \"MarkdownLoader\",\n",
        "        \"description\": \"Markdown ë¬¸ì„œ\",\n",
        "        \"example\": 'MarkdownLoader(\"readme.md\").load()',\n",
        "    },\n",
        "    \"Jupyter Notebook\": {\n",
        "        \"extensions\": [\".ipynb\"],\n",
        "        \"loader\": \"JupyterLoader\",\n",
        "        \"description\": \"Jupyter ë…¸íŠ¸ë¶ (ì½”ë“œ ë° ë§ˆí¬ë‹¤ìš´)\",\n",
        "        \"example\": 'JupyterLoader(\"notebook.ipynb\").load()',\n",
        "    },\n",
        "    \"Office ë¬¸ì„œ\": {\n",
        "        \"extensions\": [\".docx\", \".xlsx\", \".pptx\"],\n",
        "        \"loader\": \"DoclingLoader (ê³ ê¸‰)\",\n",
        "        \"description\": \"Word, Excel, PowerPoint (ê³ ê¸‰ ë¡œë” í•„ìš”)\",\n",
        "        \"example\": 'DoclingLoader(\"document.docx\").load()',\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“š ì§€ì›í•˜ëŠ” ë¬¸ì„œ í˜•ì‹:\")\n",
        "for format_name, info in supported_formats.items():\n",
        "    print(f\"\\n[{format_name}]\")\n",
        "    print(f\"  í™•ì¥ì: {', '.join(info['extensions'])}\")\n",
        "    print(f\"  ë¡œë”: {info['loader']}\")\n",
        "    print(f\"  ì„¤ëª…: {info['description']}\")\n",
        "    print(f\"  ì˜ˆì œ: {info['example']}\")\n",
        "\n",
        "# ============================================\n",
        "# ì‹¤ì œ ë¡œë”© ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ì‹¤ì œ ë¡œë”© ì˜ˆì œ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# CSV íŒŒì¼ ë¡œë”© ì˜ˆì œ\n",
        "print(\"\\n[ì˜ˆì œ 1] CSV íŒŒì¼ ë¡œë”©:\")\n",
        "# loader = CSVLoader(\"data.csv\")\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ í–‰ ìˆ˜: {len(documents)}ê°œ\")\n",
        "# for i, doc in enumerate(documents[:3], 1):\n",
        "#     print(f\"  í–‰ {i}: {doc.content[:100]}...\")\n",
        "\n",
        "# HTML íŒŒì¼ ë¡œë”© ì˜ˆì œ\n",
        "print(\"\\n[ì˜ˆì œ 2] HTML íŒŒì¼ ë¡œë”©:\")\n",
        "# loader = HTMLLoader(\"page.html\")\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ\")\n",
        "# for doc in documents:\n",
        "#     print(f\"  ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.content[:150]}...\")\n",
        "\n",
        "# Markdown íŒŒì¼ ë¡œë”© ì˜ˆì œ\n",
        "print(\"\\n[ì˜ˆì œ 3] Markdown íŒŒì¼ ë¡œë”©:\")\n",
        "# loader = MarkdownLoader(\"readme.md\")\n",
        "# documents = loader.load()\n",
        "# print(f\"  âœ… ë¡œë“œëœ ë¬¸ì„œ: {len(documents)}ê°œ\")\n",
        "# print(f\"  ë‚´ìš©: {documents[0].content[:200]}...\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ë¬¸ì„œ í˜•ì‹ë³„ íŠ¹ì§•:\")\n",
        "print(\"  - í…ìŠ¤íŠ¸: ê°€ì¥ ê°„ë‹¨í•˜ê³  ë¹ ë¦„\")\n",
        "print(\"  - PDF: ë ˆì´ì•„ì›ƒ ë³´ì¡´, ì´ë¯¸ì§€/í‘œ ì¶”ì¶œ ê°€ëŠ¥\")\n",
        "print(\"  - CSV: í…Œì´ë¸” ë°ì´í„° êµ¬ì¡°í™”\")\n",
        "print(\"  - HTML: ì›¹ ì½˜í…ì¸  ì¶”ì¶œ\")\n",
        "print(\"  - Markdown: êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸\")\n",
        "print(\"  - Jupyter: ì½”ë“œì™€ ì„¤ëª… ë¶„ë¦¬\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ë¡œë”© í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)\n",
        "\n",
        "ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì€ RAGì˜ í•µì‹¬ì…ë‹ˆë‹¤. ì ì ˆí•œ ì²­í¬ í¬ê¸°ì™€ ì˜¤ë²„ë©ì„ ì„¤ì •í•˜ë©´ ê²€ìƒ‰ ì •í™•ë„ì™€ ì‘ë‹µ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ì²­í‚¹ì˜ ì¤‘ìš”ì„±**:\n",
        "> - **ë„ˆë¬´ ì‘ìœ¼ë©´**: ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤, ì˜ë¯¸ íŒŒí¸í™”\n",
        "> - **ë„ˆë¬´ í¬ë©´**: ê²€ìƒ‰ ì •í™•ë„ ì €í•˜, ë¶ˆí•„ìš”í•œ ì •ë³´ í¬í•¨\n",
        "> - **ì ì ˆí•œ í¬ê¸°**: ì¼ë°˜ì ìœ¼ë¡œ 200-1000ì ë˜ëŠ” 100-500 í† í°\n",
        "> - **ì˜¤ë²„ë©**: ë¬¸ë§¥ ìœ ì§€, ê²½ê³„ì—ì„œ ì •ë³´ ì†ì‹¤ ë°©ì§€ (ì¼ë°˜ì ìœ¼ë¡œ 10-20%)\n",
        "\n",
        "> ğŸ’¡ **ì²­í‚¹ ì „ëµ**:\n",
        "> - **ë¬¸ì¥ ë‹¨ìœ„**: ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ì˜ë¯¸ ë³´ì¡´)\n",
        "> - **ë‹¨ë½ ë‹¨ìœ„**: ë‹¨ë½ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ë…¼ë¦¬ì  ë‹¨ìœ„)\n",
        "> - **í† í° ë‹¨ìœ„**: í† í° ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ëª¨ë¸ ì œí•œ ê³ ë ¤)\n",
        "> - **êµ¬ì¡° ê¸°ë°˜**: ë§ˆí¬ë‹¤ìš´ í—¤ë”, HTML íƒœê·¸ ë“± êµ¬ì¡°ë¥¼ í™œìš©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter,\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    Document\n",
        ")\n",
        "\n",
        "# ============================================\n",
        "# ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„í• \n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"í…ìŠ¤íŠ¸ ë¶„í•  (Chunking)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ê¸´ í…ìŠ¤íŠ¸ ì˜ˆì œ\n",
        "long_text = \"\"\"\n",
        "Pythonì€ 1991ë…„ì— ë°œí‘œëœ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
        "ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\n",
        "\n",
        "Pythonì˜ ì£¼ìš” íŠ¹ì§•:\n",
        "1. ê°„ê²°í•œ ë¬¸ë²•: ì½”ë“œê°€ ì½ê¸° ì‰½ê³  ì‘ì„±í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\n",
        "2. ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬: ìˆ˜ë§ì€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì„œë“œíŒŒí‹° íŒ¨í‚¤ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "3. í¬ë¡œìŠ¤ í”Œë«í¼: Windows, macOS, Linux ë“± ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
        "4. ë™ì  íƒ€ì…: ë³€ìˆ˜ íƒ€ì…ì„ ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
        "5. ê°ì²´ì§€í–¥: í´ë˜ìŠ¤ì™€ ê°ì²´ë¥¼ ì‚¬ìš©í•œ í”„ë¡œê·¸ë˜ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
        "\n",
        "Pythonì€ ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ, ìë™í™”, ì¸ê³µì§€ëŠ¥ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "íŠ¹íˆ ë°ì´í„° ë¶„ì„ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì•¼ì—ì„œ ë§¤ìš° ì¸ê¸° ìˆëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"\\nğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(long_text)}ì\")\n",
        "\n",
        "# ë°©ë²• 1: RecursiveCharacterTextSplitter (ê¶Œì¥)\n",
        "print(\"\\n[ë°©ë²• 1] RecursiveCharacterTextSplitter (ê¶Œì¥):\")\n",
        "splitter1 = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,      # ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)\n",
        "    chunk_overlap=50,    # ì˜¤ë²„ë© (ë¬¸ì ìˆ˜)\n",
        "    length_function=len, # ê¸¸ì´ ì¸¡ì • í•¨ìˆ˜\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # ë¶„í•  ìš°ì„ ìˆœìœ„\n",
        ")\n",
        "\n",
        "# chunks = splitter1.split_text(long_text)\n",
        "# print(f\"  âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
        "# for i, chunk in enumerate(chunks, 1):\n",
        "#     print(f\"\\n  ì²­í¬ {i} ({len(chunk)}ì):\")\n",
        "#     print(f\"    {chunk[:100]}...\")\n",
        "\n",
        "print(\"\\nğŸ’¡ RecursiveCharacterTextSplitter íŠ¹ì§•:\")\n",
        "print(\"  - ë¬¸ë‹¨, ë¬¸ì¥, ë‹¨ì–´ ìˆœì„œë¡œ ë¶„í•  ì‹œë„\")\n",
        "print(\"  - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´\")\n",
        "print(\"  - ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•\")\n",
        "\n",
        "# ë°©ë²• 2: CharacterTextSplitter (ê°„ë‹¨í•œ ë¶„í• )\n",
        "print(\"\\n[ë°©ë²• 2] CharacterTextSplitter (ê°„ë‹¨í•œ ë¶„í• ):\")\n",
        "splitter2 = CharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=50,\n",
        "    separator=\"\\n\"  # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„í• \n",
        ")\n",
        "\n",
        "# chunks = splitter2.split_text(long_text)\n",
        "# print(f\"  âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
        "\n",
        "print(\"\\nğŸ’¡ CharacterTextSplitter íŠ¹ì§•:\")\n",
        "print(\"  - ì§€ì •ëœ êµ¬ë¶„ìë¡œ ë‹¨ìˆœ ë¶„í• \")\n",
        "print(\"  - ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„\")\n",
        "print(\"  - ì˜ë¯¸ ë³´ì¡´ì€ ë‚®ìŒ\")\n",
        "\n",
        "# ë°©ë²• 3: TokenTextSplitter (í† í° ë‹¨ìœ„ ë¶„í• )\n",
        "print(\"\\n[ë°©ë²• 3] TokenTextSplitter (í† í° ë‹¨ìœ„ ë¶„í• ):\")\n",
        "splitter3 = TokenTextSplitter(\n",
        "    chunk_size=100,      # ì²­í¬ í¬ê¸° (í† í° ìˆ˜)\n",
        "    chunk_overlap=20,    # ì˜¤ë²„ë© (í† í° ìˆ˜)\n",
        "    model=\"gpt-4o\"      # í† í° ê³„ì‚°ì— ì‚¬ìš©í•  ëª¨ë¸\n",
        ")\n",
        "\n",
        "# chunks = splitter3.split_text(long_text)\n",
        "# print(f\"  âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
        "\n",
        "print(\"\\nğŸ’¡ TokenTextSplitter íŠ¹ì§•:\")\n",
        "print(\"  - ëª¨ë¸ì˜ í† í° ì œí•œì„ ê³ ë ¤\")\n",
        "print(\"  - ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚°\")\n",
        "print(\"  - LLM ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ì— ìµœì í™”\")\n",
        "\n",
        "# ë°©ë²• 4: MarkdownHeaderTextSplitter (êµ¬ì¡° ê¸°ë°˜ ë¶„í• )\n",
        "print(\"\\n[ë°©ë²• 4] MarkdownHeaderTextSplitter (êµ¬ì¡° ê¸°ë°˜ ë¶„í• ):\")\n",
        "markdown_text = \"\"\"\n",
        "# Python ì†Œê°œ\n",
        "\n",
        "Pythonì€ 1991ë…„ì— ë°œí‘œëœ ê³ ê¸‰ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\n",
        "\n",
        "## ì£¼ìš” íŠ¹ì§•\n",
        "\n",
        "### ê°„ê²°í•œ ë¬¸ë²•\n",
        "ì½”ë“œê°€ ì½ê¸° ì‰½ê³  ì‘ì„±í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.\n",
        "\n",
        "### ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "ìˆ˜ë§ì€ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì„œë“œíŒŒí‹° íŒ¨í‚¤ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "\n",
        "splitter4 = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=[\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# chunks = splitter4.split_text(markdown_text)\n",
        "# print(f\"  âœ… ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
        "# for i, chunk in enumerate(chunks, 1):\n",
        "#     print(f\"\\n  ì²­í¬ {i}:\")\n",
        "#     print(f\"    í—¤ë”: {chunk.metadata}\")\n",
        "#     print(f\"    ë‚´ìš©: {chunk.content[:100]}...\")\n",
        "\n",
        "print(\"\\nğŸ’¡ MarkdownHeaderTextSplitter íŠ¹ì§•:\")\n",
        "print(\"  - ë§ˆí¬ë‹¤ìš´ í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• \")\n",
        "print(\"  - ë…¼ë¦¬ì  êµ¬ì¡° ë³´ì¡´\")\n",
        "print(\"  - ë©”íƒ€ë°ì´í„°ì— í—¤ë” ì •ë³´ ì €ì¥\")\n",
        "\n",
        "# ============================================\n",
        "# ì²­í‚¹ ì „ëµ ë¹„êµ\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ì²­í‚¹ ì „ëµ ë¹„êµ\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "strategies = {\n",
        "    \"ì‘ì€ ì²­í¬ (100ì)\": {\n",
        "        \"chunk_size\": 100,\n",
        "        \"chunk_overlap\": 20,\n",
        "        \"ì¥ì \": \"ì •í™•í•œ ê²€ìƒ‰, ë¹ ë¥¸ ì²˜ë¦¬\",\n",
        "        \"ë‹¨ì \": \"ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤ ê°€ëŠ¥\"\n",
        "    },\n",
        "    \"ì¤‘ê°„ ì²­í¬ (500ì)\": {\n",
        "        \"chunk_size\": 500,\n",
        "        \"chunk_overlap\": 50,\n",
        "        \"ì¥ì \": \"ê· í˜•ì¡íŒ ì„±ëŠ¥\",\n",
        "        \"ë‹¨ì \": \"ì—†ìŒ (ê¶Œì¥)\"\n",
        "    },\n",
        "    \"í° ì²­í¬ (1000ì)\": {\n",
        "        \"chunk_size\": 1000,\n",
        "        \"chunk_overlap\": 100,\n",
        "        \"ì¥ì \": \"ë§¥ë½ ë³´ì¡´\",\n",
        "        \"ë‹¨ì \": \"ê²€ìƒ‰ ì •í™•ë„ ì €í•˜ ê°€ëŠ¥\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nğŸ“Š ì²­í¬ í¬ê¸°ë³„ ë¹„êµ:\")\n",
        "for strategy_name, info in strategies.items():\n",
        "    print(f\"\\n[{strategy_name}]\")\n",
        "    print(f\"  í¬ê¸°: {info['chunk_size']}ì, ì˜¤ë²„ë©: {info['chunk_overlap']}ì\")\n",
        "    print(f\"  ì¥ì : {info['ì¥ì ']}\")\n",
        "    print(f\"  ë‹¨ì : {info['ë‹¨ì ']}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ì²­í‚¹ ì „ëµ ì„ íƒ ê°€ì´ë“œ:\")\n",
        "print(\"  - ì¼ë°˜ ë¬¸ì„œ: 500ì, 50ì ì˜¤ë²„ë© (ê¶Œì¥)\")\n",
        "print(\"  - ê¸°ìˆ  ë¬¸ì„œ: 300-500ì, 30-50ì ì˜¤ë²„ë©\")\n",
        "print(\"  - ì½”ë“œ: 200-400ì, 20-40ì ì˜¤ë²„ë©\")\n",
        "print(\"  - ê¸´ ë¬¸ì„œ: 800-1000ì, 80-100ì ì˜¤ë²„ë©\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… í…ìŠ¤íŠ¸ ë¶„í•  í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)\n",
        "ë™ì  íƒ€ì… ì‹œìŠ¤í…œê³¼ í•´ì„í˜• ì–¸ì–´ì…ë‹ˆë‹¤.\n",
        "ë‹¤ì–‘í•œ í”„ë¡œê·¸ë˜ë° íŒ¨ëŸ¬ë‹¤ì„ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
        "ê°ì²´ì§€í–¥, í•¨ìˆ˜í˜•, ì ˆì°¨í˜• í”„ë¡œê·¸ë˜ë°ì´ ëª¨ë‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "í’ë¶€í•œ í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "ì»¤ë®¤ë‹ˆí‹°ê°€ í¬ê³  í™œë°œí•©ë‹ˆë‹¤.\n",
        "ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ, ìë™í™” ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "\"\"\"\n",
        "    * 5\n",
        ")  # 5ë²ˆ ë°˜ë³µ\n",
        "\n",
        "# Document ìƒì„±\n",
        "doc = Document(content=long_text.strip())\n",
        "\n",
        "# ë¶„í• \n",
        "chunks = splitter.split_documents([doc])\n",
        "\n",
        "print(f\"âœ… ì›ë³¸ ë¬¸ì„œ ê¸¸ì´: {len(doc.content)}ì\")\n",
        "print(f\"âœ… ë¶„í• ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ\")\n",
        "print(f\"\\nğŸ“Š ì²­í¬ í¬ê¸° í†µê³„:\")\n",
        "chunk_sizes = [len(chunk.content) for chunk in chunks]\n",
        "print(f\"  - í‰ê· : {sum(chunk_sizes) / len(chunk_sizes):.1f}ì\")\n",
        "print(f\"  - ìµœì†Œ: {min(chunk_sizes)}ì\")\n",
        "print(f\"  - ìµœëŒ€: {max(chunk_sizes)}ì\")\n",
        "\n",
        "print(f\"\\nğŸ“„ ì²« ë²ˆì§¸ ì²­í¬:\")\n",
        "print(f\"  {chunks[0].content[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 ë‹¤ì–‘í•œ Splitter\n",
        "\n",
        "ë¬¸ì„œ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ Splitterë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter,\n",
        "    MarkdownHeaderTextSplitter,\n",
        ")\n",
        "\n",
        "# 1. RecursiveCharacterTextSplitter (ê¶Œì¥)\n",
        "# ë¬¸ë‹¨, ë¬¸ì¥, ë‹¨ì–´ ìˆœì„œë¡œ ë¶„í• \n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "print(\"âœ… RecursiveCharacterTextSplitter: ë¬¸ë‹¨/ë¬¸ì¥/ë‹¨ì–´ ìˆœì„œë¡œ ë¶„í• \")\n",
        "\n",
        "# 2. CharacterTextSplitter\n",
        "# ë‹¨ìˆœíˆ ë¬¸ì ìˆ˜ë¡œ ë¶„í• \n",
        "char_splitter = CharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=50,\n",
        ")\n",
        "print(\"âœ… CharacterTextSplitter: ë‹¨ìˆœ ë¬¸ì ìˆ˜ ê¸°ì¤€\")\n",
        "\n",
        "# 3. TokenTextSplitter\n",
        "# í† í° ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ë” ì •í™•)\n",
        "token_splitter = TokenTextSplitter(\n",
        "    chunk_size=100,  # í† í° ìˆ˜\n",
        "    chunk_overlap=20,\n",
        ")\n",
        "print(\"âœ… TokenTextSplitter: í† í° ìˆ˜ ê¸°ì¤€ (LLMì— ìµœì í™”)\")\n",
        "\n",
        "# 4. MarkdownHeaderTextSplitter\n",
        "# Markdown í—¤ë” ê¸°ì¤€ìœ¼ë¡œ ë¶„í• \n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=[(\"#\", \"Header 1\"), (\"##\", \"Header 2\")]\n",
        ")\n",
        "print(\"âœ… MarkdownHeaderTextSplitter: Markdown í—¤ë” ê¸°ì¤€\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RAGChain ê¸°ë³¸ ì‚¬ìš©\n",
        "\n",
        "RAGChainì€ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³ , ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•œ í›„ ì§ˆì˜ì‘ë‹µì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“  ê³¼ì •ì„ ìë™í™”í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 3.1 ì›ë¼ì´ë„ˆ RAG\n",
        "\n",
        "ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `from_documents()`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from beanllm import RAGChain, Document\n",
        "\n",
        "\n",
        "async def simple_rag():\n",
        "    \"\"\"ê°„ë‹¨í•œ RAG ì˜ˆì œ\"\"\"\n",
        "\n",
        "    # ë¬¸ì„œ ì¤€ë¹„\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ 1991ë…„ì— ë°œí‘œëœ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaëŠ” 1995ë…„ì— ë°œí‘œëœ ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaScriptëŠ” ì›¹ ê°œë°œì„ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    # RAGChain ìƒì„± (ì›ë¼ì´ë„ˆ)\n",
        "    rag = await RAGChain.from_documents(\n",
        "        documents=documents,\n",
        "        embedding_model=\"text-embedding-3-small\",  # ì„ë² ë”© ëª¨ë¸\n",
        "        vector_store=\"chroma\",  # ë²¡í„° ìŠ¤í† ì–´\n",
        "    )\n",
        "\n",
        "    print(\"âœ… RAGChain ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "    # ì§ˆë¬¸í•˜ê¸°\n",
        "    response = await rag.query(\"Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
        "\n",
        "    print(f\"\\nğŸ’¬ ì§ˆë¬¸: Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
        "    print(f\"ğŸ¤– ì‘ë‹µ: {response.answer}\")\n",
        "    print(f\"\\nğŸ“š ì°¸ì¡° ë¬¸ì„œ: {len(response.sources)}ê°œ\")\n",
        "    for i, source in enumerate(response.sources[:3], 1):\n",
        "        print(f\"  {i}. {source.content[:50]}... (ìœ ì‚¬ë„: {source.score:.3f})\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# response = await simple_rag()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 ë””ë ‰í† ë¦¬ì—ì„œ RAG ìƒì„±\n",
        "\n",
        "ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ RAGë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def rag_from_directory():\n",
        "    \"\"\"ë””ë ‰í† ë¦¬ì—ì„œ RAG ìƒì„±\"\"\"\n",
        "\n",
        "    # ë””ë ‰í† ë¦¬ ê²½ë¡œì—ì„œ ì§ì ‘ RAG ìƒì„±\n",
        "    # rag = await RAGChain.from_documents(\n",
        "    #     documents_path=\"./docs\",  # ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
        "    #     embedding_model=\"text-embedding-3-small\",\n",
        "    #     vector_store=\"chroma\",\n",
        "    # )\n",
        "\n",
        "    # ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ ë¡œë“œ í›„ ìƒì„±\n",
        "    from beanllm import DirectoryLoader\n",
        "\n",
        "    # loader = DirectoryLoader(\"./docs\", glob=\"**/*.txt\")\n",
        "    # documents = loader.load()\n",
        "\n",
        "    # rag = await RAGChain.from_documents(\n",
        "    #     documents=documents,\n",
        "    #     embedding_model=\"text-embedding-3-small\",\n",
        "    #     vector_store=\"chroma\",\n",
        "    # )\n",
        "\n",
        "    print(\"ğŸ’¡ ë””ë ‰í† ë¦¬ì—ì„œ RAG ìƒì„±:\")\n",
        "    print(\"1. DirectoryLoaderë¡œ ë¬¸ì„œ ë¡œë“œ\")\n",
        "    print(\"2. RAGChain.from_documents()ë¡œ RAG ìƒì„±\")\n",
        "    print(\"3. ìë™ìœ¼ë¡œ ì„ë² ë”© ìƒì„± ë° ë²¡í„° ìŠ¤í† ì–´ ì €ì¥\")\n",
        "\n",
        "\n",
        "# await rag_from_directory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬\n",
        "\n",
        "ê¸´ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def streaming_rag():\n",
        "    \"\"\"ìŠ¤íŠ¸ë¦¬ë° RAG ì¿¼ë¦¬\"\"\"\n",
        "\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ ê°„ê²°í•˜ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"Pythonì€ ë°ì´í„° ê³¼í•™, ì›¹ ê°œë°œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    rag = await RAGChain.from_documents(\n",
        "        documents=documents,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ:\\n\")\n",
        "    print(\"ğŸ¤– AI: \", end=\"\", flush=True)\n",
        "\n",
        "    async for chunk in rag.stream_query(\"Pythonì˜ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "\n",
        "    print(\"\\n\\nâœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await streaming_rag()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ê³ ê¸‰ RAG ê¸°ëŠ¥\n",
        "\n",
        "### 4.1 ë°°ì¹˜ ì¿¼ë¦¬\n",
        "\n",
        "ì—¬ëŸ¬ ì§ˆë¬¸ì„ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def batch_rag_query():\n",
        "    \"\"\"ë°°ì¹˜ RAG ì¿¼ë¦¬\"\"\"\n",
        "\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaëŠ” ê°ì²´ì§€í–¥ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaScriptëŠ” ì›¹ ê°œë°œ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    rag = await RAGChain.from_documents(\n",
        "        documents=documents,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "    )\n",
        "\n",
        "    # ì—¬ëŸ¬ ì§ˆë¬¸\n",
        "    questions = [\n",
        "        \"Pythonì´ë€?\",\n",
        "        \"Javaë€?\",\n",
        "        \"JavaScriptë€?\",\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ“‹ ë°°ì¹˜ ì¿¼ë¦¬ ì‹œì‘...\")\n",
        "    responses = await rag.batch_query(questions)\n",
        "\n",
        "    print(f\"\\nâœ… {len(responses)}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ\\n\")\n",
        "    for i, (question, response) in enumerate(zip(questions, responses), 1):\n",
        "        print(f\"{i}. ì§ˆë¬¸: {question}\")\n",
        "        print(f\"   ì‘ë‹µ: {response.answer[:100]}...\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await batch_rag_query()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„°\n",
        "\n",
        "RAGChainì˜ ë™ì‘ì„ ì„¸ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def custom_rag():\n",
        "    \"\"\"ì»¤ìŠ¤í…€ íŒŒë¼ë¯¸í„° RAG\"\"\"\n",
        "\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ ê°„ê²°í•œ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"Pythonì€ ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    rag = await RAGChain.from_documents(\n",
        "        documents=documents,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "        vector_store=\"chroma\",\n",
        "        # RAG íŒŒë¼ë¯¸í„°\n",
        "        top_k=3,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜\n",
        "        similarity_threshold=0.7,  # ìœ ì‚¬ë„ ì„ê³„ê°’\n",
        "        # LLM íŒŒë¼ë¯¸í„°\n",
        "        llm_model=\"gpt-4o\",\n",
        "        temperature=0.7,\n",
        "        max_tokens=500,\n",
        "    )\n",
        "\n",
        "    response = await rag.query(\"Pythonì˜ íŠ¹ì§•ì€?\")\n",
        "    print(f\"âœ… ì‘ë‹µ: {response.answer}\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await custom_rag()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ë²¡í„° ìŠ¤í† ì–´ ì„ íƒ\n",
        "\n",
        "ë‹¤ì–‘í•œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 5.1 ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´\n",
        "\n",
        "ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ë²¡í„° ìŠ¤í† ì–´ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´ ì˜µì…˜\n",
        "local_stores = {\n",
        "    \"chroma\": \"ChromaDB - ì‚¬ìš©í•˜ê¸° ì‰¬ì›€, ë¹ ë¥¸ ê°œë°œ\",\n",
        "    \"faiss\": \"FAISS - Facebook, ë§¤ìš° ë¹ ë¦„\",\n",
        "    \"qdrant\": \"Qdrant - ê³ ì„±ëŠ¥, ë¡œì»¬/í´ë¼ìš°ë“œ ëª¨ë‘ ì§€ì›\",\n",
        "    \"lancedb\": \"LanceDB - ë¹ ë¥¸ ë²¡í„° ê²€ìƒ‰\",\n",
        "    \"pgvector\": \"PostgreSQL í™•ì¥, SQLê³¼ í†µí•©\",\n",
        "}\n",
        "\n",
        "print(\"ğŸ’¾ ë¡œì»¬ ë²¡í„° ìŠ¤í† ì–´:\")\n",
        "for store, description in local_stores.items():\n",
        "    print(f\"  - {store}: {description}\")\n",
        "\n",
        "# ì‚¬ìš© ì˜ˆì œ\n",
        "print(\"\\nğŸ’¡ ì‚¬ìš© ë°©ë²•:\")\n",
        "print(\"rag = await RAGChain.from_documents(\")\n",
        "print(\"    documents=docs,\")\n",
        "print(\"    vector_store='chroma',  # ë˜ëŠ” 'faiss', 'qdrant' ë“±\")\n",
        "print(\")\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 í´ë¼ìš°ë“œ ë²¡í„° ìŠ¤í† ì–´\n",
        "\n",
        "í´ë¼ìš°ë“œ ê¸°ë°˜ ë²¡í„° ìŠ¤í† ì–´ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í´ë¼ìš°ë“œ ë²¡í„° ìŠ¤í† ì–´ ì˜µì…˜\n",
        "cloud_stores = {\n",
        "    \"pinecone\": \"Pinecone - ê´€ë¦¬í˜• ë²¡í„° DB\",\n",
        "    \"weaviate\": \"Weaviate - GraphQL ì§€ì›\",\n",
        "    \"milvus\": \"Milvus - ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰\",\n",
        "}\n",
        "\n",
        "print(\"â˜ï¸ í´ë¼ìš°ë“œ ë²¡í„° ìŠ¤í† ì–´:\")\n",
        "for store, description in cloud_stores.items():\n",
        "    print(f\"  - {store}: {description}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ í´ë¼ìš°ë“œ ìŠ¤í† ì–´ ì‚¬ìš© ì‹œ:\")\n",
        "print(\"  - API í‚¤ ì„¤ì • í•„ìš”\")\n",
        "print(\"  - ë„¤íŠ¸ì›Œí¬ ì—°ê²° í•„ìš”\")\n",
        "print(\"  - í™•ì¥ì„±ê³¼ ê´€ë¦¬ í¸ì˜ì„± ì œê³µ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ì‹¤ì „ ì˜ˆì œ\n",
        "\n",
        "### 6.1 ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ\n",
        "\n",
        "ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def qa_system():\n",
        "    \"\"\"ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ\"\"\"\n",
        "\n",
        "    # ì§€ì‹ ë² ì´ìŠ¤ ë¬¸ì„œ\n",
        "    knowledge_base = [\n",
        "        Document(\n",
        "            content=\"beanllmì€ ë‹¤ì–‘í•œ LLM Providerë¥¼ í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íˆ´í‚·ì…ë‹ˆë‹¤.\",\n",
        "            metadata={\"category\": \"introduction\"},\n",
        "        ),\n",
        "        Document(\n",
        "            content=\"RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, ë¬¸ì„œ ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•œ ê¸°ìˆ ì…ë‹ˆë‹¤.\",\n",
        "            metadata={\"category\": \"rag\"},\n",
        "        ),\n",
        "        Document(\n",
        "            content=\"ë²¡í„° ìŠ¤í† ì–´ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.\",\n",
        "            metadata={\"category\": \"vector_store\"},\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # RAG ì‹œìŠ¤í…œ ìƒì„±\n",
        "    rag = await RAGChain.from_documents(\n",
        "        documents=knowledge_base,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "        vector_store=\"chroma\",\n",
        "    )\n",
        "\n",
        "    # ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸\n",
        "    questions = [\n",
        "        \"beanllmì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "        \"RAGë€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "        \"ë²¡í„° ìŠ¤í† ì–´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?\",\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ“š ë¬¸ì„œ ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ\\n\")\n",
        "    for question in questions:\n",
        "        response = await rag.query(question)\n",
        "        print(f\"â“ ì§ˆë¬¸: {question}\")\n",
        "        print(f\"ğŸ’¡ ë‹µë³€: {response.answer}\")\n",
        "        print(f\"ğŸ“„ ì°¸ì¡°: {len(response.sources)}ê°œ ë¬¸ì„œ\\n\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await qa_system()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "RAGì™€ ë¬¸ì„œ ì²˜ë¦¬ë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŠœí† ë¦¬ì–¼ë¡œ ì§„í–‰í•˜ì„¸ìš”:\n",
        "\n",
        "### ğŸ“š ì¶”ì²œ í•™ìŠµ ìˆœì„œ\n",
        "\n",
        "1. **[04_embeddings_vector_stores.ipynb](04_embeddings_vector_stores.ipynb)** - ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´\n",
        "   - ì„ë² ë”© ìƒì„± ë°©ë²•\n",
        "   - ë²¡í„° ìŠ¤í† ì–´ ì§ì ‘ ì‚¬ìš©\n",
        "   - ìœ ì‚¬ë„ ê²€ìƒ‰ ìµœì í™”\n",
        "\n",
        "2. **[05_agent_and_tools.ipynb](05_agent_and_tools.ipynb)** - Agent ë° Tools\n",
        "   - Toolì„ ì‚¬ìš©í•˜ëŠ” Agent\n",
        "   - ë³µì¡í•œ ì‘ì—… ìë™í™”\n",
        "\n",
        "3. **[08_vision_rag.ipynb](08_vision_rag.ipynb)** - Vision RAG\n",
        "   - ì´ë¯¸ì§€ ê¸°ë°˜ RAG\n",
        "   - ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰\n",
        "\n",
        "### ğŸ”— ê´€ë ¨ ë¬¸ì„œ\n",
        "\n",
        "- [API Reference](../API_REFERENCE.md#ragchain) - RAGChain API ìƒì„¸ ë¬¸ì„œ\n",
        "- [README.md](../../README.md) - í”„ë¡œì íŠ¸ ê°œìš”\n",
        "\n",
        "---\n",
        "\n",
        "**âœ… RAG & Document Processing ì™„ë£Œ! ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
