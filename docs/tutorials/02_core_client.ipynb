{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ’¬ Client - LLM ê¸°ë³¸ ì‚¬ìš©ë²•\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ beanllmì˜ í•µì‹¬ ê¸°ëŠ¥ì¸ `Client`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ LLM Providerì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“‹ ëª©ì°¨\n",
        "\n",
        "- [1. Client ê¸°ë³¸ ì‚¬ìš©](#1-client-ê¸°ë³¸-ì‚¬ìš©)\n",
        "- [2. Provider ì „í™˜](#2-provider-ì „í™˜)\n",
        "- [3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ](#3-ìŠ¤íŠ¸ë¦¬ë°-ì‘ë‹µ)\n",
        "- [4. íŒŒë¼ë¯¸í„° ì¡°ì •](#4-íŒŒë¼ë¯¸í„°-ì¡°ì •)\n",
        "- [5. Model Registry í™œìš©](#5-model-registry-í™œìš©)\n",
        "- [6. ê³ ê¸‰ ê¸°ëŠ¥](#6-ê³ ê¸‰-ê¸°ëŠ¥)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•˜ë©´:\n",
        "- âœ… Clientë¥¼ ì‚¬ìš©í•˜ì—¬ LLMê³¼ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ë‹¤ì–‘í•œ Provider ê°„ ì „í™˜ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì—¬ ì‘ë‹µì„ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… Model Registryë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "- [01_setup_and_installation.ipynb](01_setup_and_installation.ipynb) ì™„ë£Œ\n",
        "- ìµœì†Œ í•˜ë‚˜ì˜ LLM Provider API í‚¤ ì„¤ì •\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Client ê¸°ë³¸ ì‚¬ìš©\n",
        "\n",
        "`Client`ëŠ” beanllmì˜ í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ë¡œ, ë‹¤ì–‘í•œ LLM Providerë¥¼ í†µì¼ëœ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. Providerì— ìƒê´€ì—†ì´ ë™ì¼í•œ APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´, ì½”ë“œ ë³€ê²½ ì—†ì´ ë‹¤ë¥¸ Providerë¡œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 1.1 Client ìƒì„±\n",
        "\n",
        "`Client`ë¥¼ ìƒì„±í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. beanllmì€ ìë™ìœ¼ë¡œ ì ì ˆí•œ Providerë¥¼ ì„ íƒí•˜ê³  ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **í•µì‹¬ ê°œë…**: \n",
        "> - ClientëŠ” Providerì— ìƒê´€ì—†ì´ ë™ì¼í•œ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤\n",
        "> - ë‚´ë¶€ì ìœ¼ë¡œ `ParameterAdapter`ê°€ ìë™ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤\n",
        "> - Model Registryë¥¼ í†µí•´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•©ë‹ˆë‹¤\n",
        "> - Providerë³„ ì°¨ì´ì ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜ì‚¬í•­**: \n",
        "> - API í‚¤ê°€ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤\n",
        "> - ì‚¬ìš©í•˜ë ¤ëŠ” Providerì˜ SDKê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from beanllm import Client, get_model_registry\n",
        "\n",
        "# ============================================\n",
        "# Client ìƒì„± (ê¸°ë³¸ ë°©ë²•)\n",
        "# ============================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Client ìƒì„± - ê¸°ë³¸ ë°©ë²•\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ë°©ë²• 1: ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì • (ê°€ì¥ ê°„ë‹¨)\n",
        "print(\"\\n[ë°©ë²• 1] ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì •:\")\n",
        "client1 = Client(model=\"gpt-4o\")\n",
        "print(f\"  âœ… Client ìƒì„± ì™„ë£Œ!\")\n",
        "print(f\"  ğŸ“Š ëª¨ë¸: {client1.model}\")\n",
        "print(f\"  ğŸ“Š Provider: {getattr(client1, 'provider', 'ìë™ ì„ íƒ')}\")\n",
        "\n",
        "# ë°©ë²• 2: Provider ëª…ì‹œì  ì§€ì •\n",
        "print(\"\\n[ë°©ë²• 2] Provider ëª…ì‹œì  ì§€ì •:\")\n",
        "# client2 = Client(model=\"gpt-4o\", provider=\"openai\")\n",
        "# print(f\"  âœ… Client ìƒì„± ì™„ë£Œ!\")\n",
        "# print(f\"  ğŸ“Š ëª¨ë¸: {client2.model}\")\n",
        "# print(f\"  ğŸ“Š Provider: {client2.provider}\")\n",
        "\n",
        "# ë°©ë²• 3: ì¶”ê°€ ì˜µì…˜ ì§€ì •\n",
        "print(\"\\n[ë°©ë²• 3] ì¶”ê°€ ì˜µì…˜ ì§€ì •:\")\n",
        "# client3 = Client(\n",
        "#     model=\"gpt-4o\",\n",
        "#     temperature=0.7,\n",
        "#     max_tokens=1000,\n",
        "#     timeout=30\n",
        "# )\n",
        "# print(f\"  âœ… Client ìƒì„± ì™„ë£Œ!\")\n",
        "# print(f\"  ğŸ“Š ëª¨ë¸: {client3.model}\")\n",
        "# print(f\"  ğŸ“Š Temperature: {client3.temperature if hasattr(client3, 'temperature') else 'N/A'}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Client ìƒì„± ì˜µì…˜:\")\n",
        "print(\"  - model: ëª¨ë¸ ì´ë¦„ (í•„ìˆ˜)\")\n",
        "print(\"  - provider: Provider ì´ë¦„ (ì„ íƒ, ìë™ ì„ íƒë¨)\")\n",
        "print(\"  - temperature: ì°½ì˜ì„± ì¡°ì ˆ (0.0-2.0, ê¸°ë³¸ê°’: 1.0)\")\n",
        "print(\"  - max_tokens: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: ëª¨ë¸ë³„ ë‹¤ë¦„)\")\n",
        "print(\"  - timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 30)\")\n",
        "\n",
        "# Model Registryë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "try:\n",
        "    registry = get_model_registry()\n",
        "    models = registry.get_all_models()\n",
        "\n",
        "    print(f\"\\nğŸ“Š ì „ì²´ ëª¨ë¸ ìˆ˜: {len(models)}ê°œ\")\n",
        "    print(\"\\nğŸ” Providerë³„ ëª¨ë¸ ìˆ˜:\")\n",
        "    by_provider = registry.get_models_by_provider()\n",
        "    for provider, provider_models in by_provider.items():\n",
        "        print(f\"  - {provider}: {len(provider_models)}ê°œ\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ì¸ê¸° ëª¨ë¸ ì˜ˆì‹œ:\")\n",
        "    popular_models = [\"gpt-4o\", \"claude-sonnet-4-20250514\", \"gemini-2.5-pro\"]\n",
        "    for model_name in popular_models:\n",
        "        model = registry.get_model(model_name)\n",
        "        if model:\n",
        "            print(f\"  - {model.name} ({model.provider})\")\n",
        "except Exception as e:\n",
        "    print(f\"  âš ï¸ Model Registry í™•ì¸ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… Client ìƒì„± ë°©ë²• í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 ê¸°ë³¸ ì±„íŒ…\n",
        "\n",
        "ê°€ì¥ ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì€ `chat()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë©”ì„œë“œëŠ” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ LLMì˜ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ë©”ì‹œì§€ í˜•ì‹**:\n",
        "> - `role`: \"system\", \"user\", \"assistant\" ì¤‘ í•˜ë‚˜\n",
        "> - `content`: ë©”ì‹œì§€ ë‚´ìš© (ë¬¸ìì—´)\n",
        "> - System ë©”ì‹œì§€ëŠ” ëª¨ë¸ì˜ í–‰ë™ì„ ì •ì˜í•©ë‹ˆë‹¤\n",
        "> - User ë©”ì‹œì§€ëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì…ë‹ˆë‹¤\n",
        "> - Assistant ë©”ì‹œì§€ëŠ” ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ê¸°ë³¸ ì±„íŒ… ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def basic_chat():\n",
        "    \"\"\"\n",
        "    ê¸°ë³¸ ì±„íŒ… ì˜ˆì œ\n",
        "\n",
        "    ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ LLM ëŒ€í™”ì…ë‹ˆë‹¤.\n",
        "    ë‹¨ì¼ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ë³´ë‚´ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ê¸°ë³¸ ì±„íŒ… ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # ë‹¨ì¼ ë©”ì‹œì§€\n",
        "    user_message = \"ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨íˆ ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”.\"\n",
        "    print(f\"\\nğŸ’¬ ì‚¬ìš©ì: {user_message}\")\n",
        "\n",
        "    # LLM í˜¸ì¶œ\n",
        "    # response = await client.chat(\n",
        "    #     messages=[{\"role\": \"user\", \"content\": user_message}]\n",
        "    # )\n",
        "\n",
        "    # ì‘ë‹µ ì¶œë ¥\n",
        "    # print(f\"\\nğŸ¤– AI: {response.content}\")\n",
        "\n",
        "    # ë©”íƒ€ë°ì´í„° í™•ì¸\n",
        "    # print(f\"\\nğŸ“Š ì‘ë‹µ ë©”íƒ€ë°ì´í„°:\")\n",
        "    # print(f\"  - ëª¨ë¸: {response.model}\")\n",
        "    # if hasattr(response, 'usage'):\n",
        "    #     print(f\"  - ì…ë ¥ í† í°: {response.usage.prompt_tokens}\")\n",
        "    #     print(f\"  - ì¶œë ¥ í† í°: {response.usage.completion_tokens}\")\n",
        "    #     print(f\"  - ì´ í† í°: {response.usage.total_tokens}\")\n",
        "    # if hasattr(response, 'finish_reason'):\n",
        "    #     print(f\"  - ì™„ë£Œ ì´ìœ : {response.finish_reason}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ê¸°ë³¸ ì±„íŒ…ì˜ íŠ¹ì§•:\")\n",
        "    print(\"  - ê°€ì¥ ê°„ë‹¨í•œ ì‚¬ìš©ë²•\")\n",
        "    print(\"  - ë‹¨ì¼ ì§ˆë¬¸-ë‹µë³€ì— ì í•©\")\n",
        "    print(\"  - ëŒ€í™” ë§¥ë½ ì—†ìŒ (ê° í˜¸ì¶œì´ ë…ë¦½ì )\")\n",
        "\n",
        "    # return response\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# response = await basic_chat()\n",
        "\n",
        "# ============================================\n",
        "# System ë©”ì‹œì§€ ì‚¬ìš© ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def system_message_example():\n",
        "    \"\"\"\n",
        "    System ë©”ì‹œì§€ ì‚¬ìš© ì˜ˆì œ\n",
        "\n",
        "    System ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ í–‰ë™ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"System ë©”ì‹œì§€ ì‚¬ìš© ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # System ë©”ì‹œì§€ë¡œ ëª¨ë¸ì˜ ì—­í•  ì •ì˜\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ Python í”„ë¡œê·¸ë˜ë° íŠœí„°ì…ë‹ˆë‹¤. \"\n",
        "            \"ë³µì¡í•œ ê°œë…ì„ ì‰½ê²Œ ì„¤ëª…í•˜ê³ , ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì œê³µí•©ë‹ˆë‹¤.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": \"Pythonì˜ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"},\n",
        "    ]\n",
        "\n",
        "    print(\"\\nğŸ“ ë©”ì‹œì§€ êµ¬ì„±:\")\n",
        "    for msg in messages:\n",
        "        print(f\"  [{msg['role'].upper()}]: {msg['content'][:50]}...\")\n",
        "\n",
        "    # LLM í˜¸ì¶œ\n",
        "    # response = await client.chat(messages=messages)\n",
        "    # print(f\"\\nğŸ¤– AI: {response.content}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ System ë©”ì‹œì§€ì˜ ì¥ì :\")\n",
        "    print(\"  - ëª¨ë¸ì˜ í–‰ë™ì„ ì¼ê´€ë˜ê²Œ ì œì–´\")\n",
        "    print(\"  - íŠ¹ì • ì—­í• ì´ë‚˜ í†¤ ì„¤ì •\")\n",
        "    print(\"  - ì‘ë‹µ ìŠ¤íƒ€ì¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await system_message_example()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ê¸°ë³¸ ì±„íŒ… í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 ëŒ€í™”í˜• ì±„íŒ…\n",
        "\n",
        "ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ì£¼ê³ ë°›ëŠ” ëŒ€í™”ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ ëª¨ë¸ì´ ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´í•´í•˜ê³  ì¼ê´€ëœ ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬**:\n",
        "> - ê° ì‘ë‹µ í›„ Assistant ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
        "> - User ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
        "> - ì „ì²´ íˆìŠ¤í† ë¦¬ë¥¼ ë‹¤ìŒ í˜¸ì¶œì— ì „ë‹¬\n",
        "> - í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ íˆìŠ¤í† ë¦¬ ê¸¸ì´ ê´€ë¦¬ í•„ìš”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ëŒ€í™”í˜• ì±„íŒ… ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def conversation_example():\n",
        "    \"\"\"\n",
        "    ëŒ€í™”í˜• ì±„íŒ… ì˜ˆì œ\n",
        "\n",
        "    ì—¬ëŸ¬ í„´ì˜ ëŒ€í™”ë¥¼ ì´ì–´ê°€ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ìœ ì§€í•˜ì—¬ ë§¥ë½ì„ ë³´ì¡´í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ëŒ€í™”í˜• ì±„íŒ… ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì§€ì‹ì´ í’ë¶€í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?\"},\n",
        "    ]\n",
        "\n",
        "    print(\"\\n[í„´ 1]\")\n",
        "    print(f\"ğŸ’¬ ì‚¬ìš©ì: {messages[-1]['content']}\")\n",
        "\n",
        "    # ì²« ë²ˆì§¸ ì‘ë‹µ\n",
        "    # response1 = await client.chat(messages=messages)\n",
        "    # print(f\"ğŸ¤– AI: {response1.content}\\n\")\n",
        "\n",
        "    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
        "    # messages.append({\"role\": \"assistant\", \"content\": response1.content})\n",
        "    # messages.append({\"role\": \"user\", \"content\": \"ê·¸ëŸ¼ Javaì™€ ë¹„êµí•´ì£¼ì„¸ìš”.\"})\n",
        "\n",
        "    print(\"\\n[í„´ 2]\")\n",
        "    print(\n",
        "        f\"ğŸ’¬ ì‚¬ìš©ì: {messages[-1]['content'] if len(messages) > 2 else 'ê·¸ëŸ¼ Javaì™€ ë¹„êµí•´ì£¼ì„¸ìš”.'}\"\n",
        "    )\n",
        "\n",
        "    # ë‘ ë²ˆì§¸ ì‘ë‹µ\n",
        "    # response2 = await client.chat(messages=messages)\n",
        "    # print(f\"ğŸ¤– AI: {response2.content}\\n\")\n",
        "\n",
        "    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
        "    # messages.append({\"role\": \"assistant\", \"content\": response2.content})\n",
        "    # messages.append({\"role\": \"user\", \"content\": \"ì–´ë–¤ í”„ë¡œì íŠ¸ì— Pythonì„ ì‚¬ìš©í•˜ëŠ” ê²Œ ì¢‹ì„ê¹Œìš”?\"})\n",
        "\n",
        "    print(\"\\n[í„´ 3]\")\n",
        "    print(\n",
        "        f\"ğŸ’¬ ì‚¬ìš©ì: {messages[-1]['content'] if len(messages) > 4 else 'ì–´ë–¤ í”„ë¡œì íŠ¸ì— Pythonì„ ì‚¬ìš©í•˜ëŠ” ê²Œ ì¢‹ì„ê¹Œìš”?'}\"\n",
        "    )\n",
        "\n",
        "    # ì„¸ ë²ˆì§¸ ì‘ë‹µ\n",
        "    # response3 = await client.chat(messages=messages)\n",
        "    # print(f\"ğŸ¤– AI: {response3.content}\\n\")\n",
        "\n",
        "    # ëŒ€í™” í†µê³„\n",
        "    # print(\"=\" * 60)\n",
        "    # print(\"ëŒ€í™” í†µê³„\")\n",
        "    # print(\"=\" * 60)\n",
        "    # print(f\"  ì´ ëŒ€í™” í„´: {len([m for m in messages if m['role'] == 'user'])}í„´\")\n",
        "    # print(f\"  ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ\")\n",
        "    # print(f\"  System ë©”ì‹œì§€: {len([m for m in messages if m['role'] == 'system'])}ê°œ\")\n",
        "    # print(f\"  User ë©”ì‹œì§€: {len([m for m in messages if m['role'] == 'user'])}ê°œ\")\n",
        "    # print(f\"  Assistant ë©”ì‹œì§€: {len([m for m in messages if m['role'] == 'assistant'])}ê°œ\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ëŒ€í™”í˜• ì±„íŒ…ì˜ íŠ¹ì§•:\")\n",
        "    print(\"  - ì´ì „ ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€\")\n",
        "    print(\"  - ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„\")\n",
        "    print(\"  - ì°¸ì¡° ê°€ëŠ¥ (ì˜ˆ: 'ê·¸ê²ƒ', 'ìœ„ì—ì„œ ë§í•œ ê²ƒ')\")\n",
        "    print(\"  - í† í° ì‚¬ìš©ëŸ‰ì´ ì¦ê°€í•˜ë¯€ë¡œ ì£¼ì˜ í•„ìš”\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await conversation_example()\n",
        "\n",
        "# ============================================\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def conversation_history_management():\n",
        "    \"\"\"\n",
        "    ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì˜ˆì œ\n",
        "\n",
        "    ê¸´ ëŒ€í™”ì˜ ê²½ìš° íˆìŠ¤í† ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # ëŒ€í™” íˆìŠ¤í† ë¦¬\n",
        "    messages = [{\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\"}]\n",
        "\n",
        "    # ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜\n",
        "    conversations = [\n",
        "        \"ì•ˆë…•í•˜ì„¸ìš”!\",\n",
        "        \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?\",\n",
        "        \"Pythonì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\",\n",
        "        \"ê·¸ëŸ¼ JavaScriptëŠ”ìš”?\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\nğŸ“ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜:\")\n",
        "    # for i, user_msg in enumerate(conversations, 1):\n",
        "    #     messages.append({\"role\": \"user\", \"content\": user_msg})\n",
        "    #     print(f\"\\n[í„´ {i}]\")\n",
        "    #     print(f\"  ì‚¬ìš©ì: {user_msg}\")\n",
        "    #\n",
        "    #     # ì‘ë‹µ ë°›ê¸°\n",
        "    #     response = await client.chat(messages=messages)\n",
        "    #     print(f\"  AI: {response.content[:100]}...\")\n",
        "    #\n",
        "    #     # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€\n",
        "    #     messages.append({\"role\": \"assistant\", \"content\": response.content})\n",
        "    #\n",
        "    #     # íˆìŠ¤í† ë¦¬ ê¸¸ì´ í™•ì¸\n",
        "    #     total_chars = sum(len(m[\"content\"]) for m in messages)\n",
        "    #     print(f\"  íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(messages)}ê°œ ë©”ì‹œì§€, {total_chars}ì\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ íŒ:\")\n",
        "    print(\"  - í† í° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ì˜¤ë˜ëœ ë©”ì‹œì§€ ì œê±°\")\n",
        "    print(\"  - ì¤‘ìš”í•œ ë§¥ë½ë§Œ ìœ ì§€\")\n",
        "    print(\"  - ìš”ì•½ì„ ì‚¬ìš©í•˜ì—¬ íˆìŠ¤í† ë¦¬ ì••ì¶•\")\n",
        "    print(\"  - System ë©”ì‹œì§€ëŠ” í•­ìƒ ìœ ì§€\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await conversation_history_management()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ëŒ€í™”í˜• ì±„íŒ… í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Provider ì „í™˜\n",
        "\n",
        "beanllmì˜ ê°€ì¥ ê°•ë ¥í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” **Provider ê°„ ììœ ë¡œìš´ ì „í™˜**ì…ë‹ˆë‹¤. ê°™ì€ ì½”ë“œë¡œ ë‹¤ë¥¸ Providerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì–´, ë¹„ìš©, ì„±ëŠ¥, ê¸°ëŠ¥ì— ë”°ë¼ ìµœì ì˜ Providerë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 2.1 Providerë³„ Client ìƒì„±\n",
        "\n",
        "ê° ProviderëŠ” ê³ ìœ í•œ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, OpenAIëŠ” ë¹ ë¥¸ ì‘ë‹µ ì†ë„, Anthropicì€ ê¸´ ì»¨í…ìŠ¤íŠ¸, Googleì€ ë¹„ìš© íš¨ìœ¨ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. beanllmì€ ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ, ì½”ë“œ ë³€ê²½ ì—†ì´ Providerë¥¼ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **Provider ì„ íƒ ê°€ì´ë“œ**:\n",
        "> - **OpenAI**: ë¹ ë¥¸ ì‘ë‹µ, ë†’ì€ í’ˆì§ˆ, ë„“ì€ ëª¨ë¸ ì„ íƒ\n",
        "> - **Anthropic**: ê¸´ ì»¨í…ìŠ¤íŠ¸, ì•ˆì „ì„±, ë³µì¡í•œ ì¶”ë¡ \n",
        "> - **Google Gemini**: ë¹„ìš© íš¨ìœ¨, ë©€í‹°ëª¨ë‹¬, ë¹ ë¥¸ ì‘ë‹µ\n",
        "> - **DeepSeek**: ì €ë ´í•œ ë¹„ìš©, í•œêµ­ì–´ ì§€ì›\n",
        "> - **Perplexity**: ì›¹ ê²€ìƒ‰ í†µí•©, ìµœì‹  ì •ë³´\n",
        "> - **Ollama**: ë¡œì»¬ ì‹¤í–‰, í”„ë¼ì´ë²„ì‹œ, ë¬´ë£Œ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Provider ë¹„êµ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def provider_comparison():\n",
        "    \"\"\"\n",
        "    Provider ë¹„êµ ì˜ˆì œ\n",
        "\n",
        "    ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ Providerì˜ ì‘ë‹µì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
        "    ê° Providerì˜ íŠ¹ì„±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Provider ë¹„êµ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    question = \"ì–‘ì ì»´í“¨íŒ…ì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "    print(f\"\\nâ“ ì§ˆë¬¸: {question}\\n\")\n",
        "\n",
        "    providers_to_test = [\n",
        "        {\"name\": \"OpenAI\", \"model\": \"gpt-4o\", \"emoji\": \"ğŸ”µ\", \"description\": \"ë¹ ë¥¸ ì‘ë‹µ, ë†’ì€ í’ˆì§ˆ\"},\n",
        "        {\n",
        "            \"name\": \"Anthropic\",\n",
        "            \"model\": \"claude-sonnet-4-20250514\",\n",
        "            \"emoji\": \"ğŸŸ£\",\n",
        "            \"description\": \"ê¸´ ì»¨í…ìŠ¤íŠ¸, ì•ˆì „ì„±\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Google Gemini\",\n",
        "            \"model\": \"gemini-2.5-pro\",\n",
        "            \"emoji\": \"ğŸŸ¢\",\n",
        "            \"description\": \"ë¹„ìš© íš¨ìœ¨, ë©€í‹°ëª¨ë‹¬\",\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"DeepSeek\",\n",
        "            \"model\": \"deepseek-chat\",\n",
        "            \"emoji\": \"ğŸŸ¡\",\n",
        "            \"description\": \"ì €ë ´í•œ ë¹„ìš©, í•œêµ­ì–´ ì§€ì›\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for provider_info in providers_to_test:\n",
        "        print(\n",
        "            f\"\\n{provider_info['emoji']} {provider_info['name']} ({provider_info['description']}):\"\n",
        "        )\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        try:\n",
        "            client = Client(model=provider_info[\"model\"])\n",
        "            # response = await client.chat(\n",
        "            #     messages=[{\"role\": \"user\", \"content\": question}]\n",
        "            # )\n",
        "            #\n",
        "            # print(f\"  ì‘ë‹µ: {response.content[:200]}...\")\n",
        "            #\n",
        "            # # ì‘ë‹µ ì‹œê°„ ì¸¡ì • (ì‹œë®¬ë ˆì´ì…˜)\n",
        "            # import time\n",
        "            # start_time = time.time()\n",
        "            # # ì‹¤ì œ í˜¸ì¶œì€ ìœ„ì—ì„œ ì´ë¯¸ í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜\n",
        "            # elapsed_time = time.time() - start_time\n",
        "            #\n",
        "            # results.append({\n",
        "            #     \"provider\": provider_info['name'],\n",
        "            #     \"model\": provider_info['model'],\n",
        "            #     \"response_length\": len(response.content),\n",
        "            #     \"response_preview\": response.content[:100]\n",
        "            # })\n",
        "\n",
        "            print(f\"  âœ… ì„±ê³µ (ì‹œë®¬ë ˆì´ì…˜)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ ì˜¤ë¥˜: {e}\")\n",
        "            print(f\"  ğŸ’¡ API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "    # ê²°ê³¼ ë¹„êµ\n",
        "    # if results:\n",
        "    #     print(\"\\n\" + \"=\" * 60)\n",
        "    #     print(\"Provider ë¹„êµ ê²°ê³¼\")\n",
        "    #     print(\"=\" * 60)\n",
        "    #     for result in results:\n",
        "    #         print(f\"\\n[{result['provider']}]\")\n",
        "    #         print(f\"  ëª¨ë¸: {result['model']}\")\n",
        "    #         print(f\"  ì‘ë‹µ ê¸¸ì´: {result['response_length']}ì\")\n",
        "    #         print(f\"  ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {result['response_preview']}...\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ Provider ì„ íƒ íŒ:\")\n",
        "    print(\"  - ë¹ ë¥¸ ì‘ë‹µì´ í•„ìš”: OpenAI, Google Gemini\")\n",
        "    print(\"  - ê¸´ ì»¨í…ìŠ¤íŠ¸ í•„ìš”: Anthropic Claude\")\n",
        "    print(\"  - ë¹„ìš© ì ˆê°: DeepSeek, Google Gemini\")\n",
        "    print(\"  - ë¡œì»¬ ì‹¤í–‰: Ollama\")\n",
        "    print(\"  - ìµœì‹  ì •ë³´: Perplexity\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await provider_comparison()\n",
        "\n",
        "# ============================================\n",
        "# ë™ì  Provider ì „í™˜ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def dynamic_provider_switching():\n",
        "    \"\"\"\n",
        "    ë™ì  Provider ì „í™˜ ì˜ˆì œ\n",
        "\n",
        "    ëŸ°íƒ€ì„ì— Providerë¥¼ ì „í™˜í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ë™ì  Provider ì „í™˜ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    question = \"ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "    # Provider ë¦¬ìŠ¤íŠ¸\n",
        "    providers = [\n",
        "        (\"gpt-4o\", \"OpenAI\"),\n",
        "        (\"claude-sonnet-4-20250514\", \"Anthropic\"),\n",
        "        (\"gemini-2.5-pro\", \"Google\"),\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nâ“ ì§ˆë¬¸: {question}\")\n",
        "    print(f\"\\nğŸ”„ ì—¬ëŸ¬ Providerë¡œ ì‹œë„:\\n\")\n",
        "\n",
        "    # ê° Providerë¡œ ì‹œë„ (ì²« ë²ˆì§¸ ì„±ê³µí•œ ê²ƒ ì‚¬ìš©)\n",
        "    # for model, provider_name in providers:\n",
        "    #     try:\n",
        "    #         print(f\"[ì‹œë„] {provider_name} ({model})...\")\n",
        "    #         client = Client(model=model)\n",
        "    #         response = await client.chat(\n",
        "    #             messages=[{\"role\": \"user\", \"content\": question}]\n",
        "    #         )\n",
        "    #         print(f\"  âœ… ì„±ê³µ! {provider_name} ì‚¬ìš©\")\n",
        "    #         print(f\"  ì‘ë‹µ: {response.content[:150]}...\")\n",
        "    #         break\n",
        "    #     except Exception as e:\n",
        "    #         print(f\"  âŒ ì‹¤íŒ¨: {e}\")\n",
        "    #         continue\n",
        "    # else:\n",
        "    #     print(\"  âš ï¸ ëª¨ë“  Provider ì‹¤íŒ¨\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ë™ì  ì „í™˜ì˜ í™œìš©:\")\n",
        "    print(\"  - Fallback ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„\")\n",
        "    print(\"  - ë¹„ìš© ìµœì í™” (ì €ë ´í•œ Provider ìš°ì„ )\")\n",
        "    print(\"  - ì„±ëŠ¥ ìµœì í™” (ë¹ ë¥¸ Provider ìš°ì„ )\")\n",
        "    print(\"  - A/B í…ŒìŠ¤íŠ¸\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await dynamic_provider_switching()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… Provider ì „í™˜ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ìë™ Provider ì„ íƒ\n",
        "\n",
        "ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì •í•˜ë©´ ìë™ìœ¼ë¡œ ì ì ˆí•œ Providerë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import get_model_registry\n",
        "\n",
        "# Model Registryë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸\n",
        "registry = get_model_registry()\n",
        "\n",
        "# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡\n",
        "print(\"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ (ì²˜ìŒ 10ê°œ):\")\n",
        "models = registry.get_all_models()\n",
        "for i, model in enumerate(models[:10], 1):\n",
        "    print(f\"  {i}. {model.name} ({model.provider})\")\n",
        "\n",
        "# íŠ¹ì • Providerì˜ ëª¨ë¸\n",
        "print(\"\\nğŸ”µ OpenAI ëª¨ë¸:\")\n",
        "openai_models = registry.get_models_by_provider().get(\"openai\", [])\n",
        "for model in openai_models[:5]:\n",
        "    print(f\"  - {model.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ\n",
        "\n",
        "ê¸´ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ìœ¼ë ¤ë©´ `stream_chat()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë°ì€ ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ëŒ€ë¡œ ì²­í¬ ë‹¨ìœ„ë¡œ ë°›ì•„ë³¼ ìˆ˜ ìˆì–´, ì‚¬ìš©ì ê²½í—˜ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ìŠ¤íŠ¸ë¦¬ë°ì˜ ì¥ì **: \n",
        "> - **ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ**: ì¦‰ì‹œ ì‘ë‹µì´ ì‹œì‘ë˜ì–´ ëŒ€ê¸° ì‹œê°„ ê°ì†Œ\n",
        "> - **ê¸´ ì‘ë‹µ ì²˜ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ì§€ ì•Šê³  ì ì§„ì ìœ¼ë¡œ ì²˜ë¦¬\n",
        "> - **ì‹¤ì‹œê°„ í”¼ë“œë°±**: ì‘ë‹µ ìƒì„± ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥\n",
        "> - **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ì „ì²´ ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì§€ ì•Šê³  ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì²˜ë¦¬\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜ì‚¬í•­**:\n",
        "> - ìŠ¤íŠ¸ë¦¬ë°ì€ ë¹„ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ `async for`ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "> - ê° ì²­í¬ëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì „ì²´ ì‘ë‹µì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def streaming_example():\n",
        "    \"\"\"\n",
        "    ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ\n",
        "\n",
        "    ìŠ¤íŠ¸ë¦¬ë°ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ë´…ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    question = \"Pythonì˜ ì¥ì  5ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "    print(f\"\\nâ“ ì§ˆë¬¸: {question}\")\n",
        "    print(\"\\nğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹œì‘:\\n\")\n",
        "    print(\"ğŸ¤– AI: \", end=\"\", flush=True)\n",
        "\n",
        "    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘\n",
        "    full_response = \"\"\n",
        "    chunk_count = 0\n",
        "\n",
        "    # async for chunk in client.stream_chat(\n",
        "    #     messages=[{\"role\": \"user\", \"content\": question}]\n",
        "    # ):\n",
        "    #     print(chunk, end=\"\", flush=True)\n",
        "    #     full_response += chunk\n",
        "    #     chunk_count += 1\n",
        "\n",
        "    print(f\"\\n\\nâœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“Š í†µê³„:\")\n",
        "    print(f\"  - ì „ì²´ ì‘ë‹µ ê¸¸ì´: {len(full_response)}ì\")\n",
        "    print(f\"  - ìˆ˜ì‹ ëœ ì²­í¬ ìˆ˜: {chunk_count}ê°œ\")\n",
        "    print(f\"  - í‰ê·  ì²­í¬ í¬ê¸°: {len(full_response) // max(chunk_count, 1)}ì\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ìŠ¤íŠ¸ë¦¬ë°ì˜ íŠ¹ì§•:\")\n",
        "    print(\"  - ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ëŒ€ë¡œ ì¦‰ì‹œ í‘œì‹œ\")\n",
        "    print(\"  - ê¸´ ì‘ë‹µì—ì„œ íŠ¹íˆ ìœ ìš©\")\n",
        "    print(\"  - ì‚¬ìš©ìê°€ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ëŠë‚Œ ê°ì†Œ\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await streaming_example()\n",
        "\n",
        "# ============================================\n",
        "# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def streaming_with_processing():\n",
        "    \"\"\"\n",
        "    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì˜ˆì œ\n",
        "\n",
        "    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ìœ¼ë©´ì„œ ë™ì‹œì— ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    question = \"ë¨¸ì‹ ëŸ¬ë‹ì˜ ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ 3ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "    print(f\"\\nâ“ ì§ˆë¬¸: {question}\")\n",
        "\n",
        "    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ìœ¼ë©´ì„œ ì²˜ë¦¬\n",
        "    sentences = []\n",
        "    current_sentence = \"\"\n",
        "\n",
        "    print(\"\\nğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ë¬¸ì¥ ë‹¨ìœ„ ì²˜ë¦¬):\\n\")\n",
        "\n",
        "    # async for chunk in client.stream_chat(\n",
        "    #     messages=[{\"role\": \"user\", \"content\": question}]\n",
        "    # ):\n",
        "    #     current_sentence += chunk\n",
        "    #     print(chunk, end=\"\", flush=True)\n",
        "    #\n",
        "    #     # ë¬¸ì¥ì´ ì™„ì„±ë˜ë©´ ì²˜ë¦¬\n",
        "    #     if chunk in [\".\", \"!\", \"?\", \"\\n\"]:\n",
        "    #         if current_sentence.strip():\n",
        "    #             sentences.append(current_sentence.strip())\n",
        "    #             current_sentence = \"\"\n",
        "\n",
        "    # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬\n",
        "    # if current_sentence.strip():\n",
        "    #     sentences.append(current_sentence.strip())\n",
        "\n",
        "    print(f\"\\n\\nâœ… ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "    print(f\"ğŸ“Š ì¶”ì¶œëœ ë¬¸ì¥ ìˆ˜: {len(sentences)}ê°œ\")\n",
        "    # for i, sentence in enumerate(sentences[:5], 1):\n",
        "    #     print(f\"  {i}. {sentence[:80]}...\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í™œìš©:\")\n",
        "    print(\"  - ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ë¶„ì„\")\n",
        "    print(\"  - ë¬¸ì¥ ë‹¨ìœ„ ì²˜ë¦¬\")\n",
        "    print(\"  - í‚¤ì›Œë“œ ì¶”ì¶œ\")\n",
        "    print(\"  - ì§„í–‰ ìƒí™© í‘œì‹œ\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await streaming_with_processing()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘\n",
        "\n",
        "ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìˆ˜ì§‘í•˜ì—¬ ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def collect_streaming():\n",
        "    \"\"\"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìˆ˜ì§‘\"\"\"\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    chunks = []\n",
        "    async for chunk in client.stream_chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"ê°„ë‹¨í•œ ì‹œë¥¼ ì¨ì£¼ì„¸ìš”.\"}]\n",
        "    ):\n",
        "        chunks.append(chunk)\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "\n",
        "    full_text = \"\".join(chunks)\n",
        "    print(f\"\\n\\nğŸ“Š ìˆ˜ì§‘ëœ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
        "    print(f\"ğŸ“Š ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(full_text)}ì\")\n",
        "\n",
        "    return full_text\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# result = await collect_streaming()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. íŒŒë¼ë¯¸í„° ì¡°ì •\n",
        "\n",
        "LLMì˜ ì‘ë‹µì„ ì œì–´í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 4.1 Temperature (ì°½ì˜ì„±)\n",
        "\n",
        "`temperature`ëŠ” ì‘ë‹µì˜ ì°½ì˜ì„±ì„ ì œì–´í•©ë‹ˆë‹¤:\n",
        "- **ë‚®ì€ ê°’ (0.0-0.5)**: ì¼ê´€ë˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì‘ë‹µ\n",
        "- **ì¤‘ê°„ ê°’ (0.5-1.0)**: ê· í˜•ì¡íŒ ì‘ë‹µ\n",
        "- **ë†’ì€ ê°’ (1.0-2.0)**: ì°½ì˜ì ì´ê³  ë‹¤ì–‘í•œ ì‘ë‹µ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def temperature_comparison():\n",
        "    \"\"\"Temperature ë¹„êµ\"\"\"\n",
        "    question = \"ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "    temperatures = [0.1, 0.7, 1.5]\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\nğŸŒ¡ï¸ Temperature: {temp}\")\n",
        "        client = Client(model=\"gpt-4o\")\n",
        "        response = await client.chat(\n",
        "            messages=[{\"role\": \"user\", \"content\": question}],\n",
        "            temperature=temp,\n",
        "        )\n",
        "        print(f\"  {response.content}\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await temperature_comparison()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Max Tokens (ì‘ë‹µ ê¸¸ì´)\n",
        "\n",
        "`max_tokens`ëŠ” ì‘ë‹µì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì œí•œí•©ë‹ˆë‹¤.\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜**: Providerë§ˆë‹¤ íŒŒë¼ë¯¸í„° ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì§€ë§Œ, beanllmì´ ìë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n",
        "> - OpenAI: `max_tokens`\n",
        "> - Google: `max_output_tokens`\n",
        "> - Ollama: `num_predict`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def max_tokens_example():\n",
        "    \"\"\"Max Tokens ì˜ˆì œ\"\"\"\n",
        "    question = \"Pythonì˜ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"\n",
        "\n",
        "    # ì§§ì€ ì‘ë‹µ\n",
        "    print(\"ğŸ“ ì§§ì€ ì‘ë‹µ (max_tokens=50):\")\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "    response = await client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": question}],\n",
        "        max_tokens=50,\n",
        "    )\n",
        "    print(f\"  {response.content}\\n\")\n",
        "\n",
        "    # ê¸´ ì‘ë‹µ\n",
        "    print(\"ğŸ“ ê¸´ ì‘ë‹µ (max_tokens=500):\")\n",
        "    response = await client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": question}],\n",
        "        max_tokens=500,\n",
        "    )\n",
        "    print(f\"  {response.content[:200]}...\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await max_tokens_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 ê¸°íƒ€ íŒŒë¼ë¯¸í„°\n",
        "\n",
        "ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°í•©í•˜ì—¬ ì‘ë‹µì„ ì„¸ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def advanced_parameters():\n",
        "    \"\"\"ê³ ê¸‰ íŒŒë¼ë¯¸í„° ì˜ˆì œ\"\"\"\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    response = await client.chat(\n",
        "        messages=[{\"role\": \"user\", \"content\": \"ì°½ì˜ì ì¸ ìŠ¤í† ë¦¬ë¥¼ ì¨ì£¼ì„¸ìš”.\"}],\n",
        "        temperature=1.2,  # ì°½ì˜ì„± ë†’ìŒ\n",
        "        max_tokens=300,  # ìµœëŒ€ ê¸¸ì´\n",
        "        top_p=0.9,  # Nucleus sampling\n",
        "        frequency_penalty=0.5,  # ë°˜ë³µ ì–µì œ\n",
        "        presence_penalty=0.3,  # ì£¼ì œ ë‹¤ì–‘ì„±\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ“ ìƒì„±ëœ ìŠ¤í† ë¦¬:\")\n",
        "    print(response.content)\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await advanced_parameters()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Registry í™œìš©\n",
        "\n",
        "Model Registryë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ íƒìƒ‰í•˜ê³  ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 5.1 ëª¨ë¸ ì •ë³´ ì¡°íšŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import get_model_registry\n",
        "\n",
        "registry = get_model_registry()\n",
        "\n",
        "# íŠ¹ì • ëª¨ë¸ ì •ë³´\n",
        "model_name = \"gpt-4o\"\n",
        "model_info = registry.get_model(model_name)\n",
        "\n",
        "if model_info:\n",
        "    print(f\"ğŸ“Š ëª¨ë¸ ì •ë³´: {model_name}\")\n",
        "    print(f\"  - Provider: {model_info.provider}\")\n",
        "    print(f\"  - Context Window: {model_info.context_window:,} tokens\")\n",
        "    print(f\"  - Supports Streaming: {model_info.supports_streaming}\")\n",
        "    print(\n",
        "        f\"  - Max Tokens: {model_info.max_tokens if hasattr(model_info, 'max_tokens') else 'N/A'}\"\n",
        "    )\n",
        "else:\n",
        "    print(f\"âŒ ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 ëª¨ë¸ í•„í„°ë§\n",
        "\n",
        "íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Providerë³„ ëª¨ë¸\n",
        "print(\"ğŸ”µ OpenAI ëª¨ë¸:\")\n",
        "openai_models = registry.get_models_by_provider().get(\"openai\", [])\n",
        "for model in openai_models[:5]:\n",
        "    print(f\"  - {model.name}\")\n",
        "\n",
        "# ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ëª¨ë¸\n",
        "print(\"\\nğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ëª¨ë¸:\")\n",
        "all_models = registry.get_all_models()\n",
        "streaming_models = [m for m in all_models if m.supports_streaming]\n",
        "print(f\"  ì´ {len(streaming_models)}ê°œ ëª¨ë¸ì´ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ê³ ê¸‰ ê¸°ëŠ¥\n",
        "\n",
        "### 6.1 ë¹„ìš© ì¶”ì \n",
        "\n",
        "í† í° ì‚¬ìš©ëŸ‰ê³¼ ë¹„ìš©ì„ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import estimate_cost, count_tokens\n",
        "\n",
        "\n",
        "async def cost_tracking():\n",
        "    \"\"\"ë¹„ìš© ì¶”ì  ì˜ˆì œ\"\"\"\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": \"Pythonì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}]\n",
        "\n",
        "    # í† í° ìˆ˜ ê³„ì‚°\n",
        "    input_tokens = count_tokens(str(messages), model=\"gpt-4o\")\n",
        "    print(f\"ğŸ“Š ì…ë ¥ í† í°: {input_tokens}\")\n",
        "\n",
        "    # ì‘ë‹µ ë°›ê¸°\n",
        "    response = await client.chat(messages=messages)\n",
        "\n",
        "    # ì¶œë ¥ í† í° ê³„ì‚°\n",
        "    output_tokens = count_tokens(response.content, model=\"gpt-4o\")\n",
        "    print(f\"ğŸ“Š ì¶œë ¥ í† í°: {output_tokens}\")\n",
        "\n",
        "    # ë¹„ìš© ì¶”ì •\n",
        "    cost = estimate_cost(\n",
        "        model=\"gpt-4o\",\n",
        "        input_tokens=input_tokens,\n",
        "        output_tokens=output_tokens,\n",
        "    )\n",
        "    print(f\"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${cost:.6f}\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await cost_tracking()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 ì—ëŸ¬ ì²˜ë¦¬\n",
        "\n",
        "ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” ì—ëŸ¬ ì²˜ë¦¬ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def error_handling_example():\n",
        "    \"\"\"ì—ëŸ¬ ì²˜ë¦¬ ì˜ˆì œ\"\"\"\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    try:\n",
        "        response = await client.chat(\n",
        "            messages=[{\"role\": \"user\", \"content\": \"í…ŒìŠ¤íŠ¸\"}],\n",
        "            max_tokens=10,  # ë§¤ìš° ì§§ì€ ì‘ë‹µ\n",
        "        )\n",
        "        print(f\"âœ… ì„±ê³µ: {response.content}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ì—ëŸ¬ ë°œìƒ: {type(e).__name__}\")\n",
        "        print(f\"   ë©”ì‹œì§€: {str(e)}\")\n",
        "        print(\"\\nğŸ’¡ ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:\")\n",
        "        print(\"  - API í‚¤ í™•ì¸\")\n",
        "        print(\"  - ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸\")\n",
        "        print(\"  - Rate Limit í™•ì¸\")\n",
        "        print(\"  - ëª¨ë¸ ì´ë¦„ í™•ì¸\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await error_handling_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¸ ì‹œê°í™” ì˜ˆì œ\n",
        "\n",
        "> ğŸ’¡ **ì´ë¯¸ì§€ í•„ìš”**: ì•„ë˜ëŠ” ì‹œê°í™” ì˜ˆì œì…ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš© ì‹œ matplotlibì´ë‚˜ plotlyë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n",
        "\n",
        "### í† í° ì‚¬ìš©ëŸ‰ ì‹œê°í™”\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ì˜ˆì œ ë°ì´í„°\n",
        "models = [\"gpt-4o\", \"claude-sonnet-4.5\", \"gemini-2.5-pro\"]\n",
        "tokens = [150, 180, 200]\n",
        "\n",
        "plt.bar(models, tokens)\n",
        "plt.title(\"í† í° ì‚¬ìš©ëŸ‰ ë¹„êµ\")\n",
        "plt.ylabel(\"í† í° ìˆ˜\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**í•„ìš”í•œ ì´ë¯¸ì§€**: \n",
        "- ë§‰ëŒ€ ê·¸ë˜í”„: ê° ëª¨ë¸ë³„ í† í° ì‚¬ìš©ëŸ‰ ë¹„êµ\n",
        "- Xì¶•: ëª¨ë¸ ì´ë¦„\n",
        "- Yì¶•: í† í° ìˆ˜\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "Client ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŠœí† ë¦¬ì–¼ë¡œ ì§„í–‰í•˜ì„¸ìš”:\n",
        "\n",
        "### ğŸ“š ì¶”ì²œ í•™ìŠµ ìˆœì„œ\n",
        "\n",
        "1. **[03_rag_and_documents.ipynb](03_rag_and_documents.ipynb)** - RAG ë° ë¬¸ì„œ ì²˜ë¦¬\n",
        "   - ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬\n",
        "   - RAGChainìœ¼ë¡œ ì§€ì‹ ê¸°ë°˜ ì‘ë‹µ ìƒì„±\n",
        "\n",
        "2. **[04_embeddings_vector_stores.ipynb](04_embeddings_vector_stores.ipynb)** - ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´\n",
        "   - í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
        "   - ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥ ë° ê²€ìƒ‰\n",
        "\n",
        "3. **[05_agent_and_tools.ipynb](05_agent_and_tools.ipynb)** - Agent ë° Tools\n",
        "   - Toolì„ ì‚¬ìš©í•˜ëŠ” Agent ìƒì„±\n",
        "   - ë³µì¡í•œ ì‘ì—… ìë™í™”\n",
        "\n",
        "### ğŸ”— ê´€ë ¨ ë¬¸ì„œ\n",
        "\n",
        "- [API Reference](../API_REFERENCE.md#client) - Client API ìƒì„¸ ë¬¸ì„œ\n",
        "- [README.md](../../README.md) - í”„ë¡œì íŠ¸ ê°œìš”\n",
        "\n",
        "---\n",
        "\n",
        "**âœ… Client ê¸°ë³¸ ì‚¬ìš©ë²• ì™„ë£Œ! ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
