{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  Embeddings & Vector Stores - í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ beanllmì˜ ì„ë² ë”© ìƒì„±ê³¼ ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“‹ ëª©ì°¨\n",
        "\n",
        "- [1. ì„ë² ë”© ê¸°ë³¸](#1-ì„ë² ë”©-ê¸°ë³¸)\n",
        "- [2. ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸](#2-ë‹¤ì–‘í•œ-ì„ë² ë”©-ëª¨ë¸)\n",
        "- [3. ìœ ì‚¬ë„ ê³„ì‚°](#3-ìœ ì‚¬ë„-ê³„ì‚°)\n",
        "- [4. ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©](#4-ë²¡í„°-ìŠ¤í† ì–´-ì‚¬ìš©)\n",
        "- [5. ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë²•](#5-ê³ ê¸‰-ê²€ìƒ‰-ê¸°ë²•)\n",
        "- [6. ì‹¤ì „ ì˜ˆì œ](#6-ì‹¤ì „-ì˜ˆì œ)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•˜ë©´:\n",
        "- âœ… í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… MMR, Hybrid Search ë“± ê³ ê¸‰ ê¸°ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "- [01_setup_and_installation.ipynb](01_setup_and_installation.ipynb) ì™„ë£Œ\n",
        "- [02_core_client.ipynb](02_core_client.ipynb) ì™„ë£Œ\n",
        "- [03_rag_and_documents.ipynb](03_rag_and_documents.ipynb) ì™„ë£Œ (ê¶Œì¥)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. ì„ë² ë”© ê¸°ë³¸\n",
        "\n",
        "ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ë²¡í„°ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ë„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë©ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ì„ë² ë”©ì˜ í•µì‹¬**:\n",
        "> - **ì˜ë¯¸ì  í‘œí˜„**: ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ëŠ” ë¹„ìŠ·í•œ ë²¡í„°ë¥¼ ê°€ì§‘ë‹ˆë‹¤\n",
        "> - **ìœ ì‚¬ë„ ì¸¡ì •**: ë²¡í„° ê°„ ê±°ë¦¬(ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ìœ í´ë¦¬ë“œ ê±°ë¦¬)ë¡œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "> - **ë‹¤ì–‘í•œ í™œìš©**: RAG, ê²€ìƒ‰, ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„°ë§ ë“± ë‹¤ì–‘í•œ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤\n",
        "> - **ê³ ì°¨ì› ë²¡í„°**: ì¼ë°˜ì ìœ¼ë¡œ 128-1536 ì°¨ì›ì˜ ë²¡í„°ë¡œ í‘œí˜„ë©ë‹ˆë‹¤\n",
        "\n",
        "> ğŸ’¡ **ì„ë² ë”© ëª¨ë¸ ì„ íƒ**:\n",
        "> - **API ê¸°ë°˜**: OpenAI, Gemini ë“± (ë¹ ë¥´ê³  ì •í™•, ë¹„ìš© ë°œìƒ)\n",
        "> - **ë¡œì»¬ ëª¨ë¸**: HuggingFace, Ollama ë“± (ë¬´ë£Œ, í”„ë¼ì´ë²„ì‹œ, ëŠë¦´ ìˆ˜ ìˆìŒ)\n",
        "> - **ìš©ë„ë³„**: ì¼ë°˜ í…ìŠ¤íŠ¸, ì½”ë“œ, ë‹¤êµ­ì–´ ë“± íŠ¹í™” ëª¨ë¸ ì¡´ì¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import numpy as np\n",
        "from beanllm import Embedding\n",
        "\n",
        "# ============================================\n",
        "# ê¸°ë³¸ ì„ë² ë”© ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def basic_embedding():\n",
        "    \"\"\"\n",
        "    ê¸°ë³¸ ì„ë² ë”© ì˜ˆì œ\n",
        "\n",
        "    ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ê¸°ë³¸ ì„ë² ë”© ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Embedding ìƒì„±\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "    # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
        "    text = \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"\n",
        "    print(f\"\\nğŸ“ ì›ë³¸ í…ìŠ¤íŠ¸: {text}\")\n",
        "\n",
        "    # ì„ë² ë”© ìƒì„±\n",
        "    # vector = await embedding.embed(text)\n",
        "\n",
        "    # ê²°ê³¼ ë¶„ì„\n",
        "    # print(f\"\\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ!\")\n",
        "    # print(f\"ğŸ“Š ë²¡í„° ì°¨ì›: {len(vector)}ì°¨ì›\")\n",
        "    # print(f\"ğŸ“Š ë²¡í„° (ì²˜ìŒ 10ê°œ ê°’): {vector[:10]}\")\n",
        "    #\n",
        "    # # ë²¡í„° í†µê³„\n",
        "    # vector_array = np.array(vector)\n",
        "    # print(f\"\\nğŸ“Š ë²¡í„° í†µê³„:\")\n",
        "    # print(f\"  - ìµœì†Œê°’: {vector_array.min():.4f}\")\n",
        "    # print(f\"  - ìµœëŒ€ê°’: {vector_array.max():.4f}\")\n",
        "    # print(f\"  - í‰ê· : {vector_array.mean():.4f}\")\n",
        "    # print(f\"  - í‘œì¤€í¸ì°¨: {vector_array.std():.4f}\")\n",
        "    #\n",
        "    # # ë²¡í„° í¬ê¸° (L2 norm)\n",
        "    # norm = np.linalg.norm(vector_array)\n",
        "    # print(f\"  - ë²¡í„° í¬ê¸° (L2 norm): {norm:.4f}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ì„ë² ë”© ë²¡í„°ì˜ íŠ¹ì§•:\")\n",
        "    print(\"  - ê³ ì°¨ì› ë²¡í„° (ì¼ë°˜ì ìœ¼ë¡œ 128-1536 ì°¨ì›)\")\n",
        "    print(\"  - ì •ê·œí™”ëœ ë²¡í„° (í¬ê¸°ê°€ 1ì— ê°€ê¹Œì›€)\")\n",
        "    print(\"  - ì˜ë¯¸ì  ìœ ì‚¬ë„ê°€ ê±°ë¦¬ë¡œ í‘œí˜„ë¨\")\n",
        "    print(\"  - ë²¡í„° ì—°ì‚° ê°€ëŠ¥ (ë§ì…ˆ, ëº„ì…ˆ ë“±)\")\n",
        "\n",
        "    # return vector\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# vector = await basic_embedding()\n",
        "\n",
        "# ============================================\n",
        "# ì„ë² ë”© ë²¡í„° ë¶„ì„\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def embedding_analysis():\n",
        "    \"\"\"\n",
        "    ì„ë² ë”© ë²¡í„° ë¶„ì„ ì˜ˆì œ\n",
        "\n",
        "    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë¹„êµí•˜ì—¬ ìœ ì‚¬ë„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ì„ë² ë”© ë²¡í„° ë¶„ì„ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "    # ìœ ì‚¬í•œ ì˜ë¯¸ì˜ í…ìŠ¤íŠ¸ë“¤\n",
        "    texts = [\n",
        "        \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"Pythonì€ ì½”ë”©ì— ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"JavaëŠ” ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ìŠµë‹ˆë‹¤.\",\n",
        "    ]\n",
        "\n",
        "    print(\"\\nğŸ“ ë¹„êµí•  í…ìŠ¤íŠ¸:\")\n",
        "    for i, text in enumerate(texts, 1):\n",
        "        print(f\"  {i}. {text}\")\n",
        "\n",
        "    # ê° í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
        "    # vectors = []\n",
        "    # for text in texts:\n",
        "    #     vector = await embedding.embed(text)\n",
        "    #     vectors.append(vector)\n",
        "    #     print(f\"\\n  í…ìŠ¤íŠ¸ {texts.index(text) + 1} ì„ë² ë”© ì™„ë£Œ\")\n",
        "\n",
        "    # ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
        "    # from sklearn.metrics.pairwise import cosine_similarity\n",
        "    #\n",
        "    # print(\"\\nğŸ“Š í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„):\")\n",
        "    # print(\"     \", end=\"\")\n",
        "    # for i in range(len(texts)):\n",
        "    #     print(f\"  í…ìŠ¤íŠ¸{i+1}\", end=\"\")\n",
        "    # print()\n",
        "    #\n",
        "    # for i, vec1 in enumerate(vectors):\n",
        "    #     print(f\"í…ìŠ¤íŠ¸{i+1}\", end=\"\")\n",
        "    #     for j, vec2 in enumerate(vectors):\n",
        "    #         similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "    #         print(f\"  {similarity:.3f}\", end=\"\")\n",
        "    #     print()\n",
        "\n",
        "    print(\"\\nğŸ’¡ ìœ ì‚¬ë„ ë¶„ì„ ê²°ê³¼ í•´ì„:\")\n",
        "    print(\"  - 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•œ ì˜ë¯¸\")\n",
        "    print(\"  - 0.5-0.7: ê´€ë ¨ ìˆì§€ë§Œ ë‹¤ë¦„\")\n",
        "    print(\"  - 0.5 ë¯¸ë§Œ: ê±°ì˜ ê´€ë ¨ ì—†ìŒ\")\n",
        "    print(\"  - Python ê´€ë ¨ í…ìŠ¤íŠ¸ë“¤ì€ ë†’ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§\")\n",
        "    print(\"  - ë‚ ì”¨ ê´€ë ¨ í…ìŠ¤íŠ¸ëŠ” ë‚®ì€ ìœ ì‚¬ë„ë¥¼ ê°€ì§\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await embedding_analysis()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ê¸°ë³¸ ì„ë² ë”© í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 ë°°ì¹˜ ì„ë² ë”©\n",
        "\n",
        "ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì„ë² ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë‹¨ì¼ ìš”ì²­ë³´ë‹¤ í›¨ì”¬ íš¨ìœ¨ì ì´ë©°, API ë¹„ìš©ë„ ì ˆê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ë°°ì¹˜ ì„ë² ë”©ì˜ ì¥ì **:\n",
        "> - **íš¨ìœ¨ì„±**: ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ì‹œê°„ ì ˆì•½\n",
        "> - **ë¹„ìš© ì ˆê°**: API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œ\n",
        "> - **ì¼ê´€ì„±**: ë™ì¼í•œ ëª¨ë¸ê³¼ ì„¤ì •ìœ¼ë¡œ ì¼ê´„ ì²˜ë¦¬\n",
        "> - **ë³‘ë ¬ ì²˜ë¦¬**: ë‚´ë¶€ì ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥\n",
        "\n",
        "> âš ï¸ **ì£¼ì˜ì‚¬í•­**:\n",
        "> - ë°°ì¹˜ í¬ê¸°ëŠ” ëª¨ë¸ê³¼ API ì œí•œì— ë”°ë¼ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤\n",
        "> - ì¼ë°˜ì ìœ¼ë¡œ 100-1000ê°œ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ë°°ì¹˜ ì„ë² ë”© ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "async def batch_embedding():\n",
        "    \"\"\"\n",
        "    ë°°ì¹˜ ì„ë² ë”© ì˜ˆì œ\n",
        "    \n",
        "    ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì„ë² ë”©í•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ë°°ì¹˜ ì„ë² ë”© ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "    \n",
        "    # ì—¬ëŸ¬ í…ìŠ¤íŠ¸\n",
        "    texts = [\n",
        "        \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"JavaëŠ” ê°ì²´ì§€í–¥ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"JavaScriptëŠ” ì›¹ ê°œë°œ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"C++ëŠ” ì‹œìŠ¤í…œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"GoëŠ” êµ¬ê¸€ì—ì„œ ê°œë°œí•œ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "    ]\n",
        "    \n",
        "    print(f\"\\nğŸ“ ì„ë² ë”©í•  í…ìŠ¤íŠ¸: {len(texts)}ê°œ\")\n",
        "    for i, text in enumerate(texts, 1):\n",
        "        print(f\"  {i}. {text}\")\n",
        "    \n",
        "    # ë°°ì¹˜ ì„ë² ë”©\n",
        "    print(\"\\n[ë°°ì¹˜ ì„ë² ë”© ìˆ˜í–‰]...\")\n",
        "    # vectors = await embedding.embed_batch(texts)\n",
        "    # \n",
        "    # print(f\"âœ… ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ!\")\n",
        "    # print(f\"ğŸ“Š ìƒì„±ëœ ë²¡í„° ìˆ˜: {len(vectors)}ê°œ\")\n",
        "    # print(f\"ğŸ“Š ë²¡í„° ì°¨ì›: {len(vectors[0]) if vectors else 0}ì°¨ì›\")\n",
        "    \n",
        "    # ê° ë²¡í„° ì •ë³´\n",
        "    # print(\"\\nğŸ“Š ê° ë²¡í„° ì •ë³´:\")\n",
        "    # for i, (text, vector) in enumerate(zip(texts, vectors), 1):\n",
        "    #     norm = np.linalg.norm(vector)\n",
        "    #     print(f\"  {i}. {text[:30]}...\")\n",
        "    #     print(f\"     ë²¡í„° í¬ê¸°: {norm:.4f}\")\n",
        "    \n",
        "    print(\"\\nğŸ’¡ ë°°ì¹˜ ì„ë² ë”©ì˜ ì¥ì :\")\n",
        "    print(\"  - ë‹¨ì¼ ìš”ì²­ìœ¼ë¡œ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ì²˜ë¦¬\")\n",
        "    print(\"  - API í˜¸ì¶œ íšŸìˆ˜ ê°ì†Œë¡œ ë¹„ìš© ì ˆê°\")\n",
        "    print(\"  - ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•\")\n",
        "    print(\"  - ì¼ê´€ëœ ì„ë² ë”© ìƒì„±\")\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await batch_embedding()\n",
        "\n",
        "# ============================================\n",
        "# ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "async def large_batch_embedding():\n",
        "    \"\"\"\n",
        "    ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ\n",
        "    \n",
        "    ë§ì€ í…ìŠ¤íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "    \n",
        "    # ë§ì€ í…ìŠ¤íŠ¸ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)\n",
        "    # texts = [f\"ë¬¸ì„œ {i}ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.\" for i in range(100)]\n",
        "    # print(f\"\\nğŸ“ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸: {len(texts)}ê°œ\")\n",
        "    \n",
        "    # ë°°ì¹˜ í¬ê¸° ì„¤ì •\n",
        "    batch_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ìˆ˜\n",
        "    \n",
        "    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬\n",
        "    # all_vectors = []\n",
        "    # total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "    # \n",
        "    # print(f\"\\n[ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘]\")\n",
        "    # print(f\"  ì´ í…ìŠ¤íŠ¸: {len(texts)}ê°œ\")\n",
        "    # print(f\"  ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ\")\n",
        "    # print(f\"  ì´ ë°°ì¹˜ ìˆ˜: {total_batches}ê°œ\")\n",
        "    # \n",
        "    # for i in range(0, len(texts), batch_size):\n",
        "    #     batch = texts[i:i + batch_size]\n",
        "    #     batch_num = i // batch_size + 1\n",
        "    #     \n",
        "    #     print(f\"\\n  [ë°°ì¹˜ {batch_num}/{total_batches}] ì²˜ë¦¬ ì¤‘...\")\n",
        "    #     vectors = await embedding.embed_batch(batch)\n",
        "    #     all_vectors.extend(vectors)\n",
        "    #     print(f\"    âœ… ì™„ë£Œ: {len(vectors)}ê°œ ë²¡í„° ìƒì„±\")\n",
        "    # \n",
        "    # print(f\"\\nâœ… ì „ì²´ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "    # print(f\"ğŸ“Š ì´ ìƒì„±ëœ ë²¡í„°: {len(all_vectors)}ê°œ\")\n",
        "    \n",
        "    print(\"\\nğŸ’¡ ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ íŒ:\")\n",
        "    print(\"  - ë°°ì¹˜ í¬ê¸°ëŠ” API ì œí•œì— ë§ê²Œ ì¡°ì •\")\n",
        "    print(\"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤\")\n",
        "    print(\"  - ì§„í–‰ ìƒí™© ì¶”ì  (í”„ë¡œê·¸ë ˆìŠ¤ ë°”)\")\n",
        "    print(\"  - ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€\")\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await large_batch_embedding()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ë°°ì¹˜ ì„ë² ë”© í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "    # ë°°ì¹˜ ì„ë² ë”©\n",
        "    vectors = await embedding.embed_batch(texts)\n",
        "\n",
        "    print(f\"âœ… í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}\")\n",
        "    print(f\"âœ… ë²¡í„° ìˆ˜: {len(vectors)}\")\n",
        "    print(f\"ğŸ“Š ê° ë²¡í„° ì°¨ì›: {len(vectors[0])}\")\n",
        "\n",
        "    # ê° í…ìŠ¤íŠ¸ì™€ ë²¡í„° ì¶œë ¥\n",
        "    for i, (text, vec) in enumerate(zip(texts, vectors), 1):\n",
        "        print(f\"\\n{i}. {text}\")\n",
        "        print(f\"   ë²¡í„° (ì²˜ìŒ 5ê°œ): {vec[:5]}\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await batch_embedding()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸\n",
        "\n",
        "beanllmì€ ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 2.1 API ê¸°ë°˜ ì„ë² ë”©\n",
        "\n",
        "í´ë¼ìš°ë“œ APIë¥¼ ì‚¬ìš©í•˜ëŠ” ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸\n",
        "api_models = {\n",
        "    \"text-embedding-3-small\": \"OpenAI - ë¹ ë¥´ê³  ì €ë ´\",\n",
        "    \"text-embedding-3-large\": \"OpenAI - ë†’ì€ ì •í™•ë„\",\n",
        "    \"text-embedding-ada-002\": \"OpenAI - ì´ì „ ë²„ì „\",\n",
        "    \"gemini-embedding\": \"Google Gemini - ë‹¤êµ­ì–´ ì§€ì›\",\n",
        "    \"voyage-2\": \"Voyage AI - ê³ ì„±ëŠ¥\",\n",
        "    \"jina-embeddings-v2\": \"Jina AI - ë‹¤êµ­ì–´\",\n",
        "    \"mistral-embed\": \"Mistral AI - ì˜¤í”ˆì†ŒìŠ¤\",\n",
        "    \"cohere-embed\": \"Cohere - ì—”í„°í”„ë¼ì´ì¦ˆ\",\n",
        "}\n",
        "\n",
        "print(\"ğŸŒ API ê¸°ë°˜ ì„ë² ë”© ëª¨ë¸:\")\n",
        "for model, description in api_models.items():\n",
        "    print(f\"  - {model}: {description}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ì‚¬ìš© ì˜ˆì œ:\")\n",
        "print(\"embedding = Embedding(model='text-embedding-3-small')\")\n",
        "print(\"vector = await embedding.embed('í…ìŠ¤íŠ¸')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 ë¡œì»¬ ì„ë² ë”© ëª¨ë¸\n",
        "\n",
        "ë¡œì»¬ì—ì„œ ì‹¤í–‰ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤. API ë¹„ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¡œì»¬ ì„ë² ë”© ëª¨ë¸\n",
        "local_models = {\n",
        "    \"Qwen3-Embedding-8B\": \"Qwen - ë‹¤êµ­ì–´, ê³ ì„±ëŠ¥\",\n",
        "    \"NV-Embed-v2\": \"NVIDIA - ì½”ë“œ íŠ¹í™”\",\n",
        "    \"Code Embedding\": \"ì½”ë“œ ê²€ìƒ‰ íŠ¹í™”\",\n",
        "    \"HuggingFace Models\": \"ë‹¤ì–‘í•œ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸\",\n",
        "}\n",
        "\n",
        "print(\"ğŸ’» ë¡œì»¬ ì„ë² ë”© ëª¨ë¸:\")\n",
        "for model, description in local_models.items():\n",
        "    print(f\"  - {model}: {description}\")\n",
        "\n",
        "print(\"\\nğŸ’¡ ì‚¬ìš© ì˜ˆì œ:\")\n",
        "print(\"from beanllm.domain.embeddings.local import HuggingFaceEmbedding\")\n",
        "print(\"embedding = HuggingFaceEmbedding(model='sentence-transformers/all-MiniLM-L6-v2')\")\n",
        "print(\"vector = embedding.embed_sync('í…ìŠ¤íŠ¸')\")\n",
        "\n",
        "print(\"\\nâš ï¸ ì£¼ì˜:\")\n",
        "print(\"  - GPU ê¶Œì¥ (ë¹ ë¥¸ ì²˜ë¦¬)\")\n",
        "print(\"  - ì²« ë¡œë”© ì‹œ ì‹œê°„ ì†Œìš”\")\n",
        "print(\"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ìœ ì‚¬ë„ ê³„ì‚°\n",
        "\n",
        "ì„ë² ë”© ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ë¹„ìŠ·í•œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### 3.1 ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
        "\n",
        "ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ìœ ì‚¬ë„ ì¸¡ì • ë°©ë²•ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import Embedding, cosine_similarity\n",
        "\n",
        "\n",
        "async def similarity_example():\n",
        "    \"\"\"ìœ ì‚¬ë„ ê³„ì‚° ì˜ˆì œ\"\"\"\n",
        "\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ë“¤\n",
        "    texts = [\n",
        "        \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"Pythonì€ ê°„ê²°í•œ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\",\n",
        "        \"JavaëŠ” ê°ì²´ì§€í–¥ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"ê³ ì–‘ì´ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì…ë‹ˆë‹¤.\",\n",
        "    ]\n",
        "\n",
        "    # ì„ë² ë”© ìƒì„±\n",
        "    vectors = await embedding.embed_batch(texts)\n",
        "\n",
        "    # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ì™€ ë‚˜ë¨¸ì§€ ë¹„êµ\n",
        "    base_vector = vectors[0]\n",
        "    base_text = texts[0]\n",
        "\n",
        "    print(f\"ğŸ“Š ê¸°ì¤€ í…ìŠ¤íŠ¸: {base_text}\\n\")\n",
        "    print(\"ğŸ“Š ìœ ì‚¬ë„ ë¹„êµ:\")\n",
        "    for i, (text, vec) in enumerate(zip(texts[1:], vectors[1:]), 2):\n",
        "        similarity = cosine_similarity(base_vector, vec)\n",
        "        print(f\"  {i}. {text}\")\n",
        "        print(f\"     ìœ ì‚¬ë„: {similarity:.3f}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await similarity_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 ìœ í´ë¦¬ë“œ ê±°ë¦¬\n",
        "\n",
        "ë²¡í„° ê°„ ê±°ë¦¬ë¡œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import euclidean_distance\n",
        "\n",
        "\n",
        "async def distance_example():\n",
        "    \"\"\"ê±°ë¦¬ ê³„ì‚° ì˜ˆì œ\"\"\"\n",
        "\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "    text1 = \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"\n",
        "    text2 = \"JavaëŠ” í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "    vec1 = await embedding.embed(text1)\n",
        "    vec2 = await embedding.embed(text2)\n",
        "\n",
        "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)\n",
        "    similarity = cosine_similarity(vec1, vec2)\n",
        "\n",
        "    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)\n",
        "    distance = euclidean_distance(vec1, vec2)\n",
        "\n",
        "    print(f\"ğŸ“Š í…ìŠ¤íŠ¸ 1: {text1}\")\n",
        "    print(f\"ğŸ“Š í…ìŠ¤íŠ¸ 2: {text2}\")\n",
        "    print(f\"\\nâœ… ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {similarity:.3f} (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)\")\n",
        "    print(f\"âœ… ìœ í´ë¦¬ë“œ ê±°ë¦¬: {distance:.3f} (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await distance_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. ë²¡í„° ìŠ¤í† ì–´ ì‚¬ìš©\n",
        "\n",
        "ë²¡í„° ìŠ¤í† ì–´ëŠ” ì„ë² ë”© ë²¡í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "\n",
        "### 4.1 ChromaDB ì‚¬ìš©\n",
        "\n",
        "ê°€ì¥ ê°„ë‹¨í•œ ë²¡í„° ìŠ¤í† ì–´ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import ChromaVectorStore, Embedding, Document\n",
        "\n",
        "\n",
        "async def chroma_example():\n",
        "    \"\"\"ChromaDB ì˜ˆì œ\"\"\"\n",
        "\n",
        "    # ì„ë² ë”© í•¨ìˆ˜ ì¤€ë¹„\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "    embed_func = lambda texts: (\n",
        "        embedding.embed_sync(texts)\n",
        "        if hasattr(embedding, \"embed_sync\")\n",
        "        else [await embedding.embed(t) for t in texts]\n",
        "    )\n",
        "\n",
        "    # Vector Store ìƒì„±\n",
        "    vector_store = ChromaVectorStore(\n",
        "        collection_name=\"test_collection\",\n",
        "        embedding_function=embed_func,\n",
        "    )\n",
        "\n",
        "    # ë¬¸ì„œ ì¶”ê°€\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaëŠ” ê°ì²´ì§€í–¥ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaScriptëŠ” ì›¹ ê°œë°œ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    ids = vector_store.add_documents(documents)\n",
        "    print(f\"âœ… ë¬¸ì„œ ì¶”ê°€ ì™„ë£Œ: {len(ids)}ê°œ\")\n",
        "\n",
        "    # ìœ ì‚¬ë„ ê²€ìƒ‰\n",
        "    query = \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\"\n",
        "    results = vector_store.similarity_search(query, k=2)\n",
        "\n",
        "    print(f\"\\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: {query}\")\n",
        "    print(f\"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ\\n\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{i}. {result.document.content}\")\n",
        "        print(f\"   ìœ ì‚¬ë„: {result.score:.3f}\\n\")\n",
        "\n",
        "    return vector_store\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# store = await chroma_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 from_documents í¸ì˜ í•¨ìˆ˜\n",
        "\n",
        "ë¬¸ì„œì—ì„œ ì§ì ‘ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import from_documents, Document\n",
        "\n",
        "\n",
        "async def from_documents_example():\n",
        "    \"\"\"from_documents ì˜ˆì œ\"\"\"\n",
        "\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ ê°„ê²°í•œ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaëŠ” ê°•ë ¥í•œ íƒ€ì… ì‹œìŠ¤í…œì„ ê°€ì§‘ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaScriptëŠ” ë™ì  íƒ€ì… ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    # í•œ ì¤„ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
        "    vector_store = await from_documents(\n",
        "        documents=documents,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "        vector_store=\"chroma\",\n",
        "    )\n",
        "\n",
        "    print(\"âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!\")\n",
        "\n",
        "    # ê²€ìƒ‰\n",
        "    results = vector_store.similarity_search(\"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\", k=2)\n",
        "    print(f\"\\nğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ\")\n",
        "    for result in results:\n",
        "        print(f\"  - {result.document.content[:50]}...\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await from_documents_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë²•\n",
        "\n",
        "### 5.1 MMR (Maximal Marginal Relevance)\n",
        "\n",
        "ê²€ìƒ‰ ê²°ê³¼ì˜ ë‹¤ì–‘ì„±ì„ ë†’ì´ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import mmr_search, Embedding\n",
        "\n",
        "\n",
        "async def mmr_example():\n",
        "    \"\"\"MMR ê²€ìƒ‰ ì˜ˆì œ\"\"\"\n",
        "\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "    # ë¬¸ì„œë“¤\n",
        "    documents = [\n",
        "        Document(content=\"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"Pythonì€ ê°„ê²°í•œ ë¬¸ë²•ì´ íŠ¹ì§•ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"Pythonì€ ë°ì´í„° ê³¼í•™ì— ë§ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaëŠ” ê°ì²´ì§€í–¥ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"JavaScriptëŠ” ì›¹ ê°œë°œ ì–¸ì–´ì…ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    # ì„ë² ë”© ìƒì„±\n",
        "    texts = [doc.content for doc in documents]\n",
        "    vectors = await embedding.embed_batch(texts)\n",
        "\n",
        "    # MMR ê²€ìƒ‰\n",
        "    query = \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\"\n",
        "    query_vector = await embedding.embed(query)\n",
        "\n",
        "    results = mmr_search(\n",
        "        query_vector=query_vector,\n",
        "        document_vectors=vectors,\n",
        "        documents=documents,\n",
        "        k=3,\n",
        "        lambda_param=0.5,  # ë‹¤ì–‘ì„± ì¡°ì ˆ (0.0=ìœ ì‚¬ë„ë§Œ, 1.0=ë‹¤ì–‘ì„±ë§Œ)\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ” MMR ê²€ìƒ‰ ê²°ê³¼ (ì¿¼ë¦¬: {query}):\\n\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{i}. {result.content}\")\n",
        "        print(f\"   ìœ ì‚¬ë„: {result.score:.3f}\\n\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await mmr_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Hard Negative Mining\n",
        "\n",
        "ëŒ€ì¡° í•™ìŠµì„ ìœ„í•œ Hard Negativeë¥¼ ì°¾ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import find_hard_negatives, Embedding\n",
        "\n",
        "\n",
        "async def hard_negative_example():\n",
        "    \"\"\"Hard Negative Mining ì˜ˆì œ\"\"\"\n",
        "\n",
        "    embedding = Embedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "    # Positive ì˜ˆì œ\n",
        "    positive = \"Pythonì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.\"\n",
        "\n",
        "    # Candidate ë¬¸ì„œë“¤\n",
        "    candidates = [\n",
        "        \"JavaëŠ” ê°ì²´ì§€í–¥ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"JavaScriptëŠ” ì›¹ ê°œë°œ ì–¸ì–´ì…ë‹ˆë‹¤.\",\n",
        "        \"ê³ ì–‘ì´ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ì…ë‹ˆë‹¤.\",\n",
        "        \"ì»´í“¨í„°ëŠ” ì „ì ê¸°ê¸°ì…ë‹ˆë‹¤.\",\n",
        "    ]\n",
        "\n",
        "    # Hard Negative ì°¾ê¸°\n",
        "    hard_negatives = find_hard_negatives(\n",
        "        positive_text=positive,\n",
        "        candidate_texts=candidates,\n",
        "        embedding_model=embedding,\n",
        "        top_k=2,  # ìƒìœ„ 2ê°œ ì„ íƒ\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Positive: {positive}\\n\")\n",
        "    print(f\"ğŸ“Š Hard Negatives (ìƒìœ„ {len(hard_negatives)}ê°œ):\")\n",
        "    for i, neg in enumerate(hard_negatives, 1):\n",
        "        print(f\"  {i}. {neg}\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await hard_negative_example()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ì‹¤ì „ ì˜ˆì œ\n",
        "\n",
        "### 6.1 ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
        "\n",
        "ì„ë² ë”©ê³¼ ë²¡í„° ìŠ¤í† ì–´ë¥¼ í™œìš©í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•´ë´…ì‹œë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def search_system():\n",
        "    \"\"\"ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\"\"\"\n",
        "\n",
        "    # ì§€ì‹ ë² ì´ìŠ¤\n",
        "    knowledge_base = [\n",
        "        Document(content=\"beanllmì€ ë‹¤ì–‘í•œ LLM Providerë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"RAGëŠ” ë¬¸ì„œ ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•©ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\"),\n",
        "        Document(content=\"ë²¡í„° ìŠ¤í† ì–´ëŠ” ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"),\n",
        "    ]\n",
        "\n",
        "    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±\n",
        "    vector_store = await from_documents(\n",
        "        documents=knowledge_base,\n",
        "        embedding_model=\"text-embedding-3-small\",\n",
        "        vector_store=\"chroma\",\n",
        "    )\n",
        "\n",
        "    # ê²€ìƒ‰ ì¿¼ë¦¬ë“¤\n",
        "    queries = [\n",
        "        \"LLM Provider\",\n",
        "        \"ë¬¸ì„œ ê²€ìƒ‰\",\n",
        "        \"í…ìŠ¤íŠ¸ ë³€í™˜\",\n",
        "    ]\n",
        "\n",
        "    print(\"ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\\n\")\n",
        "    for query in queries:\n",
        "        results = vector_store.similarity_search(query, k=2)\n",
        "        print(f\"â“ ì¿¼ë¦¬: {query}\")\n",
        "        for i, result in enumerate(results, 1):\n",
        "            print(f\"  {i}. {result.document.content} (ìœ ì‚¬ë„: {result.score:.3f})\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await search_system()\n",
        "print(\"ğŸ’¡ ìœ„ì˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¸ ì‹œê°í™” ì˜ˆì œ\n",
        "\n",
        "### ì„ë² ë”© ë²¡í„° ì‹œê°í™”\n",
        "\n",
        "> ğŸ’¡ **ì´ë¯¸ì§€ í•„ìš”**: 2D ë˜ëŠ” 3D ê³µê°„ì— ì„ë² ë”© ë²¡í„°ë¥¼ ì‹œê°í™”í•œ ê·¸ë˜í”„\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "# ì„ë² ë”© ë²¡í„°ë“¤\n",
        "vectors = [...]  # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ë²¡í„°\n",
        "texts = [...]    # í•´ë‹¹ í…ìŠ¤íŠ¸ë“¤\n",
        "\n",
        "# t-SNEë¡œ 2D ë³€í™˜\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "vectors_2d = tsne.fit_transform(vectors)\n",
        "\n",
        "# ì‹œê°í™”\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
        "for i, text in enumerate(texts):\n",
        "    plt.annotate(text[:20], (vectors_2d[i, 0], vectors_2d[i, 1]))\n",
        "plt.title('ì„ë² ë”© ë²¡í„° ì‹œê°í™” (t-SNE)')\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**í•„ìš”í•œ ì´ë¯¸ì§€**:\n",
        "- ì‚°ì ë„: ê° í…ìŠ¤íŠ¸ê°€ 2D ê³µê°„ì— ì ìœ¼ë¡œ í‘œì‹œ\n",
        "- ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ê°€ê¹Œì´ ìœ„ì¹˜\n",
        "- í…ìŠ¤íŠ¸ ë¼ë²¨ í‘œì‹œ\n",
        "- ìƒ‰ìƒìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„ (ì„ íƒì )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "ì„ë² ë”©ê³¼ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŠœí† ë¦¬ì–¼ë¡œ ì§„í–‰í•˜ì„¸ìš”:\n",
        "\n",
        "### ğŸ“š ì¶”ì²œ í•™ìŠµ ìˆœì„œ\n",
        "\n",
        "1. **[05_agent_and_tools.ipynb](05_agent_and_tools.ipynb)** - Agent ë° Tools\n",
        "   - Toolì„ ì‚¬ìš©í•˜ëŠ” Agent\n",
        "   - ë³µì¡í•œ ì‘ì—… ìë™í™”\n",
        "\n",
        "2. **[06_chain_and_graph.ipynb](06_chain_and_graph.ipynb)** - Chain ë° Graph\n",
        "   - ì‘ì—… ì²´ì¸ êµ¬ì„±\n",
        "   - ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°\n",
        "\n",
        "3. **[08_vision_rag.ipynb](08_vision_rag.ipynb)** - Vision RAG\n",
        "   - ì´ë¯¸ì§€ ì„ë² ë”©\n",
        "   - ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰\n",
        "\n",
        "### ğŸ”— ê´€ë ¨ ë¬¸ì„œ\n",
        "\n",
        "- [API Reference](../API_REFERENCE.md#embeddings) - Embeddings API ìƒì„¸ ë¬¸ì„œ\n",
        "- [README.md](../../README.md) - í”„ë¡œì íŠ¸ ê°œìš”\n",
        "\n",
        "---\n",
        "\n",
        "**âœ… Embeddings & Vector Stores ì™„ë£Œ! ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
