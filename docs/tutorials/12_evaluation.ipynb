{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“Š Evaluation - ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ beanllmì˜ í‰ê°€ ê¸°ëŠ¥ì„ í•™ìŠµí•©ë‹ˆë‹¤. LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "## ğŸ“‹ ëª©ì°¨\n",
        "\n",
        "- [1. í‰ê°€ ê¸°ë³¸](#1-í‰ê°€-ê¸°ë³¸)\n",
        "- [2. ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸](#2-ë²¤ì¹˜ë§ˆí¬-í…ŒìŠ¤íŠ¸)\n",
        "- [3. ì»¤ìŠ¤í…€ í‰ê°€](#3-ì»¤ìŠ¤í…€-í‰ê°€)\n",
        "- [4. ì‹¤ì „ ì˜ˆì œ](#4-ì‹¤ì „-ì˜ˆì œ)\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•˜ë©´:\n",
        "- âœ… ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "- âœ… ëª¨ë¸ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“š ì‚¬ì „ ìš”êµ¬ì‚¬í•­\n",
        "\n",
        "- [01_setup_and_installation.ipynb](01_setup_and_installation.ipynb) ì™„ë£Œ\n",
        "- [02_core_client.ipynb](02_core_client.ipynb) ì™„ë£Œ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í‰ê°€ ê¸°ë³¸\n",
        "\n",
        "ëª¨ë¸ì˜ ì‘ë‹µ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ê¸°ë³¸ ë°©ë²•ì…ë‹ˆë‹¤. LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ë¹„êµí•˜ëŠ” ê²ƒì€ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ë°°í¬í•˜ê¸° ì „ í•„ìˆ˜ì ì¸ ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **í‰ê°€ì˜ í•µì‹¬**:\n",
        "> - **ì •í™•ë„ (Accuracy)**: ëª¨ë¸ì´ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë¹„ìœ¨\n",
        "> - **ì¼ê´€ì„± (Consistency)**: ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ì¼ê´€ëœ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì •ë„\n",
        "> - **ê´€ë ¨ì„± (Relevance)**: ì‘ë‹µì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ” ì •ë„\n",
        "> - **ìœ ì°½ì„± (Fluency)**: ì‘ë‹µì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ë¬¸ë²•ì  ì •í™•ì„±\n",
        "> - **ì™„ì „ì„± (Completeness)**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ì™„ì „í•œ ì •ë„\n",
        "\n",
        "> ğŸ’¡ **í‰ê°€ ë°©ë²•**:\n",
        "> - **ìë™ í‰ê°€**: ë©”íŠ¸ë¦­ ê¸°ë°˜ ìë™ ì¸¡ì • (ë¹ ë¥´ê³  ê°ê´€ì )\n",
        "> - **ì¸ê°„ í‰ê°€**: ì‚¬ëŒì´ ì§ì ‘ í‰ê°€ (ì •í™•í•˜ì§€ë§Œ ë¹„ìš©ì´ ë†’ìŒ)\n",
        "> - **í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€**: ìë™ í‰ê°€ì™€ ì¸ê°„ í‰ê°€ë¥¼ ê²°í•©\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import Client, evaluate\n",
        "import asyncio\n",
        "\n",
        "# ============================================\n",
        "# ê¸°ë³¸ í‰ê°€ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def basic_evaluation():\n",
        "    \"\"\"\n",
        "    ê¸°ë³¸ í‰ê°€ ì˜ˆì œ\n",
        "\n",
        "    ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ê¸°ë³¸ í‰ê°€ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # í‰ê°€ ë°ì´í„°ì…‹\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"input\": \"Pythonì´ë€?\",\n",
        "            \"expected\": \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´\",\n",
        "            \"context\": \"í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•œ ì§ˆë¬¸\",\n",
        "        },\n",
        "        {\"input\": \"RAGë€?\", \"expected\": \"ê²€ìƒ‰ ì¦ê°• ìƒì„±\", \"context\": \"AI ê¸°ìˆ ì— ëŒ€í•œ ì§ˆë¬¸\"},\n",
        "        {\n",
        "            \"input\": \"ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
        "            \"expected\": \"ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•˜ëŠ” AI ê¸°ìˆ \",\n",
        "            \"context\": \"AI ê°œë…ì— ëŒ€í•œ ì§ˆë¬¸\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ“Š í‰ê°€ ë°ì´í„°ì…‹: {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\")\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"  {i}. {case['input']}\")\n",
        "\n",
        "    # í‰ê°€ ì‹¤í–‰\n",
        "    print(\"\\n[í‰ê°€ ì‹¤í–‰ ì¤‘...]\")\n",
        "    # results = await evaluate(\n",
        "    #     model=client,\n",
        "    #     test_cases=test_cases,\n",
        "    #     metrics=[\"accuracy\", \"relevance\", \"consistency\", \"latency\"]\n",
        "    # )\n",
        "    #\n",
        "    # print(\"\\nâœ… í‰ê°€ ì™„ë£Œ!\")\n",
        "    # print(f\"\\nğŸ“Š í‰ê°€ ê²°ê³¼:\")\n",
        "    # print(f\"  - ì •í™•ë„ (Accuracy): {results['accuracy']:.2%}\")\n",
        "    # print(f\"  - ê´€ë ¨ì„± (Relevance): {results['relevance']:.2%}\")\n",
        "    # print(f\"  - ì¼ê´€ì„± (Consistency): {results['consistency']:.2%}\")\n",
        "    # print(f\"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {results['latency']:.2f}ì´ˆ\")\n",
        "    #\n",
        "    # # ê°œë³„ ê²°ê³¼\n",
        "    # print(f\"\\nğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:\")\n",
        "    # for i, result in enumerate(results['details'], 1):\n",
        "    #     print(f\"\\n  í…ŒìŠ¤íŠ¸ {i}: {test_cases[i-1]['input']}\")\n",
        "    #     print(f\"    - ì •í™•ë„: {result['accuracy']:.2%}\")\n",
        "    #     print(f\"    - ê´€ë ¨ì„±: {result['relevance']:.2%}\")\n",
        "    #     print(f\"    - ì‘ë‹µ ì‹œê°„: {result['latency']:.2f}ì´ˆ\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ í‰ê°€ ë©”íŠ¸ë¦­ ì„¤ëª…:\")\n",
        "    print(\"  - accuracy: ì˜ˆìƒ ë‹µë³€ê³¼ ì‹¤ì œ ë‹µë³€ì˜ ì¼ì¹˜ë„\")\n",
        "    print(\"  - relevance: ì‘ë‹µì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆëŠ” ì •ë„\")\n",
        "    print(\"  - consistency: ë™ì¼ ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ì¼ê´€ì„±\")\n",
        "    print(\"  - latency: ì‘ë‹µ ìƒì„± ì‹œê°„ (ì„±ëŠ¥ ì¸¡ì •)\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await basic_evaluation()\n",
        "\n",
        "# ============================================\n",
        "# ìƒì„¸ í‰ê°€ ì˜ˆì œ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def detailed_evaluation():\n",
        "    \"\"\"\n",
        "    ìƒì„¸ í‰ê°€ ì˜ˆì œ\n",
        "\n",
        "    ë” ë§ì€ ë©”íŠ¸ë¦­ê³¼ ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ìƒì„¸ í‰ê°€ ì˜ˆì œ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # ë‹¤ì–‘í•œ ìœ í˜•ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"input\": \"Pythonì˜ ì¥ì ì„ 3ê°€ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.\",\n",
        "            \"expected\": \"ê°„ê²°í•œ ë¬¸ë²•, ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬, í¬ë¡œìŠ¤ í”Œë«í¼\",\n",
        "            \"type\": \"ìš”ì•½\",\n",
        "        },\n",
        "        {\n",
        "            \"input\": \"ë‹¤ìŒ ì½”ë“œì˜ ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”: print('Hello)\",\n",
        "            \"expected\": \"ë”°ì˜´í‘œê°€ ë‹«íˆì§€ ì•ŠìŒ\",\n",
        "            \"type\": \"ì½”ë“œ ë¶„ì„\",\n",
        "        },\n",
        "        {\n",
        "            \"input\": \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì€?\",\n",
        "            \"expected\": \"ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ì§‘í•©\",\n",
        "            \"type\": \"ë¹„êµ\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ“Š ë‹¤ì–‘í•œ ìœ í˜•ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ\")\n",
        "    for i, case in enumerate(test_cases, 1):\n",
        "        print(f\"  {i}. [{case['type']}] {case['input']}\")\n",
        "\n",
        "    # ìƒì„¸ í‰ê°€ ì‹¤í–‰\n",
        "    # results = await evaluate(\n",
        "    #     model=client,\n",
        "    #     test_cases=test_cases,\n",
        "    #     metrics=[\"accuracy\", \"relevance\", \"fluency\", \"completeness\", \"latency\"],\n",
        "    #     temperature=0.7,  # ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì˜¨ë„ ì„¤ì •\n",
        "    #     max_tokens=500\n",
        "    # )\n",
        "    #\n",
        "    # print(\"\\nâœ… ìƒì„¸ í‰ê°€ ì™„ë£Œ!\")\n",
        "    # print(f\"\\nğŸ“Š ì¢…í•© ê²°ê³¼:\")\n",
        "    # for metric, value in results['summary'].items():\n",
        "    #     if metric != 'details':\n",
        "    #         if isinstance(value, float):\n",
        "    #             print(f\"  - {metric}: {value:.2%}\")\n",
        "    #         else:\n",
        "    #             print(f\"  - {metric}: {value}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ìƒì„¸ í‰ê°€ ë©”íŠ¸ë¦­:\")\n",
        "    print(\"  - fluency: ì‘ë‹µì˜ ìì—°ìŠ¤ëŸ¬ì›€ê³¼ ë¬¸ë²•ì  ì •í™•ì„±\")\n",
        "    print(\"  - completeness: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì˜ ì™„ì „ì„±\")\n",
        "    print(\"  - coherence: ì‘ë‹µì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±\")\n",
        "    print(\"  - informativeness: ì •ë³´ì˜ ìœ ìš©ì„±\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await detailed_evaluation()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ê¸°ë³¸ í‰ê°€ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²¤ì¹˜ë§ˆí¬ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ ë¹„êµí•˜ê³  ì¸¡ì •í•˜ëŠ” í‘œì¤€í™”ëœ ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ë²¤ì¹˜ë§ˆí¬ì˜ ì¥ì **:\n",
        "> - **í‘œì¤€í™”ëœ í‰ê°€**: ëª¨ë“  ëª¨ë¸ì„ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€\n",
        "> - **ë¹„êµ ê°€ëŠ¥ì„±**: ë‹¤ë¥¸ ëª¨ë¸ê³¼ ì§ì ‘ ë¹„êµ ê°€ëŠ¥\n",
        "> - **ì¬í˜„ì„±**: ë™ì¼í•œ ì¡°ê±´ì—ì„œ ë°˜ë³µ í‰ê°€ ê°€ëŠ¥\n",
        "> - **ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°**: ì—°êµ¬ ë…¼ë¬¸ê³¼ ê²°ê³¼ ë¹„êµ ê°€ëŠ¥\n",
        "\n",
        "> ğŸ’¡ **ì£¼ìš” ë²¤ì¹˜ë§ˆí¬**:\n",
        "> - **MMLU**: ë‹¤ì¤‘ ê³¼ëª© ì´í•´ (ìˆ˜í•™, ë¬¼ë¦¬, ì—­ì‚¬ ë“±)\n",
        "> - **HellaSwag**: ìƒì‹ ì¶”ë¡  ëŠ¥ë ¥\n",
        "> - **GSM8K**: ìˆ˜í•™ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥\n",
        "> - **HumanEval**: ì½”ë“œ ìƒì„± ëŠ¥ë ¥\n",
        "> - **TruthfulQA**: ì‚¬ì‹¤ì„±ê³¼ ì •í™•ì„±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beanllm import run_benchmark\n",
        "import asyncio\n",
        "\n",
        "# ============================================\n",
        "# ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def benchmark_test():\n",
        "    \"\"\"\n",
        "    ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì˜ˆì œ\n",
        "\n",
        "    í‘œì¤€ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # MMLU ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\n",
        "    print(\"\\n[MMLU ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰]\")\n",
        "    print(\"MMLU (Massive Multitask Language Understanding)\")\n",
        "    print(\"ë‹¤ì¤‘ ê³¼ëª© ì´í•´ ëŠ¥ë ¥ì„ í‰ê°€í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ì…ë‹ˆë‹¤.\")\n",
        "\n",
        "    # results = await run_benchmark(\n",
        "    #     model=\"gpt-4o\",\n",
        "    #     benchmark=\"mmlu\",\n",
        "    #     subset=\"all\"  # ë˜ëŠ” íŠ¹ì • ê³¼ëª©ë§Œ ì„ íƒ\n",
        "    # )\n",
        "    #\n",
        "    # print(\"\\nâœ… MMLU ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ!\")\n",
        "    # print(f\"\\nğŸ“Š ì „ì²´ ê²°ê³¼:\")\n",
        "    # print(f\"  - ì „ì²´ ì •í™•ë„: {results['overall_accuracy']:.2%}\")\n",
        "    # print(f\"  - ì´ ë¬¸ì œ ìˆ˜: {results['total_questions']}ê°œ\")\n",
        "    # print(f\"  - ì •ë‹µ ìˆ˜: {results['correct_answers']}ê°œ\")\n",
        "    #\n",
        "    # print(f\"\\nğŸ“Š ê³¼ëª©ë³„ ê²°ê³¼:\")\n",
        "    # for subject, accuracy in results['subject_scores'].items():\n",
        "    #     print(f\"  - {subject}: {accuracy:.2%}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ MMLU ë²¤ì¹˜ë§ˆí¬ íŠ¹ì§•:\")\n",
        "    print(\"  - 57ê°œ ê³¼ëª©, 15,908ê°œ ë¬¸ì œ\")\n",
        "    print(\"  - ìˆ˜í•™, ë¬¼ë¦¬, ì—­ì‚¬, ë²•ë¥  ë“± ë‹¤ì–‘í•œ ë¶„ì•¼\")\n",
        "    print(\"  - 4ì§€ì„ ë‹¤í˜• ë¬¸ì œ\")\n",
        "    print(\"  - ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ì§€ì‹ í‰ê°€\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await benchmark_test()\n",
        "\n",
        "# ============================================\n",
        "# ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def multiple_benchmarks():\n",
        "    \"\"\"\n",
        "    ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ ì˜ˆì œ\n",
        "\n",
        "    ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì˜ ë‹¤ì–‘í•œ ëŠ¥ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ë¹„êµ\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    benchmarks = {\n",
        "        \"MMLU\": {\"description\": \"ë‹¤ì¤‘ ê³¼ëª© ì´í•´\", \"focus\": \"ì¼ë°˜ ì§€ì‹\", \"questions\": \"15,908ê°œ\"},\n",
        "        \"HellaSwag\": {\"description\": \"ìƒì‹ ì¶”ë¡ \", \"focus\": \"ìƒì‹ê³¼ ì¶”ë¡ \", \"questions\": \"10,000ê°œ\"},\n",
        "        \"GSM8K\": {\"description\": \"ìˆ˜í•™ ë¬¸ì œ\", \"focus\": \"ìˆ˜í•™ì  ì¶”ë¡ \", \"questions\": \"8,500ê°œ\"},\n",
        "        \"HumanEval\": {\"description\": \"ì½”ë“œ ìƒì„±\", \"focus\": \"í”„ë¡œê·¸ë˜ë° ëŠ¥ë ¥\", \"questions\": \"164ê°œ\"},\n",
        "    }\n",
        "\n",
        "    print(\"\\nğŸ“Š ì§€ì›í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬:\")\n",
        "    for name, info in benchmarks.items():\n",
        "        print(f\"\\n[{name}]\")\n",
        "        print(f\"  ì„¤ëª…: {info['description']}\")\n",
        "        print(f\"  ì´ˆì : {info['focus']}\")\n",
        "        print(f\"  ë¬¸ì œ ìˆ˜: {info['questions']}\")\n",
        "\n",
        "    # ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)\n",
        "    print(\"\\n[ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜]\")\n",
        "    # all_results = {}\n",
        "    #\n",
        "    # for benchmark_name in benchmarks.keys():\n",
        "    #     print(f\"\\n  [{benchmark_name}] ì‹¤í–‰ ì¤‘...\")\n",
        "    #     results = await run_benchmark(\n",
        "    #         model=\"gpt-4o\",\n",
        "    #         benchmark=benchmark_name.lower()\n",
        "    #     )\n",
        "    #     all_results[benchmark_name] = results\n",
        "    #     print(f\"    âœ… ì™„ë£Œ: {results['accuracy']:.2%}\")\n",
        "    #\n",
        "    # # ì¢…í•© ê²°ê³¼\n",
        "    # print(\"\\nğŸ“Š ì¢…í•© ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:\")\n",
        "    # print(f\"{'ë²¤ì¹˜ë§ˆí¬':<15} {'ì •í™•ë„':<10} {'ë¬¸ì œ ìˆ˜':<10}\")\n",
        "    # print(\"-\" * 40)\n",
        "    # for name, results in all_results.items():\n",
        "    #     print(f\"{name:<15} {results['accuracy']:>8.2%} {results['total_questions']:>10}ê°œ\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ë²¤ì¹˜ë§ˆí¬ ì„ íƒ ê°€ì´ë“œ:\")\n",
        "    print(\"  - ì¼ë°˜ ì§€ì‹ í‰ê°€: MMLU\")\n",
        "    print(\"  - ìƒì‹ ì¶”ë¡  í‰ê°€: HellaSwag\")\n",
        "    print(\"  - ìˆ˜í•™ ëŠ¥ë ¥ í‰ê°€: GSM8K\")\n",
        "    print(\"  - ì½”ë“œ ìƒì„± í‰ê°€: HumanEval\")\n",
        "    print(\"  - ì¢…í•© í‰ê°€: ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await multiple_benchmarks()\n",
        "\n",
        "# ============================================\n",
        "# ëª¨ë¸ ë¹„êµ í‰ê°€\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def model_comparison():\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ ë¹„êµ í‰ê°€ ì˜ˆì œ\n",
        "\n",
        "    ì—¬ëŸ¬ ëª¨ë¸ì„ ë™ì¼í•œ ë²¤ì¹˜ë§ˆí¬ë¡œ ë¹„êµí•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ëª¨ë¸ ë¹„êµ í‰ê°€\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    models_to_compare = [\"gpt-4o\", \"claude-sonnet-4-20250514\", \"gemini-2.5-pro\"]\n",
        "\n",
        "    print(f\"\\nğŸ“Š ë¹„êµí•  ëª¨ë¸: {len(models_to_compare)}ê°œ\")\n",
        "    for i, model in enumerate(models_to_compare, 1):\n",
        "        print(f\"  {i}. {model}\")\n",
        "\n",
        "    # ê° ëª¨ë¸ì— ëŒ€í•´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)\n",
        "    print(\"\\n[ëª¨ë¸ë³„ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰]\")\n",
        "    # comparison_results = {}\n",
        "    #\n",
        "    # for model in models_to_compare:\n",
        "    #     print(f\"\\n  [{model}] í‰ê°€ ì¤‘...\")\n",
        "    #     results = await run_benchmark(\n",
        "    #         model=model,\n",
        "    #         benchmark=\"mmlu\",\n",
        "    #         subset=\"stem\"  # STEM ê³¼ëª©ë§Œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)\n",
        "    #     )\n",
        "    #     comparison_results[model] = results\n",
        "    #     print(f\"    âœ… ì™„ë£Œ: {results['overall_accuracy']:.2%}\")\n",
        "    #\n",
        "    # # ë¹„êµ ê²°ê³¼\n",
        "    # print(\"\\nğŸ“Š ëª¨ë¸ ë¹„êµ ê²°ê³¼:\")\n",
        "    # print(f\"{'ëª¨ë¸':<30} {'ì •í™•ë„':<10} {'ìˆœìœ„':<5}\")\n",
        "    # print(\"-\" * 50)\n",
        "    #\n",
        "    # sorted_models = sorted(\n",
        "    #     comparison_results.items(),\n",
        "    #     key=lambda x: x[1]['overall_accuracy'],\n",
        "    #     reverse=True\n",
        "    # )\n",
        "    #\n",
        "    # for rank, (model, results) in enumerate(sorted_models, 1):\n",
        "    #     print(f\"{model:<30} {results['overall_accuracy']:>8.2%} {rank:>5}ìœ„\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ëª¨ë¸ ë¹„êµ íŒ:\")\n",
        "    print(\"  - ë™ì¼í•œ ë²¤ì¹˜ë§ˆí¬ì™€ ì„¤ì • ì‚¬ìš©\")\n",
        "    print(\"  - ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ë¡œ ì¢…í•© í‰ê°€\")\n",
        "    print(\"  - ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜• ê³ ë ¤\")\n",
        "    print(\"  - íŠ¹ì • ì‘ì—…ì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await model_comparison()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­\n",
        "# ============================================\n",
        "\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "\n",
        "def custom_accuracy_metric(predicted: str, expected: str) -> float:\n",
        "    \"\"\"\n",
        "    ì»¤ìŠ¤í…€ ì •í™•ë„ ë©”íŠ¸ë¦­\n",
        "\n",
        "    í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    # ì˜ˆìƒ ë‹µë³€ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
        "    expected_keywords = set(expected.lower().split())\n",
        "    predicted_keywords = set(predicted.lower().split())\n",
        "\n",
        "    # í‚¤ì›Œë“œ ì¼ì¹˜ìœ¨ ê³„ì‚°\n",
        "    if not expected_keywords:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = expected_keywords & predicted_keywords\n",
        "    accuracy = len(intersection) / len(expected_keywords)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def custom_relevance_metric(predicted: str, query: str) -> float:\n",
        "    \"\"\"\n",
        "    ì»¤ìŠ¤í…€ ê´€ë ¨ì„± ë©”íŠ¸ë¦­\n",
        "\n",
        "    ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    query_keywords = set(query.lower().split())\n",
        "    predicted_keywords = set(predicted.lower().split())\n",
        "\n",
        "    if not query_keywords:\n",
        "        return 0.0\n",
        "\n",
        "    # ì§ˆë¬¸ í‚¤ì›Œë“œê°€ ë‹µë³€ì— í¬í•¨ëœ ë¹„ìœ¨\n",
        "    matched = query_keywords & predicted_keywords\n",
        "    relevance = len(matched) / len(query_keywords)\n",
        "\n",
        "    return relevance\n",
        "\n",
        "\n",
        "print(\"âœ… ì»¤ìŠ¤í…€ í‰ê°€ ë©”íŠ¸ë¦­ ì •ì˜ ì™„ë£Œ!\")\n",
        "print(\"\\nğŸ’¡ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ì˜ˆì œ:\")\n",
        "print(\"  - í‚¤ì›Œë“œ ê¸°ë°˜ ì •í™•ë„\")\n",
        "print(\"  - ê´€ë ¨ì„± ì ìˆ˜\")\n",
        "print(\"  - ê¸¸ì´ ê¸°ë°˜ ì ìˆ˜\")\n",
        "print(\"  - ë„ë©”ì¸ íŠ¹í™” ë©”íŠ¸ë¦­\")\n",
        "\n",
        "# ============================================\n",
        "# ì»¤ìŠ¤í…€ í‰ê°€ ì‹¤í–‰\n",
        "# ============================================\n",
        "\n",
        "\n",
        "async def custom_evaluation_example():\n",
        "    \"\"\"\n",
        "    ì»¤ìŠ¤í…€ í‰ê°€ ì‹¤í–‰ ì˜ˆì œ\n",
        "\n",
        "    ì •ì˜í•œ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ì»¤ìŠ¤í…€ í‰ê°€ ì‹¤í–‰\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    from beanllm import Client\n",
        "\n",
        "    client = Client(model=\"gpt-4o\")\n",
        "\n",
        "    # ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"input\": \"Pythonì˜ ì£¼ìš” íŠ¹ì§•ì€?\",\n",
        "            \"expected\": \"ê°„ê²°í•œ ë¬¸ë²• ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ í¬ë¡œìŠ¤ í”Œë«í¼\",\n",
        "            \"domain\": \"í”„ë¡œê·¸ë˜ë°\",\n",
        "        },\n",
        "        {\"input\": \"ë¨¸ì‹ ëŸ¬ë‹ì´ë€?\", \"expected\": \"ë°ì´í„° í•™ìŠµ ì•Œê³ ë¦¬ì¦˜\", \"domain\": \"AI\"},\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ“Š ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ\")\n",
        "\n",
        "    # ì»¤ìŠ¤í…€ í‰ê°€ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)\n",
        "    # results = []\n",
        "    #\n",
        "    # for case in test_cases:\n",
        "    #     # ëª¨ë¸ ì‘ë‹µ ìƒì„±\n",
        "    #     response = await client.chat(\n",
        "    #         messages=[{\"role\": \"user\", \"content\": case[\"input\"]}]\n",
        "    #     )\n",
        "    #\n",
        "    #     # ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê³„ì‚°\n",
        "    #     accuracy = custom_accuracy_metric(response.content, case[\"expected\"])\n",
        "    #     relevance = custom_relevance_metric(response.content, case[\"input\"])\n",
        "    #\n",
        "    #     results.append({\n",
        "    #         \"input\": case[\"input\"],\n",
        "    #         \"predicted\": response.content,\n",
        "    #         \"expected\": case[\"expected\"],\n",
        "    #         \"accuracy\": accuracy,\n",
        "    #         \"relevance\": relevance\n",
        "    #     })\n",
        "    #\n",
        "    #     print(f\"\\n  [{case['domain']}] {case['input']}\")\n",
        "    #     print(f\"    ì •í™•ë„: {accuracy:.2%}\")\n",
        "    #     print(f\"    ê´€ë ¨ì„±: {relevance:.2%}\")\n",
        "    #\n",
        "    # # ì¢…í•© ê²°ê³¼\n",
        "    # avg_accuracy = sum(r[\"accuracy\"] for r in results) / len(results)\n",
        "    # avg_relevance = sum(r[\"relevance\"] for r in results) / len(results)\n",
        "    #\n",
        "    # print(f\"\\nğŸ“Š ì¢…í•© ê²°ê³¼:\")\n",
        "    # print(f\"  í‰ê·  ì •í™•ë„: {avg_accuracy:.2%}\")\n",
        "    # print(f\"  í‰ê·  ê´€ë ¨ì„±: {avg_relevance:.2%}\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ ì»¤ìŠ¤í…€ í‰ê°€ í™œìš©:\")\n",
        "    print(\"  - ë„ë©”ì¸ íŠ¹í™” í‰ê°€\")\n",
        "    print(\"  - ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ë°˜ì˜\")\n",
        "    print(\"  - ëª¨ë¸ ê°œì„  ì¶”ì \")\n",
        "    print(\"  - A/B í…ŒìŠ¤íŠ¸\")\n",
        "\n",
        "\n",
        "# ì‹¤í–‰ (ì£¼ì„ í•´ì œ)\n",
        "# await custom_evaluation_example()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… ì»¤ìŠ¤í…€ í‰ê°€ í•™ìŠµ ì™„ë£Œ!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ì»¤ìŠ¤í…€ í‰ê°€\n",
        "\n",
        "ìì‹ ë§Œì˜ í‰ê°€ ë©”íŠ¸ë¦­ê³¼ ë°ì´í„°ì…‹ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "> ğŸ’¡ **ì»¤ìŠ¤í…€ í‰ê°€ì˜ ì¥ì **:\n",
        "> - **ë„ë©”ì¸ íŠ¹í™”**: íŠ¹ì • ë„ë©”ì¸ì— ë§ëŠ” í‰ê°€\n",
        "> - **ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­**: ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì— ë§ëŠ” í‰ê°€\n",
        "> - **ìœ ì—°ì„±**: ì›í•˜ëŠ” ë©”íŠ¸ë¦­ê³¼ ê¸°ì¤€ ì •ì˜\n",
        "> - **ë°˜ë³µ ê°œì„ **: ëª¨ë¸ ê°œì„  ê³¼ì • ì¶”ì \n",
        "\n",
        "Evaluationì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤! ë‹¤ìŒ íŠœí† ë¦¬ì–¼ë¡œ ì§„í–‰í•˜ì„¸ìš”:\n",
        "\n",
        "### ğŸ“š ì¶”ì²œ í•™ìŠµ ìˆœì„œ\n",
        "\n",
        "1. **[13_finetuning.ipynb](13_finetuning.ipynb)** - Fine-tuning\n",
        "   - ëª¨ë¸ íŒŒì¸íŠœë‹\n",
        "   - ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í•™ìŠµ\n",
        "\n",
        "2. **[14_production_features.ipynb](14_production_features.ipynb)** - Production Features\n",
        "   - í”„ë¡œë•ì…˜ í™˜ê²½ ìµœì í™”\n",
        "   - ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…\n",
        "\n",
        "3. **[15_distributed_architecture.ipynb](15_distributed_architecture.ipynb)** - Distributed Architecture\n",
        "   - ë¶„ì‚° ì‹œìŠ¤í…œ êµ¬ì„±\n",
        "   - Redis & Kafka í™œìš©\n",
        "\n",
        "### ğŸ”— ê´€ë ¨ ë¬¸ì„œ\n",
        "\n",
        "- [API Reference](../API_REFERENCE.md#evaluation) - Evaluation API ìƒì„¸ ë¬¸ì„œ\n",
        "- [README.md](../../README.md) - í”„ë¡œì íŠ¸ ê°œìš”\n",
        "\n",
        "---\n",
        "\n",
        "**âœ… Evaluation ì™„ë£Œ! ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ì§„í–‰í•˜ì„¸ìš”.**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
