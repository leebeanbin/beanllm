# ğŸ“š beanllm API Reference

Complete API reference for all beanllm components.

## Table of Contents

### Core Components
- [Client](#client) - Basic LLM client
- [LLM Providers](#llm-providers) - 7 LLM providers (OpenAI, Anthropic, Google, DeepSeek, Perplexity, Ollama)
- [RAGChain](#ragchain) - RAG (Retrieval-Augmented Generation) system
- [Agent](#agent) - AI agent with tools
- [Chain](#chain) - Chain execution

### Document Processing
- [beanPDFLoader](#beanpdfloader) - Advanced PDF processing with 3-layer architecture
- [Document Loaders](#document-loaders) - Docling, Jupyter, HTML, Text, CSV loaders
- [Text Splitters](#text-splitters) - Semantic text chunking

### Embeddings & Retrieval
- [Embeddings](#embeddings) - Qwen3-Embedding-8B, Code, Matryoshka embeddings
- [Vector Stores](#vector-stores) - Milvus, LanceDB, pgvector, Chroma, FAISS
- [Retrieval](#retrieval) - HyDE, Hybrid Search, Reranking

### Advanced Features
- [MultiAgentCoordinator](#multiagentcoordinator) - Multi-agent collaboration
- [Graph](#graph) - Graph-based workflows
- [StateGraph](#stategraph) - State-based graph execution
- [Audio](#audio) - 8 STT engines (SenseVoice, Granite, Whisper, etc.)
- [Vision](#vision) - Qwen3-VL, YOLOv12, SAM 3, Florence-2

### Specialized Features
- [VisionRAG](#visionrag) - Vision + RAG with image understanding
- [WebSearch](#websearch) - Web search integration
- [Evaluator](#evaluator) - LLM evaluation (TruLens, RAGAS)
- [FineTuningManager](#finetuningmanager) - Model fine-tuning
- [Advanced LLM Features](#advanced-llm-features) - Structured Outputs, Prompt Caching, Parallel Tool Calling

---

## Installation

```bash
# Basic installation
pip install beanllm

# With all providers
pip install beanllm[all]

# Specific providers
pip install beanllm[openai,anthropic]
```

---

## Quick Start

```python
import asyncio
from beanllm import Client

async def main():
    # Initialize client
    client = Client(model="gpt-4")

    # Simple chat
    response = await client.chat(
        messages=[{"role": "user", "content": "Hello, how are you?"}]
    )
    print(response.content)

asyncio.run(main())
```

---

# API Documentation

## Core Components

### Client

ê¸°ë³¸ LLM í´ë¼ì´ì–¸íŠ¸. ê°€ì¥ ê°„ë‹¨í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### `__init__(model, provider=None, api_key=None, **kwargs)`

**íŒŒë¼ë¯¸í„°:**
- `model` (str): ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4", "claude-3-opus", "gemini-pro")
- `provider` (str, optional): Provider ì´ë¦„. ìƒëµ ì‹œ ëª¨ë¸ëª…ì—ì„œ ìë™ ê°ì§€
- `api_key` (str, optional): API í‚¤. ìƒëµ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
- `**kwargs`: Providerë³„ ì¶”ê°€ ì„¤ì •

**ì˜ˆì œ:**
```python
from beanllm import Client

# OpenAI
client = Client(model="gpt-4")

# Anthropic (provider ìë™ ê°ì§€)
client = Client(model="claude-3-opus-20240229")

# ëª…ì‹œì  provider ì§€ì •
client = Client(model="gpt-4", provider="openai")
```

#### `chat(messages, system=None, temperature=None, max_tokens=None, **kwargs)` (async)

ì±„íŒ… ì™„ë£Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `messages` (List[Dict[str, str]]): ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ `[{"role": "user", "content": "..."}]`
- `system` (str, optional): ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
- `temperature` (float, optional): ìƒ˜í”Œë§ ì˜¨ë„ (0.0-2.0)
- `max_tokens` (int, optional): ìµœëŒ€ ìƒì„± í† í° ìˆ˜
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `ChatResponse`

**ì˜ˆì œ:**
```python
response = await client.chat(
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7,
    max_tokens=1000
)
print(response.content)
```

#### `stream_chat(messages, **kwargs)` (async)

ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì±„íŒ… ì™„ë£Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:** `chat()`ì™€ ë™ì¼

**ë°˜í™˜:** `AsyncIterator[str]`

**ì˜ˆì œ:**
```python
async for chunk in client.stream_chat(messages=[{"role": "user", "content": "Tell me a story"}]):
    print(chunk, end="", flush=True)
```

---

### LLM Providers

beanllm supports 7 LLM providers with automatic fallback and unified interface.

#### Supported Providers

| Provider | Models | Features |
|----------|---------|----------|
| **OpenAI** | GPT-4, GPT-4o, GPT-4o-mini | Structured Outputs, Vision, Tool Calling |
| **Anthropic** | Claude Opus 4, Sonnet 4.5, Haiku 3.5 | Prompt Caching, Vision, Tool Calling |
| **Google** | Gemini 2.5 Pro, Flash | Large context (2M tokens), Vision |
| **DeepSeek** | DeepSeek-V3 (671B MoE) | Cost-efficient, OpenAI-compatible |
| **Perplexity** | Sonar, Sonar-Pro | Real-time web search, Citations |
| **Ollama** | Llama 3.3, Qwen2.5, etc. | Local deployment, Privacy |
| **X.AI** | Grok 2 | Coming soon |

#### Usage Examples

```python
from beanllm import Client

# OpenAI
client = Client(model="gpt-4o")

# Anthropic
client = Client(model="claude-sonnet-4-20250514")

# Google Gemini
client = Client(model="gemini-2.5-pro")

# DeepSeek (cost-efficient)
client = Client(model="deepseek-chat")

# Perplexity (real-time web search)
client = Client(model="sonar-pro")

# Ollama (local)
client = Client(model="llama3.3:70b", provider="ollama")
```

#### Environment Variables

```bash
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="..."
export DEEPSEEK_API_KEY="sk-..."
export PERPLEXITY_API_KEY="pplx-..."
export OLLAMA_HOST="http://localhost:11434"  # Optional
```

---

### RAGChain

RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ. ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.

#### `from_documents(source, chunk_size=500, chunk_overlap=50, embedding_model="text-embedding-3-small", llm_model="gpt-4o-mini", **kwargs)`

íŒ©í† ë¦¬ ë©”ì„œë“œë¡œ RAG ì‹œìŠ¤í…œì„ ìƒì„±í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `source` (str | List): ë¬¸ì„œ ê²½ë¡œ ë˜ëŠ” ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
- `chunk_size` (int): ì²­í¬ í¬ê¸°
- `chunk_overlap` (int): ì²­í¬ ê²¹ì¹¨
- `embedding_model` (str): ì„ë² ë”© ëª¨ë¸ ì´ë¦„
- `llm_model` (str): LLM ëª¨ë¸ ì´ë¦„
- `**kwargs`: ì¶”ê°€ ì„¤ì •

**ì˜ˆì œ:**
```python
from beanllm import RAGChain

rag = RAGChain.from_documents(
    source="documents.txt",
    chunk_size=500,
    embedding_model="text-embedding-3-small",
    llm_model="gpt-4"
)
```

#### `add_documents(documents)` (async)

ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `documents` (List[str] | List[Document]): ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

**ì˜ˆì œ:**
```python
documents = [
    "Python is a programming language.",
    "Machine learning is a subset of AI.",
]
await rag.add_documents(documents)
```

#### `query(question, top_k=3, **kwargs)` (async)

ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `question` (str): ì§ˆë¬¸
- `top_k` (int): ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `RAGResponse`

**ì˜ˆì œ:**
```python
response = await rag.query(
    question="What is Python?",
    top_k=3
)
print(response.answer)
print(response.sources)  # ì‚¬ìš©ëœ ë¬¸ì„œë“¤
```

---

### Agent

ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI ì—ì´ì „íŠ¸.

#### `__init__(model, tools=None, max_iterations=10, **kwargs)`

**íŒŒë¼ë¯¸í„°:**
- `model` (str): LLM ëª¨ë¸ ì´ë¦„
- `tools` (List[Tool], optional): ì‚¬ìš©í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸
- `max_iterations` (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
- `**kwargs`: ì¶”ê°€ ì„¤ì •

**ì˜ˆì œ:**
```python
from beanllm import Agent
from beanllm import search_web, calculator

agent = Agent(
    model="gpt-4",
    tools=[search_web, calculator]
)
```

#### `run(task, max_iterations=10, **kwargs)` (async)

ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `task` (str): ìˆ˜í–‰í•  ì‘ì—…
- `max_iterations` (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `AgentResponse`

**ì˜ˆì œ:**
```python
response = await agent.run(
    task="Calculate 123 * 456 and search for the result online",
    max_iterations=5
)
print(response.final_answer)
print(response.steps)  # ì‹¤í–‰ ë‹¨ê³„
```

---

### Chain

ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸.

#### `__init__(client, memory=None, verbose=False)`

**íŒŒë¼ë¯¸í„°:**
- `client` (Client): LLM í´ë¼ì´ì–¸íŠ¸
- `memory` (Memory, optional): ë©”ëª¨ë¦¬ ê°ì²´
- `verbose` (bool): ë””ë²„ê·¸ ì¶œë ¥ ì—¬ë¶€

#### `run(user_input, **kwargs)` (async)

ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `user_input` (str): ì‚¬ìš©ì ì…ë ¥
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `ChainResult`

**ì˜ˆì œ:**
```python
from beanllm import Chain, Client

client = Client(model="gpt-4")
chain = Chain(client=client)

response = await chain.run("Translate 'hello' to French")
print(response.output)
```

---

## Document Processing

### beanPDFLoader

ê³ ê¸‰ PDF ì²˜ë¦¬ë¥¼ ìœ„í•œ 3-Layer ì•„í‚¤í…ì²˜ ë¡œë”.

**3-Layer ì•„í‚¤í…ì²˜:**
- **Fast Layer** (PyMuPDF): ë¹ ë¥¸ ì²˜ë¦¬ (~130 pages/sec), ì´ë¯¸ì§€ ì¶”ì¶œ
- **Accurate Layer** (pdfplumber): ì •í™•í•œ í…Œì´ë¸” ì¶”ì¶œ (~10 pages/sec)
- **ML Layer** (marker-pdf): êµ¬ì¡° ë³´ì¡´ Markdown ë³€í™˜ (98% ì •í™•ë„)

#### `__init__(file_path, strategy="auto", extract_tables=True, extract_images=False, to_markdown=False, **kwargs)`

**íŒŒë¼ë¯¸í„°:**
- `file_path` (str | Path): PDF íŒŒì¼ ê²½ë¡œ
- `strategy` (str): íŒŒì‹± ì „ëµ
  - `"auto"`: ìë™ ì„ íƒ (ê¸°ë³¸ê°’)
  - `"fast"`: PyMuPDF (ë¹ ë¥¸ ì²˜ë¦¬)
  - `"accurate"`: pdfplumber (ì •í™•í•œ í…Œì´ë¸”)
  - `"ml"`: marker-pdf (ML ê¸°ë°˜, optional)
- `extract_tables` (bool): í…Œì´ë¸” ì¶”ì¶œ ì—¬ë¶€ (ê¸°ë³¸: True)
- `extract_images` (bool): ì´ë¯¸ì§€ ì¶”ì¶œ ì—¬ë¶€ (ê¸°ë³¸: False)
- `to_markdown` (bool): Markdown ë³€í™˜ ì—¬ë¶€ (ê¸°ë³¸: False)
- `enable_ocr` (bool): OCR í™œì„±í™” (í–¥í›„ êµ¬í˜„)
- `layout_analysis` (bool): ë ˆì´ì•„ì›ƒ ë¶„ì„ (í–¥í›„ êµ¬í˜„)
- `max_pages` (int, optional): ìµœëŒ€ ì²˜ë¦¬ í˜ì´ì§€ ìˆ˜
- `page_range` (tuple[int, int], optional): ì²˜ë¦¬í•  í˜ì´ì§€ ë²”ìœ„

**ì˜ˆì œ:**
```python
from beanllm.domain.loaders.pdf import beanPDFLoader

# ê¸°ë³¸ ì‚¬ìš© (ìë™ ì „ëµ)
loader = beanPDFLoader("document.pdf")
docs = loader.load()

# í…Œì´ë¸” ì¶”ì¶œ
loader = beanPDFLoader("report.pdf", extract_tables=True)
docs = loader.load()
tables = loader._result["tables"]

# Markdown ë³€í™˜
loader = beanPDFLoader("article.pdf", to_markdown=True)
docs = loader.load()
markdown = loader._result["markdown"]

# ML Layer ì‚¬ìš© (marker-pdf í•„ìš”)
loader = beanPDFLoader("complex.pdf", strategy="ml", to_markdown=True)
docs = loader.load()
```

#### `load()` â†’ `List[Document]`

PDFë¥¼ ë¡œë”©í•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

**ë°˜í™˜ê°’:**
- `List[Document]`: í˜ì´ì§€ë³„ Document ë¦¬ìŠ¤íŠ¸

**ì˜ˆì œ:**
```python
loader = beanPDFLoader("document.pdf")
docs = loader.load()

for doc in docs:
    print(f"Page {doc.metadata['page']}: {doc.content[:100]}...")
```

#### ê³ ê¸‰ ê¸°ëŠ¥

**1. í…Œì´ë¸” ì¶”ì¶œ ë° ë³€í™˜**

```python
from beanllm.domain.loaders.pdf import beanPDFLoader
from beanllm.domain.loaders.pdf.extractors import TableExtractor

# í…Œì´ë¸” ì¶”ì¶œ
loader = beanPDFLoader("report.pdf", extract_tables=True)
docs = loader.load()

# í…Œì´ë¸” ì¡°íšŒ
extractor = TableExtractor(docs)
all_tables = extractor.get_all_tables()
high_quality = extractor.get_high_quality_tables(min_confidence=0.8)

# Markdown ë³€í™˜
markdown_tables = extractor.export_to_markdown()
```

**2. Markdown ë³€í™˜ ë° Layout Analysis**

```python
from beanllm.domain.loaders.pdf import beanPDFLoader
from beanllm.domain.loaders.pdf.utils import LayoutAnalyzer

# Markdown ë³€í™˜
loader = beanPDFLoader("article.pdf", to_markdown=True)
docs = loader.load()
markdown = loader._result["markdown"]

# Layout ë¶„ì„
analyzer = LayoutAnalyzer()
for doc in docs:
    page_data = {"text": doc.content, "width": doc.metadata["width"],
                 "height": doc.metadata["height"], "metadata": doc.metadata}
    layout = analyzer.analyze_layout(page_data)
    print(f"Columns: {layout['columns']}, Multi-column: {layout['is_multi_column']}")
```

**3. MarkerEngine (ML Layer)**

```python
# ML Layer ì‚¬ìš© (marker-pdf ì„¤ì¹˜ í•„ìš”: pip install beanllm[ml])
from beanllm.domain.loaders.pdf.engines import MarkerEngine

engine = MarkerEngine(
    use_gpu=False,      # GPU ì‚¬ìš© ì—¬ë¶€
    enable_cache=True,  # ê²°ê³¼ ìºì‹±
    cache_size=10,      # ìºì‹œ í¬ê¸°
)

# ë‹¨ì¼ PDF ì²˜ë¦¬
result = engine.extract("document.pdf", {
    "to_markdown": True,
    "extract_tables": True,
    "extract_images": True,
})

# Batch ì²˜ë¦¬
results = engine.extract_batch(
    ["doc1.pdf", "doc2.pdf", "doc3.pdf"],
    {"to_markdown": True}
)

# ìºì‹œ í†µê³„
stats = engine.get_cache_stats()
print(f"Cache: {stats['cache_size']}/{stats['cache_limit']}")
```

**4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**

```
Engine       Time(s)    Pages/s    Memory(MB)
------------------------------------------------
PyMuPDF      0.03       129.61     0.20
pdfplumber   0.42       9.59       41.41
marker-pdf   ~10s/100pg (GPU), 98% accuracy
```

---

### Document Loaders

ë‹¤ì–‘í•œ ë¬¸ì„œ í˜•ì‹ ì§€ì› (Office, Jupyter, HTML, Text, CSV).

#### DoclingLoader - Office Files (97.9% accuracy)

```python
from beanllm.domain.loaders import DoclingLoader

# PDF, DOCX, XLSX, PPTX, HTML ì§€ì›
loader = DoclingLoader(
    "document.docx",
    extract_tables=True,
    extract_images=False,
    ocr_enabled=False
)
docs = loader.load()

# í…Œì´ë¸” ë°ì´í„° ì ‘ê·¼
tables = loader.get_tables()
```

#### JupyterLoader - Jupyter Notebooks

```python
from beanllm.domain.loaders import JupyterLoader

loader = JupyterLoader(
    "notebook.ipynb",
    include_outputs=True,
    filter_cell_types=["code", "markdown"]  # Optional
)
docs = loader.load()
```

#### HTMLLoader - Multi-tier Fallback

```python
from beanllm.domain.loaders import HTMLLoader

# 3-tier fallback: Trafilatura â†’ Readability â†’ BeautifulSoup
loader = HTMLLoader(
    "https://example.com",
    fallback_chain=["trafilatura", "readability", "beautifulsoup"]
)
docs = loader.load()
```

#### Text & CSV Loaders

```python
from beanllm.domain.loaders import TextLoader, CSVLoader

# Text íŒŒì¼
text_loader = TextLoader("document.txt")
docs = text_loader.load()

# CSV íŒŒì¼
csv_loader = CSVLoader("data.csv")
docs = csv_loader.load()
```

---

### Text Splitters

ì˜ë¯¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• .

```python
from beanllm.domain.splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " "]
)

chunks = splitter.split_documents(docs)
```

---

## Embeddings & Retrieval

### Embeddings

ìµœì‹  ì„ë² ë”© ëª¨ë¸ ì§€ì› (Qwen3-Embedding-8B, Code, Matryoshka).

#### Qwen3-Embedding-8B - Top Multilingual Model

```python
from beanllm.domain.embeddings import Qwen3Embedding

# Qwen3-Embedding (SOTA multilingual)
qwen3 = Qwen3Embedding(model_size="8B")  # or "4B", "2B"
vectors = qwen3.embed_sync(["í•œê¸€ í…ìŠ¤íŠ¸", "English text", "æ—¥æœ¬èª"])
```

#### Code Embeddings - Specialized for Code Search

```python
from beanllm.domain.embeddings import CodeEmbedding

# Code-specialized embeddings
code_emb = CodeEmbedding(model="jinaai/jina-embeddings-v3")
code_vectors = code_emb.embed_sync([
    "def hello_world():",
    "class MyClass:",
    "import numpy as np"
])
```

#### Matryoshka Embeddings - 83% Storage Savings

```python
from beanllm.domain.embeddings import MatryoshkaEmbedding, OpenAIEmbedding, truncate_embedding

# Dimension reduction (1536 â†’ 512)
base_emb = OpenAIEmbedding(model="text-embedding-3-large")
mat_emb = MatryoshkaEmbedding(base_embedding=base_emb, output_dimension=512)
reduced_vectors = mat_emb.embed_sync(["text"])  # 512 dims instead of 1536

# Or truncate existing embeddings
full_vector = base_emb.embed_sync(["text"])[0]  # 1536 dims
reduced = truncate_embedding(full_vector, target_dim=512)  # 512 dims
```

---

### Vector Stores

ê³ ì„±ëŠ¥ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì§€ì› (Milvus, LanceDB, pgvector, Chroma, FAISS).

#### Milvus - High Performance

```python
from beanllm.domain.vector_stores import MilvusVectorStore

milvus = MilvusVectorStore(
    collection_name="docs",
    embedding=embedding,
    connection_args={"host": "localhost", "port": "19530"}
)
milvus.add_documents(docs)
results = milvus.similarity_search("query", k=5)
```

#### LanceDB - Modern Vector DB

```python
from beanllm.domain.vector_stores import LanceDBVectorStore

lancedb = LanceDBVectorStore(
    table_name="docs",
    embedding=embedding,
    uri="./lancedb_data"
)
lancedb.add_documents(docs)
results = lancedb.similarity_search("query", k=5)
```

#### pgvector - PostgreSQL Extension

```python
from beanllm.domain.vector_stores import PGVectorStore

pgvector = PGVectorStore(
    collection_name="docs",
    embedding=embedding,
    connection_string="postgresql://user:pass@localhost/dbname"
)
pgvector.add_documents(docs)
results = pgvector.similarity_search("query", k=5)
```

---

### Retrieval

ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë²• (HyDE, Hybrid Search, Reranking).

#### HyDE - Hypothetical Document Embeddings

```python
from beanllm.domain.retrieval import HyDE

# Query expansion using LLM
hyde = HyDE(llm=client, embedding=embedding)
expanded_query = hyde.expand_query("What is quantum computing?")
# Returns: hypothetical answer + original query
```

#### Hybrid Search - Combine Vector + Keyword

```python
from beanllm.domain.retrieval import HybridSearch

hybrid = HybridSearch(
    vector_store=vector_store,
    keyword_search=bm25_search,
    alpha=0.5  # 0.5 = equal weight
)
results = hybrid.search("query", k=10)
```

#### Reranking - Cross-Encoder

```python
from beanllm.domain.retrieval import Reranker

reranker = Reranker(model="cross-encoder/ms-marco-MiniLM-L-6-v2")
reranked = reranker.rerank(
    query="query",
    documents=initial_results,
    top_k=5
)
```

---

## Advanced Features

### MultiAgentCoordinator

ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ì—…í•˜ëŠ” ì‹œìŠ¤í…œ.

#### `__init__(agents, communication_bus=None)`

**íŒŒë¼ë¯¸í„°:**
- `agents` (Dict[str, Agent]): ì—ì´ì „íŠ¸ ë”•ì…”ë„ˆë¦¬ (id: agent)
- `communication_bus` (CommunicationBus, optional): í†µì‹  ë²„ìŠ¤

#### `execute_sequential(task, agent_order, **kwargs)` (async)

ìˆœì°¨ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `task` (str): ì‘ì—…
- `agent_order` (List[str]): ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œ

#### `execute_debate(task, agent_ids=None, rounds=3, **kwargs)` (async)

í† ë¡  ë°©ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**ì˜ˆì œ:**
```python
from beanllm import MultiAgentCoordinator, Agent

researcher = Agent(model="gpt-4")
writer = Agent(model="gpt-4")

coordinator = MultiAgentCoordinator(
    agents={"researcher": researcher, "writer": writer}
)

result = await coordinator.execute_sequential(
    task="Research AI trends and write a summary",
    agent_order=["researcher", "writer"]
)
```

---

### Graph

ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°.

#### `__init__(enable_cache=True)`

**íŒŒë¼ë¯¸í„°:**
- `enable_cache` (bool): ìºì‹± í™œì„±í™” ì—¬ë¶€

#### `add_node(node)`

ê·¸ë˜í”„ì— ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `add_edge(from_node, to_node)`

ë…¸ë“œ ê°„ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `run(initial_state, verbose=False)` (async)

ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

---

### StateGraph

ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ ì‹¤í–‰ ì‹œìŠ¤í…œ.

#### `__init__(state_schema=None, config=None)`

**íŒŒë¼ë¯¸í„°:**
- `state_schema` (Dict, optional): ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
- `config` (GraphConfig, optional): ê·¸ë˜í”„ ì„¤ì •

#### `add_node(name, func)`

ìƒíƒœ ê·¸ë˜í”„ì— ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `set_entry_point(node_name)`

ì§„ì…ì ì„ ì„¤ì •í•©ë‹ˆë‹¤.

#### `add_conditional_edge(from_node, condition_func, edge_mapping=None)`

ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `invoke(initial_state, execution_id=None)` (async)

ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**ì˜ˆì œ:**
```python
from beanllm import StateGraph

graph = StateGraph(state_schema={"count": 0, "message": ""})

def increment(state):
    state["count"] += 1
    return state

graph.add_node("increment", increment)
graph.set_entry_point("increment")

result = await graph.invoke({"count": 0, "message": "start"})
```

---

### Audio

8ê°œ STT ì—”ì§„ ì§€ì› (SenseVoice, Granite, Whisper, etc.)

#### SenseVoice - 15x Faster + Emotion Recognition

```python
from beanllm.domain.audio import beanSTT

# 15x faster than Whisper-Large
stt = beanSTT(engine="sensevoice", language="ko")
result = stt.transcribe("korean_audio.mp3")
print(result.text)
print(result.metadata["emotion"])  # Emotion recognition (SER)
print(result.metadata["events"])   # Audio event detection (AED)
```

#### Granite Speech 8B - Enterprise-grade (WER 5.85%)

```python
from beanllm.domain.audio import beanSTT

# Open ASR Leaderboard #2
stt = beanSTT(engine="granite", language="en")
result = stt.transcribe("english_audio.mp3")
print(f"Transcription: {result.text}")
print(f"WER: {result.metadata.get('wer', 'N/A')}")  # 5.85%
```

#### All 8 STT Engines

```python
from beanllm.domain.audio import beanSTT

# 1. SenseVoice-Small (Alibaba) - 15x faster, emotion
# 2. Granite Speech 8B (IBM) - WER 5.85%, enterprise
# 3. Whisper V3 Turbo (OpenAI) - Balanced
# 4. Distil-Whisper - Efficient
# 5. Parakeet TDT (NVIDIA) - High accuracy
# 6. Canary (NVIDIA) - Multilingual
# 7. Moonshine (Useful Sensors) - Edge devices

engines = ["sensevoice", "granite", "whisper-v3-turbo", "distil-whisper",
           "parakeet", "canary", "moonshine"]
```

#### TextToSpeech - Text-to-Speech

```python
from beanllm import TextToSpeech

tts = TextToSpeech(provider="openai", voice="alloy")
audio_bytes = tts.synthesize("Hello, world!")
```

#### AudioRAG - ì˜¤ë””ì˜¤ ê²€ìƒ‰ ë° QA

```python
from beanllm import AudioRAG

audio_rag = AudioRAG()
audio_rag.add_audio("interview.mp3", audio_id="interview_1")
results = audio_rag.search("What did they say about AI?", top_k=3)
```

---

### Vision

ìµœì‹  Vision AI ëª¨ë¸ ì§€ì› (Qwen3-VL, YOLOv12, SAM 3, Florence-2).

#### Qwen3-VL - Vision-Language Model (128K context)

```python
from beanllm.domain.vision import create_vision_task_model

# Qwen3-VL (VQA, OCR, Captioning, Multi-image Chat)
qwen = create_vision_task_model("qwen3vl", model_size="8B")

# Image Captioning
caption = qwen.caption(image="photo.jpg")

# Visual Question Answering
answer = qwen.vqa(image="photo.jpg", question="What is in this image?")

# OCR Text Extraction
text = qwen.ocr(image="document.jpg")

# Multi-image Chat (128K context)
response = qwen.chat(
    images=["img1.jpg", "img2.jpg", "img3.jpg"],
    prompt="Compare these images and describe the differences"
)
```

#### YOLOv12 - Object Detection & Segmentation

```python
from beanllm.domain.vision import create_vision_task_model

# YOLOv12 (latest)
yolo = create_vision_task_model("yolo", version="12")
detections = yolo.predict(image="photo.jpg", conf=0.5)

for det in detections:
    print(f"Object: {det['class']}, Confidence: {det['confidence']:.2f}")
```

#### SAM 3 - Segment Anything Model

```python
from beanllm.domain.vision import create_vision_task_model

# SAM 3 (Zero-shot segmentation)
sam = create_vision_task_model("sam2")
masks = sam.predict(
    image="photo.jpg",
    points=[[500, 375]],  # Click point
    labels=[1]  # 1=foreground, 0=background
)
```

#### Florence-2 - Unified Vision Tasks

```python
from beanllm.domain.vision import create_vision_task_model

# Florence-2 (Captioning, Detection, OCR, Grounding)
florence = create_vision_task_model("florence2", model_size="large")

# Dense Captioning
captions = florence.caption(image="photo.jpg", task="dense")

# Object Detection
detections = florence.detect(image="photo.jpg")

# OCR
text = florence.ocr(image="document.jpg")
```

---

## Specialized Features

### VisionRAG

ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG.

#### `from_images(source, generate_captions=True, llm_model="gpt-4o", **kwargs)`

ì´ë¯¸ì§€ë¡œë¶€í„° VisionRAGë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `source` (str | List): ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸
- `generate_captions` (bool): ìë™ ìº¡ì…˜ ìƒì„± ì—¬ë¶€
- `llm_model` (str): LLM ëª¨ë¸

**ì˜ˆì œ:**
```python
from beanllm import VisionRAG

vision_rag = VisionRAG.from_images(
    source="images/",
    generate_captions=True,
    llm_model="gpt-4o"
)

# ì´ë¯¸ì§€ ê²€ìƒ‰ ë° ì§ˆì˜
response = vision_rag.query(
    question="What objects are in the images?",
    k=3,
    include_images=True
)
```

---

### WebSearch

ì›¹ ê²€ìƒ‰ í†µí•©.

#### `search(query, engine=None, **kwargs)`

ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `query` (str): ê²€ìƒ‰ ì¿¼ë¦¬
- `engine` (str, optional): ê²€ìƒ‰ ì—”ì§„ ("google", "bing", "duckduckgo")

**ì˜ˆì œ:**
```python
from beanllm import WebSearch

search = WebSearch(default_engine="duckduckgo")
results = search.search("latest AI news")

for result in results:
    print(result.title, result.url)
```

---

### Evaluator

RAG í‰ê°€ ë° ëª¨ë‹ˆí„°ë§ (TruLens, RAGAS).

#### TruLens - RAG Performance Evaluation

```python
from beanllm.domain.evaluation import TruLensEvaluator

# TruLensë¡œ RAG ì„±ëŠ¥ í‰ê°€
evaluator = TruLensEvaluator(app_name="my_rag")
results = evaluator.evaluate(
    query="What is quantum computing?",
    response="Quantum computing uses quantum mechanics...",
    context=["Document 1 text", "Document 2 text"]
)

# Metrics: Groundedness, Context Relevance, Answer Relevance
print(results.scores)
# {'groundedness': 0.95, 'context_relevance': 0.88, 'answer_relevance': 0.92}
```

#### RAGAS - RAG Assessment

```python
from beanllm.domain.evaluation import RAGASEvaluator

# RAGAS metrics
evaluator = RAGASEvaluator(metrics=["faithfulness", "answer_relevancy"])
result = evaluator.evaluate(
    question="What is Python?",
    answer="Python is a programming language",
    contexts=["Python is a high-level language..."],
    ground_truth="Python is a programming language"  # Optional
)
print(result.scores)
```

#### Traditional Metrics

```python
from beanllm import Evaluator

evaluator = Evaluator(metrics=["bleu", "rouge", "f1"])
result = evaluator.evaluate(
    prediction="The cat sat on the mat",
    reference="A cat was sitting on the mat"
)
print(result.scores)
```

---

### FineTuningManager

ëª¨ë¸ íŒŒì¸íŠœë‹.

#### `prepare_and_upload(examples, output_path, validate=True)`

í›ˆë ¨ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê³  ì—…ë¡œë“œí•©ë‹ˆë‹¤.

#### `start_training(model, training_file, validation_file=None, **kwargs)`

íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.

**ì˜ˆì œ:**
```python
from beanllm import FineTuningManager

manager = FineTuningManager(provider="openai")

# ë°ì´í„° ì¤€ë¹„
file_id = manager.prepare_and_upload(
    examples=[...],
    output_path="training.jsonl"
)

# í›ˆë ¨ ì‹œì‘
job = manager.start_training(
    model="gpt-3.5-turbo",
    training_file=file_id
)

# ì§„í–‰ ìƒí™© í™•ì¸
progress = manager.get_training_progress(job.id)
```

---

### Advanced LLM Features

ê³ ê¸‰ LLM ê¸°ëŠ¥ (Structured Outputs, Prompt Caching, Parallel Tool Calling).

ìì„¸í•œ ë‚´ìš©ì€ [ADVANCED_FEATURES.md](ADVANCED_FEATURES.md)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.

#### Structured Outputs - 100% Schema Accuracy

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()

response = await client.chat.completions.create(
    model="gpt-4o-2024-08-06",
    messages=[{"role": "user", "content": "Extract: John Doe, 30, john@example.com"}],
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "user_info",
            "strict": True,  # 100% accuracy guarantee
            "schema": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "age": {"type": "integer"},
                    "email": {"type": "string"}
                },
                "required": ["name", "age", "email"]
            }
        }
    }
)
```

#### Prompt Caching - 85% Latency Reduction, 10x Cost Savings

```python
from anthropic import AsyncAnthropic

client = AsyncAnthropic()

response = await client.messages.create(
    model="claude-sonnet-4-20250514",
    system=[{
        "type": "text",
        "text": "Long system prompt..." * 1000,
        "cache_control": {"type": "ephemeral"}  # Cache for 5 minutes
    }],
    messages=[{"role": "user", "content": "Question"}],
    extra_headers={"anthropic-beta": "prompt-caching-2024-07-31"}
)

# Check cache usage
print(response.usage.cache_read_input_tokens)  # Cached tokens
```

#### Parallel Tool Calling - Concurrent Execution

```python
from openai import AsyncOpenAI

client = AsyncOpenAI()

tools = [
    {"type": "function", "function": {"name": "get_weather", "description": "..."}},
    {"type": "function", "function": {"name": "get_time", "description": "..."}}
]

response = await client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": "Weather in Seoul and time in Tokyo?"}],
    tools=tools,
    parallel_tool_calls=True  # Execute both simultaneously
)
```

---

## Common Types

### Response Objects

All facade methods return specific response objects:

- `ChatResponse` - Chat completion response
- `RAGResponse` - RAG query response
- `AgentResponse` - Agent execution response
- `AudioResponse` - Audio processing response
- `EvaluationResponse` - Evaluation results
- etc.

### Common Parameters

Most facades support these common parameters:

- `model` (str): Model name (e.g., "gpt-4", "claude-3-opus")
- `temperature` (float): Sampling temperature (0.0 - 2.0)
- `max_tokens` (int): Maximum tokens to generate
- `stream` (bool): Enable streaming responses

---

## Error Handling

```python
import asyncio
from beanllm import Client
from beanllm.utils.exceptions import LLMKitError

async def main():
    try:
        client = Client(model="gpt-4")
        response = await client.chat(
            messages=[{"role": "user", "content": "Hello"}]
        )
        print(response.content)
    except LLMKitError as e:
        print(f"Error: {e}")

asyncio.run(main())
```

---

## Environment Variables

beanllm uses environment variables for API keys:

```bash
# LLM Providers (7 providers)
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="..."
export DEEPSEEK_API_KEY="sk-..."
export PERPLEXITY_API_KEY="pplx-..."
export OLLAMA_HOST="http://localhost:11434"  # Optional

# Or use .env file in project root
```

---

## Additional Resources

- [GitHub Repository](https://github.com/leebeanbin/beanllm)
- [PyPI Package](https://pypi.org/project/beanllm/)
- [Examples](../examples/)
- [Architecture Guide](../ARCHITECTURE.md)

---

**Last Updated:** 2025-12-31
**Version:** 0.2.0 (2024-2025 Update)
