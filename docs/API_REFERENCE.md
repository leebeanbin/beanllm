# ğŸ“š beanllm API Reference

Complete API reference for all beanllm components.

## Table of Contents

### Core Components
- [Client](#client) - Basic LLM client
- [RAGChain](#ragchain) - RAG (Retrieval-Augmented Generation) system
- [Agent](#agent) - AI agent with tools
- [Chain](#chain) - Chain execution

### Advanced Features
- [MultiAgentCoordinator](#multiagentcoordinator) - Multi-agent collaboration
- [Graph](#graph) - Graph-based workflows
- [StateGraph](#stategraph) - State-based graph execution
- [Audio](#audio) - Audio processing (speech-to-text, text-to-speech)

### Specialized Features
- [VisionRAG](#visionrag) - Vision + RAG with image understanding
- [WebSearch](#websearch) - Web search integration
- [Evaluator](#evaluator) - LLM evaluation and metrics
- [FineTuningManager](#finetuningmanager) - Model fine-tuning

---

## Installation

```bash
# Basic installation
pip install beanllm

# With all providers
pip install beanllm[all]

# Specific providers
pip install beanllm[openai,anthropic]
```

---

## Quick Start

```python
import asyncio
from beanllm import Client

async def main():
    # Initialize client
    client = Client(model="gpt-4")

    # Simple chat
    response = await client.chat(
        messages=[{"role": "user", "content": "Hello, how are you?"}]
    )
    print(response.content)

asyncio.run(main())
```

---

# API Documentation

## Core Components

### Client

ê¸°ë³¸ LLM í´ë¼ì´ì–¸íŠ¸. ê°€ì¥ ê°„ë‹¨í•œ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

#### `__init__(model, provider=None, api_key=None, **kwargs)`

**íŒŒë¼ë¯¸í„°:**
- `model` (str): ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4", "claude-3-opus", "gemini-pro")
- `provider` (str, optional): Provider ì´ë¦„. ìƒëµ ì‹œ ëª¨ë¸ëª…ì—ì„œ ìë™ ê°ì§€
- `api_key` (str, optional): API í‚¤. ìƒëµ ì‹œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë“œ
- `**kwargs`: Providerë³„ ì¶”ê°€ ì„¤ì •

**ì˜ˆì œ:**
```python
from beanllm import Client

# OpenAI
client = Client(model="gpt-4")

# Anthropic (provider ìë™ ê°ì§€)
client = Client(model="claude-3-opus-20240229")

# ëª…ì‹œì  provider ì§€ì •
client = Client(model="gpt-4", provider="openai")
```

#### `chat(messages, system=None, temperature=None, max_tokens=None, **kwargs)` (async)

ì±„íŒ… ì™„ë£Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `messages` (List[Dict[str, str]]): ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ `[{"role": "user", "content": "..."}]`
- `system` (str, optional): ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
- `temperature` (float, optional): ìƒ˜í”Œë§ ì˜¨ë„ (0.0-2.0)
- `max_tokens` (int, optional): ìµœëŒ€ ìƒì„± í† í° ìˆ˜
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `ChatResponse`

**ì˜ˆì œ:**
```python
response = await client.chat(
    messages=[{"role": "user", "content": "Hello!"}],
    temperature=0.7,
    max_tokens=1000
)
print(response.content)
```

#### `stream_chat(messages, **kwargs)` (async)

ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì±„íŒ… ì™„ë£Œë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:** `chat()`ì™€ ë™ì¼

**ë°˜í™˜:** `AsyncIterator[str]`

**ì˜ˆì œ:**
```python
async for chunk in client.stream_chat(messages=[{"role": "user", "content": "Tell me a story"}]):
    print(chunk, end="", flush=True)
```

---

### RAGChain

RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ. ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.

#### `from_documents(source, chunk_size=500, chunk_overlap=50, embedding_model="text-embedding-3-small", llm_model="gpt-4o-mini", **kwargs)`

íŒ©í† ë¦¬ ë©”ì„œë“œë¡œ RAG ì‹œìŠ¤í…œì„ ìƒì„±í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `source` (str | List): ë¬¸ì„œ ê²½ë¡œ ë˜ëŠ” ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
- `chunk_size` (int): ì²­í¬ í¬ê¸°
- `chunk_overlap` (int): ì²­í¬ ê²¹ì¹¨
- `embedding_model` (str): ì„ë² ë”© ëª¨ë¸ ì´ë¦„
- `llm_model` (str): LLM ëª¨ë¸ ì´ë¦„
- `**kwargs`: ì¶”ê°€ ì„¤ì •

**ì˜ˆì œ:**
```python
from beanllm import RAGChain

rag = RAGChain.from_documents(
    source="documents.txt",
    chunk_size=500,
    embedding_model="text-embedding-3-small",
    llm_model="gpt-4"
)
```

#### `add_documents(documents)` (async)

ë¬¸ì„œë¥¼ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `documents` (List[str] | List[Document]): ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

**ì˜ˆì œ:**
```python
documents = [
    "Python is a programming language.",
    "Machine learning is a subset of AI.",
]
await rag.add_documents(documents)
```

#### `query(question, top_k=3, **kwargs)` (async)

ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `question` (str): ì§ˆë¬¸
- `top_k` (int): ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `RAGResponse`

**ì˜ˆì œ:**
```python
response = await rag.query(
    question="What is Python?",
    top_k=3
)
print(response.answer)
print(response.sources)  # ì‚¬ìš©ëœ ë¬¸ì„œë“¤
```

---

### Agent

ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” AI ì—ì´ì „íŠ¸.

#### `__init__(model, tools=None, max_iterations=10, **kwargs)`

**íŒŒë¼ë¯¸í„°:**
- `model` (str): LLM ëª¨ë¸ ì´ë¦„
- `tools` (List[Tool], optional): ì‚¬ìš©í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸
- `max_iterations` (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
- `**kwargs`: ì¶”ê°€ ì„¤ì •

**ì˜ˆì œ:**
```python
from beanllm import Agent
from beanllm import search_web, calculator

agent = Agent(
    model="gpt-4",
    tools=[search_web, calculator]
)
```

#### `run(task, max_iterations=10, **kwargs)` (async)

ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `task` (str): ìˆ˜í–‰í•  ì‘ì—…
- `max_iterations` (int): ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `AgentResponse`

**ì˜ˆì œ:**
```python
response = await agent.run(
    task="Calculate 123 * 456 and search for the result online",
    max_iterations=5
)
print(response.final_answer)
print(response.steps)  # ì‹¤í–‰ ë‹¨ê³„
```

---

### Chain

ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸.

#### `__init__(client, memory=None, verbose=False)`

**íŒŒë¼ë¯¸í„°:**
- `client` (Client): LLM í´ë¼ì´ì–¸íŠ¸
- `memory` (Memory, optional): ë©”ëª¨ë¦¬ ê°ì²´
- `verbose` (bool): ë””ë²„ê·¸ ì¶œë ¥ ì—¬ë¶€

#### `run(user_input, **kwargs)` (async)

ì²´ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `user_input` (str): ì‚¬ìš©ì ì…ë ¥
- `**kwargs`: ì¶”ê°€ íŒŒë¼ë¯¸í„°

**ë°˜í™˜:** `ChainResult`

**ì˜ˆì œ:**
```python
from beanllm import Chain, Client

client = Client(model="gpt-4")
chain = Chain(client=client)

response = await chain.run("Translate 'hello' to French")
print(response.output)
```

---

## Advanced Features

### MultiAgentCoordinator

ì—¬ëŸ¬ ì—ì´ì „íŠ¸ê°€ í˜‘ì—…í•˜ëŠ” ì‹œìŠ¤í…œ.

#### `__init__(agents, communication_bus=None)`

**íŒŒë¼ë¯¸í„°:**
- `agents` (Dict[str, Agent]): ì—ì´ì „íŠ¸ ë”•ì…”ë„ˆë¦¬ (id: agent)
- `communication_bus` (CommunicationBus, optional): í†µì‹  ë²„ìŠ¤

#### `execute_sequential(task, agent_order, **kwargs)` (async)

ìˆœì°¨ì ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `task` (str): ì‘ì—…
- `agent_order` (List[str]): ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œ

#### `execute_debate(task, agent_ids=None, rounds=3, **kwargs)` (async)

í† ë¡  ë°©ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**ì˜ˆì œ:**
```python
from beanllm import MultiAgentCoordinator, Agent

researcher = Agent(model="gpt-4")
writer = Agent(model="gpt-4")

coordinator = MultiAgentCoordinator(
    agents={"researcher": researcher, "writer": writer}
)

result = await coordinator.execute_sequential(
    task="Research AI trends and write a summary",
    agent_order=["researcher", "writer"]
)
```

---

### Graph

ê·¸ë˜í”„ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°.

#### `__init__(enable_cache=True)`

**íŒŒë¼ë¯¸í„°:**
- `enable_cache` (bool): ìºì‹± í™œì„±í™” ì—¬ë¶€

#### `add_node(node)`

ê·¸ë˜í”„ì— ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `add_edge(from_node, to_node)`

ë…¸ë“œ ê°„ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `run(initial_state, verbose=False)` (async)

ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

---

### StateGraph

ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ ì‹¤í–‰ ì‹œìŠ¤í…œ.

#### `__init__(state_schema=None, config=None)`

**íŒŒë¼ë¯¸í„°:**
- `state_schema` (Dict, optional): ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜
- `config` (GraphConfig, optional): ê·¸ë˜í”„ ì„¤ì •

#### `add_node(name, func)`

ìƒíƒœ ê·¸ë˜í”„ì— ë…¸ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `set_entry_point(node_name)`

ì§„ì…ì ì„ ì„¤ì •í•©ë‹ˆë‹¤.

#### `add_conditional_edge(from_node, condition_func, edge_mapping=None)`

ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

#### `invoke(initial_state, execution_id=None)` (async)

ìƒíƒœ ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.

**ì˜ˆì œ:**
```python
from beanllm import StateGraph

graph = StateGraph(state_schema={"count": 0, "message": ""})

def increment(state):
    state["count"] += 1
    return state

graph.add_node("increment", increment)
graph.set_entry_point("increment")

result = await graph.invoke({"count": 0, "message": "start"})
```

---

### Audio

ìŒì„± ì²˜ë¦¬ (STT, TTS).

#### WhisperSTT - Speech-to-Text

```python
from beanllm import WhisperSTT

stt = WhisperSTT(model="base")
text = stt.transcribe("speech.mp3")
print(text)
```

#### TextToSpeech - Text-to-Speech

```python
from beanllm import TextToSpeech

tts = TextToSpeech(provider="openai", voice="alloy")
audio_bytes = tts.synthesize("Hello, world!")
```

#### AudioRAG - ì˜¤ë””ì˜¤ ê²€ìƒ‰ ë° QA

```python
from beanllm import AudioRAG

audio_rag = AudioRAG()
audio_rag.add_audio("interview.mp3", audio_id="interview_1")
results = audio_rag.search("What did they say about AI?", top_k=3)
```

---

## Specialized Features

### VisionRAG

ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ê¸°ë°˜ RAG.

#### `from_images(source, generate_captions=True, llm_model="gpt-4o", **kwargs)`

ì´ë¯¸ì§€ë¡œë¶€í„° VisionRAGë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `source` (str | List): ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸
- `generate_captions` (bool): ìë™ ìº¡ì…˜ ìƒì„± ì—¬ë¶€
- `llm_model` (str): LLM ëª¨ë¸

**ì˜ˆì œ:**
```python
from beanllm import VisionRAG

vision_rag = VisionRAG.from_images(
    source="images/",
    generate_captions=True,
    llm_model="gpt-4o"
)

# ì´ë¯¸ì§€ ê²€ìƒ‰ ë° ì§ˆì˜
response = vision_rag.query(
    question="What objects are in the images?",
    k=3,
    include_images=True
)
```

---

### WebSearch

ì›¹ ê²€ìƒ‰ í†µí•©.

#### `search(query, engine=None, **kwargs)`

ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `query` (str): ê²€ìƒ‰ ì¿¼ë¦¬
- `engine` (str, optional): ê²€ìƒ‰ ì—”ì§„ ("google", "bing", "duckduckgo")

**ì˜ˆì œ:**
```python
from beanllm import WebSearch

search = WebSearch(default_engine="duckduckgo")
results = search.search("latest AI news")

for result in results:
    print(result.title, result.url)
```

---

### Evaluator

LLM í‰ê°€ ë° ë©”íŠ¸ë¦­.

#### `evaluate(prediction, reference, **kwargs)`

ëª¨ë¸ ì¶œë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

**íŒŒë¼ë¯¸í„°:**
- `prediction` (str): ì˜ˆì¸¡ ê²°ê³¼
- `reference` (str): ì •ë‹µ ì°¸ì¡°

**ë°˜í™˜:** `EvaluationResult`

**ì˜ˆì œ:**
```python
from beanllm import Evaluator

evaluator = Evaluator(metrics=["bleu", "rouge", "f1"])
result = evaluator.evaluate(
    prediction="The cat sat on the mat",
    reference="A cat was sitting on the mat"
)
print(result.scores)
```

---

### FineTuningManager

ëª¨ë¸ íŒŒì¸íŠœë‹.

#### `prepare_and_upload(examples, output_path, validate=True)`

í›ˆë ¨ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê³  ì—…ë¡œë“œí•©ë‹ˆë‹¤.

#### `start_training(model, training_file, validation_file=None, **kwargs)`

íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.

**ì˜ˆì œ:**
```python
from beanllm import FineTuningManager

manager = FineTuningManager(provider="openai")

# ë°ì´í„° ì¤€ë¹„
file_id = manager.prepare_and_upload(
    examples=[...],
    output_path="training.jsonl"
)

# í›ˆë ¨ ì‹œì‘
job = manager.start_training(
    model="gpt-3.5-turbo",
    training_file=file_id
)

# ì§„í–‰ ìƒí™© í™•ì¸
progress = manager.get_training_progress(job.id)
```

---

## Common Types

### Response Objects

All facade methods return specific response objects:

- `ChatResponse` - Chat completion response
- `RAGResponse` - RAG query response
- `AgentResponse` - Agent execution response
- `AudioResponse` - Audio processing response
- `EvaluationResponse` - Evaluation results
- etc.

### Common Parameters

Most facades support these common parameters:

- `model` (str): Model name (e.g., "gpt-4", "claude-3-opus")
- `temperature` (float): Sampling temperature (0.0 - 2.0)
- `max_tokens` (int): Maximum tokens to generate
- `stream` (bool): Enable streaming responses

---

## Error Handling

```python
import asyncio
from beanllm import Client
from beanllm.utils.exceptions import LLMKitError

async def main():
    try:
        client = Client(model="gpt-4")
        response = await client.chat(
            messages=[{"role": "user", "content": "Hello"}]
        )
        print(response.content)
    except LLMKitError as e:
        print(f"Error: {e}")

asyncio.run(main())
```

---

## Environment Variables

beanllm uses environment variables for API keys:

```bash
# OpenAI
export OPENAI_API_KEY="your-key"

# Anthropic
export ANTHROPIC_API_KEY="your-key"

# Google
export GOOGLE_API_KEY="your-key"

# Or use .env file
```

---

## Additional Resources

- [GitHub Repository](https://github.com/leebeanbin/beanllm)
- [PyPI Package](https://pypi.org/project/beanllm/)
- [Examples](../examples/)
- [Architecture Guide](../ARCHITECTURE.md)

---

**Last Updated:** 2025-12-28
**Version:** 0.1.1
