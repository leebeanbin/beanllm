"""
ML Tools (Audio, OCR, Evaluation) - ê¸°ì¡´ beanllm ML ê¸°ëŠ¥ì„ MCP toolë¡œ wrapping

ğŸ¯ í•µì‹¬: ìƒˆë¡œìš´ ì½”ë“œë¥¼ ë§Œë“¤ì§€ ì•Šê³  ê¸°ì¡´ ì½”ë“œë¥¼ í•¨ìˆ˜í™”!
"""
import asyncio
from typing import List, Dict, Any, Optional
from pathlib import Path
from fastmcp import FastMCP

# ê¸°ì¡´ beanllm ì½”ë“œ import (wrapping ëŒ€ìƒ)
from beanllm.facade.ml import AudioFacade, EvaluationFacade
from beanllm.domain.ocr import beanOCR
from beanllm.dto.request.audio import AudioRequest
from beanllm.dto.request.evaluation import EvaluationRequest
from mcp_server.config import MCPServerConfig

# FastMCP ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
mcp = FastMCP("ML Tools")


# =============================================================================
# Audio Tools - ìŒì„± ì¸ì‹ ë° ì „ì‚¬
# =============================================================================


@mcp.tool()
async def transcribe_audio(
    audio_path: str,
    engine: str = "whisper",
    language: Optional[str] = None,
    model_size: str = "base",
) -> dict:
    """
    ìŒì„± íŒŒì¼ ì „ì‚¬ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (.mp3, .wav, .m4a ë“±)
        engine: ì „ì‚¬ ì—”ì§„ (whisper, distil-whisper, canary, etc.)
        language: ì–¸ì–´ ì½”ë“œ (Noneì´ë©´ ìë™ ê°ì§€)
        model_size: ëª¨ë¸ í¬ê¸° (tiny, base, small, medium, large)

    Returns:
        dict: ì „ì‚¬ í…ìŠ¤íŠ¸, íƒ€ì„ìŠ¤íƒ¬í”„, ì‹ ë¢°ë„

    Example:
        User: "ì´ ìŒì„± íŒŒì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì¤˜"
        â†’ transcribe_audio(
            audio_path="/path/to/audio.mp3",
            engine="whisper"
        )
    """
    try:
        # ğŸ¯ ê¸°ì¡´ AudioFacade ì‚¬ìš©!
        request = AudioRequest(
            audio_path=audio_path,
            engine=engine,
            language=language,
            model_size=model_size,
        )

        facade = AudioFacade()
        result = await asyncio.to_thread(facade.transcribe, request)

        return {
            "success": True,
            "text": result.text,
            "segments": result.segments,
            "language": result.language,
            "confidence": result.confidence,
            "duration_seconds": result.duration,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }


@mcp.tool()
async def batch_transcribe_audio(
    audio_paths: List[str],
    engine: str = "whisper",
    language: Optional[str] = None,
) -> dict:
    """
    ì—¬ëŸ¬ ìŒì„± íŒŒì¼ ì¼ê´„ ì „ì‚¬ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        audio_paths: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        engine: ì „ì‚¬ ì—”ì§„
        language: ì–¸ì–´ ì½”ë“œ

    Returns:
        dict: ê° íŒŒì¼ì˜ ì „ì‚¬ ê²°ê³¼

    Example:
        User: "ì´ í´ë”ì˜ ëª¨ë“  ìŒì„± íŒŒì¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì¤˜"
        â†’ batch_transcribe_audio(
            audio_paths=["/path/to/audio1.mp3", "/path/to/audio2.mp3"]
        )
    """
    try:
        results = []
        facade = AudioFacade()

        for audio_path in audio_paths:
            request = AudioRequest(
                audio_path=audio_path,
                engine=engine,
                language=language,
            )
            result = await asyncio.to_thread(facade.transcribe, request)
            results.append(
                {
                    "file": audio_path,
                    "text": result.text,
                    "language": result.language,
                    "duration": result.duration,
                }
            )

        return {
            "success": True,
            "total_files": len(audio_paths),
            "results": results,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }


# =============================================================================
# OCR Tools - ë¬¸ì„œ ì´ë¯¸ì§€ ì¸ì‹
# =============================================================================


@mcp.tool()
async def recognize_text_ocr(
    image_path: str,
    engine: str = "tesseract",
    language: str = "eng",
    preprocess: bool = True,
) -> dict:
    """
    ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        image_path: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ (.png, .jpg, .jpeg, etc.)
        engine: OCR ì—”ì§„ (tesseract, easyocr, paddleocr, surya, etc.)
        language: ì–¸ì–´ ì½”ë“œ (eng, kor, etc.)
        preprocess: ì „ì²˜ë¦¬ í™œì„±í™” (ë…¸ì´ì¦ˆ ì œê±°, ê¸°ìš¸ê¸° ë³´ì • ë“±)

    Returns:
        dict: ì¶”ì¶œëœ í…ìŠ¤íŠ¸, ë°”ìš´ë”© ë°•ìŠ¤, ì‹ ë¢°ë„

    Example:
        User: "ì´ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•´ì¤˜"
        â†’ recognize_text_ocr(
            image_path="/path/to/image.png",
            engine="tesseract"
        )
    """
    try:
        # ğŸ¯ ê¸°ì¡´ beanOCR ì‚¬ìš©!
        ocr = beanOCR(engine=engine, lang=language, preprocess=preprocess)

        result = await asyncio.to_thread(ocr.extract_text, image_path)

        return {
            "success": True,
            "text": result.text,
            "boxes": result.boxes,
            "confidence": result.confidence,
            "engine": engine,
            "language": language,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }


@mcp.tool()
async def batch_recognize_text_ocr(
    image_paths: List[str],
    engine: str = "tesseract",
    language: str = "eng",
) -> dict:
    """
    ì—¬ëŸ¬ ì´ë¯¸ì§€ ì¼ê´„ OCR ì²˜ë¦¬ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ëª©ë¡
        engine: OCR ì—”ì§„
        language: ì–¸ì–´ ì½”ë“œ

    Returns:
        dict: ê° ì´ë¯¸ì§€ì˜ OCR ê²°ê³¼

    Example:
        User: "ì´ í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•´ì¤˜"
        â†’ batch_recognize_text_ocr(
            image_paths=["/path/to/img1.png", "/path/to/img2.png"]
        )
    """
    try:
        results = []
        ocr = beanOCR(engine=engine, lang=language)

        for image_path in image_paths:
            result = await asyncio.to_thread(ocr.extract_text, image_path)
            results.append(
                {
                    "file": image_path,
                    "text": result.text,
                    "confidence": result.confidence,
                }
            )

        return {
            "success": True,
            "total_files": len(image_paths),
            "results": results,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }


# =============================================================================
# Evaluation Tools - ëª¨ë¸ í‰ê°€ ë° ë²¤ì¹˜ë§ˆí‚¹
# =============================================================================


@mcp.tool()
async def evaluate_model(
    model: str,
    evaluation_type: str = "answer_relevancy",
    test_data: Optional[List[Dict[str, Any]]] = None,
    metrics: Optional[List[str]] = None,
) -> dict:
    """
    LLM ëª¨ë¸ í‰ê°€ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        model: í‰ê°€í•  ëª¨ë¸ëª…
        evaluation_type: í‰ê°€ ìœ í˜• (answer_relevancy, faithfulness, context_recall, etc.)
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„° [{"question": "...", "answer": "...", "context": "..."}]
        metrics: í‰ê°€ ë©”íŠ¸ë¦­ ëª©ë¡ (Noneì´ë©´ ê¸°ë³¸ê°’)

    Returns:
        dict: í‰ê°€ ì ìˆ˜, ìƒì„¸ ê²°ê³¼

    Example:
        User: "qwen2.5:0.5b ëª¨ë¸ ì„±ëŠ¥ í‰ê°€í•´ì¤˜"
        â†’ evaluate_model(
            model="qwen2.5:0.5b",
            evaluation_type="answer_relevancy"
        )
    """
    try:
        # ğŸ¯ ê¸°ì¡´ EvaluationFacade ì‚¬ìš©!
        request = EvaluationRequest(
            model=model,
            evaluation_type=evaluation_type,
            test_data=test_data or [],
            metrics=metrics or ["answer_relevancy", "faithfulness"],
        )

        facade = EvaluationFacade()
        result = await asyncio.to_thread(facade.evaluate, request)

        return {
            "success": True,
            "model": model,
            "evaluation_type": evaluation_type,
            "overall_score": result.overall_score,
            "metric_scores": result.metric_scores,
            "details": result.details,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }


@mcp.tool()
async def benchmark_models(
    models: List[str],
    benchmark_type: str = "lm_eval_harness",
    tasks: Optional[List[str]] = None,
) -> dict:
    """
    ì—¬ëŸ¬ ëª¨ë¸ ë²¤ì¹˜ë§ˆí‚¹ ë¹„êµ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        models: ë²¤ì¹˜ë§ˆí¬í•  ëª¨ë¸ ëª©ë¡
        benchmark_type: ë²¤ì¹˜ë§ˆí¬ ìœ í˜• (lm_eval_harness, ragas, deepeval, etc.)
        tasks: íƒœìŠ¤í¬ ëª©ë¡ (Noneì´ë©´ ê¸°ë³¸ê°’)

    Returns:
        dict: ê° ëª¨ë¸ì˜ ë²¤ì¹˜ë§ˆí¬ ì ìˆ˜, ìˆœìœ„

    Example:
        User: "qwen2.5ë‘ llama ëª¨ë¸ ì„±ëŠ¥ ë¹„êµí•´ì¤˜"
        â†’ benchmark_models(
            models=["qwen2.5:0.5b", "llama3.2:1b"],
            benchmark_type="lm_eval_harness"
        )
    """
    try:
        results = []
        facade = EvaluationFacade()

        for model in models:
            request = EvaluationRequest(
                model=model,
                evaluation_type=benchmark_type,
                tasks=tasks or ["hellaswag", "arc_easy"],
            )
            result = await asyncio.to_thread(facade.benchmark, request)
            results.append(
                {
                    "model": model,
                    "overall_score": result.overall_score,
                    "task_scores": result.task_scores,
                }
            )

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results_sorted = sorted(
            results, key=lambda x: x["overall_score"], reverse=True
        )

        return {
            "success": True,
            "benchmark_type": benchmark_type,
            "total_models": len(models),
            "results": results_sorted,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }


@mcp.tool()
async def compare_model_outputs(
    models: List[str],
    prompt: str,
    temperature: float = 0.7,
) -> dict:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ (ê¸°ì¡´ ì½”ë“œ ì¬ì‚¬ìš©)

    Args:
        models: ë¹„êµí•  ëª¨ë¸ ëª©ë¡
        prompt: ë™ì¼í•œ í”„ë¡¬í”„íŠ¸
        temperature: ìƒì„± ì˜¨ë„

    Returns:
        dict: ê° ëª¨ë¸ì˜ ì¶œë ¥, í† í° ìˆ˜, ì‘ë‹µ ì‹œê°„

    Example:
        User: "qwen2.5ë‘ llamaë¡œ ê°™ì€ ì§ˆë¬¸ì— ë‹µë³€ ë¹„êµí•´ì¤˜"
        â†’ compare_model_outputs(
            models=["qwen2.5:0.5b", "llama3.2:1b"],
            prompt="AIì˜ ë¯¸ë˜ëŠ”?"
        )
    """
    try:
        # ê¸°ì¡´ Client ì‚¬ìš©
        from beanllm.facade.core import Client
        import time

        results = []

        for model in models:
            client = Client(model=model)
            start_time = time.time()

            response = await client.chat(
                messages=[{"role": "user", "content": prompt}], temperature=temperature
            )

            elapsed_time = time.time() - start_time

            results.append(
                {
                    "model": model,
                    "output": response.content,
                    "tokens": response.usage.get("total_tokens", 0),
                    "response_time_seconds": round(elapsed_time, 2),
                }
            )

        return {
            "success": True,
            "prompt": prompt,
            "total_models": len(models),
            "results": results,
        }

    except Exception as e:
        return {
            "success": False,
            "error": str(e),
        }
