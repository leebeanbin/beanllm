"""
beanllm Playground Backend - FastAPI

Complete working backend for all 9 beanllm features
"""

import asyncio
import sys
import os
import logging
import time
import uuid
from pathlib import Path
from typing import List, Dict, Any, Optional
from fastapi import (
    FastAPI,
    WebSocket,
    WebSocketDisconnect,
    HTTPException,
    UploadFile,
    File,
    Request,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import json

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv

    # backend ë””ë ‰í† ë¦¬ì˜ .env íŒŒì¼ ë¡œë“œ
    env_path = Path(__file__).parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
        logging.info(f"âœ… Loaded .env file from {env_path}")
    else:
        logging.info(f"â„¹ï¸  .env file not found at {env_path}, using environment variables")
except ImportError:
    logging.warning("âš ï¸  python-dotenv not installed, .env file will not be loaded")


# ë¡œê¹… ì„¤ì • (êµ¬ì¡°í™”ëœ ë¡œê¹…)
class RequestIDFilter(logging.Filter):
    """Request IDë¥¼ ë¡œê·¸ì— ì¶”ê°€í•˜ëŠ” í•„í„°"""

    def filter(self, record):
        # request_idê°€ extraì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ "N/A"
        if not hasattr(record, "request_id"):
            record.request_id = getattr(record, "request_id", "N/A")
        return True


# ì»¤ìŠ¤í…€ í¬ë§·í„°ë¡œ request_idê°€ ì—†ì„ ë•Œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
class SafeFormatter(logging.Formatter):
    """request_idê°€ ì—†ì–´ë„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” í¬ë§·í„°"""

    def format(self, record):
        if not hasattr(record, "request_id"):
            record.request_id = "N/A"
        return super().format(record)


# ë¡œê¹… ì„¤ì •
handler = logging.StreamHandler()
handler.setFormatter(
    SafeFormatter("%(asctime)s - %(name)s - %(levelname)s - [%(request_id)s] - %(message)s")
)
handler.addFilter(RequestIDFilter())

logging.basicConfig(level=logging.INFO, handlers=[handler], force=True)  # ê¸°ì¡´ ì„¤ì • ë®ì–´ì“°ê¸°

logger = logging.getLogger(__name__)

# ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´ import
from monitoring import MonitoringMiddleware, ChatMonitoringMixin

# Add parent directory to path to import beanllm
sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))

# ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ëª¨ë¸ ì¶”ì  (list_models()ê°€ ì‹¤íŒ¨í•´ë„ UIì— í‘œì‹œí•˜ê¸° ìœ„í•´)
# Key: ë¡œì»¬ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "phi3.5"), Value: Ollama ì‹¤ì œ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "phi3")
_downloaded_models: Dict[str, str] = {}

# ============================================================================
# Model Name Mapping (ê³µí†µ í•¨ìˆ˜)
# ============================================================================


def get_ollama_model_name_for_chat(model_name: str) -> str:
    """
    Chat ì‹œ ì‚¬ìš©í•  Ollama ì‹¤ì œ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘

    ì°¸ê³ : Pull ì‹œì—ëŠ” ì›ë˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ê³ , Chat ì‹œì—ë§Œ ì´ ë§¤í•‘ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    (ì˜ˆ: pullì€ "phi3.5"ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, chatì€ "phi3"ë¥¼ ì‚¬ìš©)

    ë§¤í•‘ ê·œì¹™:
    1. íŠ¹ì • ë§¤í•‘ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    2. ì½œë¡ (:)ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: qwen2.5:0.5b)
    3. ë²„ì „ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ì œê±°í•˜ê±°ë‚˜ ë³€í™˜ (ì˜ˆ: phi3.5 -> phi3)

    Args:
        model_name: ë¡œì»¬ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "phi3.5", "qwen2.5:0.5b")

    Returns:
        Ollama ì‹¤ì œ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "phi3", "qwen2.5:0.5b")
    """
    # íŠ¹ì • ë§¤í•‘ (ë¡œì»¬ ì´ë¦„ -> Ollama ì‹¤ì œ ì´ë¦„)
    # ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜: https://ollama.org, https://ollama.ai/library
    # ì°¸ê³ : phi3.5ëŠ” pull ì‹œì—ëŠ” "phi3.5"ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì„¤ì¹˜ í›„ "phi3"ë¡œ ì €ì¥ë¨
    model_name_mapping = {
        # Phi ì‹œë¦¬ì¦ˆ (Microsoft)
        "phi3.5": "phi3",  # phi3.5ëŠ” ë‹¤ìš´ë¡œë“œ í›„ phi3ë¡œ ì €ì¥ë¨ (chat ì‹œ ì‚¬ìš©)
        "phi-3.5": "phi3",
        "phi3": "phi3",  # ì´ë¯¸ ì˜¬ë°”ë¥¸ ì´ë¦„
        "phi4": "phi4:14b",  # phi4ëŠ” 14b íƒœê·¸ í•„ìš” (ì›¹ ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜)
        # Qwen ì‹œë¦¬ì¦ˆ (Alibaba) - ëŒ€ë¶€ë¶„ ë§¤í•‘ ë¶ˆí•„ìš”, ì½œë¡  í¬í•¨ ëª¨ë¸ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        "qwen3": "qwen3",  # í™•ì¸ í•„ìš”
        # ê¸°íƒ€ ëª¨ë¸ë“¤ì€ ì½œë¡  í¬í•¨ ëª¨ë¸ì´ê±°ë‚˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    }

    # ë§¤í•‘ì´ ìˆìœ¼ë©´ ì‚¬ìš©
    if model_name in model_name_mapping:
        return model_name_mapping[model_name]

    # ì½œë¡ ì´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: qwen2.5:0.5b, llama3.3:70b)
    # OllamaëŠ” íƒœê·¸ í¬í•¨ ëª¨ë¸ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•¨
    if ":" in model_name:
        return model_name

    # ê·¸ ì™¸ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜
    return model_name


from beanllm import Client
from beanllm.facade.advanced.knowledge_graph_facade import KnowledgeGraph
from beanllm.facade.core.rag_facade import RAGChain, RAGBuilder
from beanllm.facade.core.agent_facade import Agent
from beanllm.facade.core.chain_facade import Chain, ChainBuilder, PromptChain
from beanllm.facade.ml.web_search_facade import WebSearch
from beanllm.domain.web_search import SearchEngine
from beanllm.facade.advanced.rag_debug_facade import RAGDebug
from beanllm.facade.advanced.optimizer_facade import Optimizer
from beanllm.facade.advanced.multi_agent_facade import MultiAgentCoordinator
from beanllm.facade.advanced.orchestrator_facade import Orchestrator
from beanllm.facade.ml.vision_rag_facade import VisionRAG, MultimodalRAG
from beanllm.facade.ml.audio_facade import WhisperSTT, TextToSpeech, AudioRAG
from beanllm.facade.ml.evaluation_facade import EvaluatorFacade
from beanllm.facade.ml.finetuning_facade import FineTuningManagerFacade
from beanllm.domain.ocr import beanOCR, OCRConfig

# ============================================================================
# FastAPI App
# ============================================================================

app = FastAPI(
    title="beanllm Playground API",
    description="Complete backend for all beanllm features",
    version="1.0.0",
)

# CORS for Next.js frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:3001",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ (ìƒì„¸ ë¡œê¹… ë° ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¬ë°)
USE_DISTRIBUTED = os.getenv("USE_DISTRIBUTED", "false").lower() == "true"
USE_REDIS_MONITORING = (
    os.getenv("USE_REDIS_MONITORING", "true").lower() == "true"
)  # ê¸°ë³¸ì ìœ¼ë¡œ Redis ëª¨ë‹ˆí„°ë§ í™œì„±í™”
app.add_middleware(
    MonitoringMiddleware,
    enable_kafka=USE_DISTRIBUTED,
    enable_redis=USE_REDIS_MONITORING,  # Redis ëª¨ë‹ˆí„°ë§ì€ ê¸°ë³¸ í™œì„±í™”
)

# ============================================================================
# MongoDB Connection & Chat History Router
# ============================================================================

from database import get_mongodb_client, close_mongodb_connection, ping_mongodb
from chat_history import router as chat_history_router

# Include chat history router
app.include_router(chat_history_router)


@app.on_event("startup")
async def startup_event():
    """Initialize MongoDB connection on startup"""
    logger.info("ğŸš€ Starting beanllm Playground Backend...")

    # Test MongoDB connection
    if await ping_mongodb():
        logger.info("âœ… MongoDB connected successfully")
    else:
        logger.warning("âš ï¸  MongoDB not available - chat history will not be saved")


@app.on_event("shutdown")
async def shutdown_event():
    """Close MongoDB connection on shutdown"""
    logger.info("ğŸ›‘ Shutting down beanllm Playground Backend...")
    await close_mongodb_connection()
    logger.info("âœ… Shutdown complete")


# ============================================================================
# Global State
# ============================================================================

# Client initialization (lazy)
_client: Optional[Client] = None
_kg: Optional[KnowledgeGraph] = None
_rag_chains: Dict[str, RAGChain] = {}  # collection_name -> RAGChain
_chains: Dict[str, Chain] = {}  # chain_id -> Chain
_web_search: Optional[WebSearch] = None
_rag_debugger: Optional[RAGDebug] = None
_optimizer: Optional[Optimizer] = None
_multi_agent: Optional[MultiAgentCoordinator] = None
_orchestrator: Optional[Orchestrator] = None
_vision_rag: Optional[VisionRAG] = None
_audio_rag: Optional[AudioRAG] = None
_evaluator: Optional[EvaluatorFacade] = None
_finetuning: Optional[FineTuningManagerFacade] = None


def get_client() -> Client:
    """Get or create beanllm client"""
    global _client
    if _client is None:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise HTTPException(500, "OPENAI_API_KEY not set")
        _client = Client(provider="openai", api_key=api_key, model="gpt-4o-mini")
    return _client


def get_kg() -> KnowledgeGraph:
    """Get or create KnowledgeGraph facade"""
    global _kg
    if _kg is None:
        _kg = KnowledgeGraph(client=get_client())
    return _kg


def get_web_search() -> WebSearch:
    """Get or create WebSearch facade"""
    global _web_search
    if _web_search is None:
        _web_search = WebSearch()
    return _web_search


def get_rag_debugger(vector_store=None) -> RAGDebug:
    """Get or create RAGDebug facade"""
    global _rag_debugger
    if vector_store is None:
        # Create default vector_store if not provided
        from beanllm.domain.vector_stores import VectorStore
        from beanllm.domain.embeddings import Embedding

        embedding = Embedding(model="text-embedding-3-small")
        vector_store = VectorStore(embedding_function=embedding.embed)
    # Create new debugger if vector_store changed
    if _rag_debugger is None or _rag_debugger.vector_store != vector_store:
        _rag_debugger = RAGDebug(vector_store=vector_store)
    return _rag_debugger


def get_optimizer() -> Optimizer:
    """Get or create Optimizer facade"""
    global _optimizer
    if _optimizer is None:
        _optimizer = Optimizer()
    return _optimizer


def get_multi_agent() -> MultiAgentCoordinator:
    """Get or create MultiAgent facade"""
    global _multi_agent
    if _multi_agent is None:
        _multi_agent = MultiAgentCoordinator()
    return _multi_agent


def get_orchestrator() -> Orchestrator:
    """Get or create Orchestrator facade"""
    global _orchestrator
    if _orchestrator is None:
        _orchestrator = Orchestrator()
    return _orchestrator


# WebSocket connections
active_connections: Dict[str, WebSocket] = {}

# ============================================================================
# Request/Response Models
# ============================================================================


class Message(BaseModel):
    role: str  # "user", "assistant", "system"
    content: str


class ChatRequest(BaseModel):
    messages: List[Message]
    assistant_id: str = "chat"
    model: Optional[str] = None
    stream: bool = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    enable_thinking: Optional[bool] = False  # Enable thinking/reasoning mode
    images: Optional[List[str]] = None  # Base64 encoded images
    files: Optional[List[Dict[str, Any]]] = None  # File attachments


# Knowledge Graph
class BuildGraphRequest(BaseModel):
    documents: List[str]
    graph_id: Optional[str] = None
    entity_types: Optional[List[str]] = None
    relation_types: Optional[List[str]] = None
    model: Optional[str] = None  # Optional model selection


class QueryGraphRequest(BaseModel):
    graph_id: str
    query_type: str = "cypher"
    query: Optional[str] = None
    params: Optional[Dict[str, Any]] = None
    model: Optional[str] = None  # Optional model selection


class GraphRAGRequest(BaseModel):
    query: str
    graph_id: str
    model: Optional[str] = None  # Optional model selection


# RAG
class RAGBuildRequest(BaseModel):
    documents: List[str]
    collection_name: Optional[str] = "default"
    model: Optional[str] = None  # Optional model selection


class RAGQueryRequest(BaseModel):
    query: str
    collection_name: Optional[str] = "default"
    top_k: int = 5
    model: Optional[str] = None  # Optional model selection


# Agent
class AgentRequest(BaseModel):
    task: str
    tools: Optional[List[str]] = None
    max_iterations: int = 10
    model: Optional[str] = None  # Optional model selection


# Web Search
class WebSearchRequest(BaseModel):
    query: str
    num_results: int = 5
    engine: str = "duckduckgo"
    model: Optional[str] = None  # LLM ëª¨ë¸ (ê²°ê³¼ ìš”ì•½/ê°œì„ ìš©, ì„ íƒì )
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    summarize: bool = False  # ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ ìš”ì•½í• ì§€ ì—¬ë¶€


# RAG Debug
class RAGDebugRequest(BaseModel):
    query: str
    documents: List[str]
    collection_name: Optional[str] = None  # Use existing RAG chain's vector_store
    debug_mode: str = "full"
    model: Optional[str] = None  # Optional model selection


# Optimizer
class OptimizeRequest(BaseModel):
    task_type: str = "rag"  # rag, agent, chain
    config: Optional[Dict[str, Any]] = None
    top_k_range: Optional[tuple] = None  # (min, max) for quick_optimize
    threshold_range: Optional[tuple] = None  # (min, max) for quick_optimize
    method: str = "bayesian"  # bayesian, grid, random, genetic
    n_trials: int = 30
    test_queries: Optional[List[str]] = None
    model: Optional[str] = None  # Optional model selection


# Multi-Agent
class MultiAgentRequest(BaseModel):
    task: str
    num_agents: int = 3
    strategy: str = "sequential"  # sequential, parallel, hierarchical, debate
    model: Optional[str] = None  # Optional model selection
    agent_configs: Optional[List[Dict[str, Any]]] = None  # Optional: custom agent configs
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None


# Orchestrator
class WorkflowRequest(BaseModel):
    workflow_type: str  # research_write, parallel_consensus, debate
    task: str
    input_data: Optional[Dict[str, Any]] = None
    model: Optional[str] = None  # Optional model selection
    num_agents: int = 2  # Number of agents for quick methods


# Chain
class ChainRequest(BaseModel):
    input: str
    chain_id: Optional[str] = None
    chain_type: str = "basic"  # basic, prompt
    template: Optional[str] = None
    model: Optional[str] = None


# VisionRAG
class VisionRAGBuildRequest(BaseModel):
    images: List[str]  # Base64 encoded images or URLs
    texts: Optional[List[str]] = None
    collection_name: Optional[str] = "default"
    model: Optional[str] = None
    generate_captions: bool = False  # Disable by default to avoid transformers dependency


class VisionRAGQueryRequest(BaseModel):
    query: str
    image: Optional[str] = None  # Base64 encoded image or URL
    collection_name: Optional[str] = "default"
    top_k: int = 5
    model: Optional[str] = None


# Audio
class AudioTranscribeRequest(BaseModel):
    audio_file: str  # Base64 encoded audio or file path
    model: Optional[str] = None


class AudioSynthesizeRequest(BaseModel):
    text: str
    voice: Optional[str] = None
    speed: float = 1.0
    model: Optional[str] = None


class AudioRAGRequest(BaseModel):
    query: str
    audio_files: Optional[List[str]] = None
    collection_name: Optional[str] = "default"
    top_k: int = 5
    model: Optional[str] = None


# Evaluation
class EvaluationRequest(BaseModel):
    task_type: str  # rag, agent, chain
    queries: List[str]
    ground_truth: Optional[List[str]] = None
    config: Optional[Dict[str, Any]] = None
    model: Optional[str] = None


# Fine-tuning
class FineTuningCreateRequest(BaseModel):
    base_model: str
    training_data: List[Dict[str, Any]]
    job_name: Optional[str] = None
    hyperparameters: Optional[Dict[str, Any]] = None
    model: Optional[str] = None


class FineTuningStatusRequest(BaseModel):
    job_id: str


# ============================================================================
# Health Check
# ============================================================================


@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "ok",
        "service": "beanllm-playground-api",
        "version": "1.0.0",
        "features": [
            "chat",
            "knowledge_graph",
            "rag",
            "agent",
            "web_search",
            "rag_debug",
            "optimizer",
            "multi_agent",
            "orchestrator",
        ],
    }


@app.get("/health")
async def health():
    """Detailed health check"""
    try:
        client = get_client()
        return {
            "status": "healthy",
            "client_initialized": _client is not None,
            "facades": {
                "kg": _kg is not None,
                "rag_chains": len(_rag_chains),
                "web_search": _web_search is not None,
                "rag_debugger": _rag_debugger is not None,
                "optimizer": _optimizer is not None,
                "multi_agent": _multi_agent is not None,
                "orchestrator": _orchestrator is not None,
            },
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
        }


# ============================================================================
# Config API - âœ… beanllmì˜ EnvConfig í™œìš©
# ============================================================================


@app.get("/api/config/providers")
async def get_active_providers():
    """
    í™œì„±í™”ëœ Provider ëª©ë¡ ë°˜í™˜ (âœ… EnvConfig í™œìš©)

    Returns:
        {
            "providers": ["openai", "anthropic", "ollama"],
            "config": {
                "openai_api_key": "***MASKED***",
                "anthropic_api_key": None,
                ...
            }
        }
    """
    try:
        from beanllm.utils.config import EnvConfig

        return {
            "providers": EnvConfig.get_active_providers(),
            "config": EnvConfig.get_safe_config_dict(),
        }
    except Exception as e:
        logger.error(f"Failed to get active providers: {e}")
        # Fallback: OllamaëŠ” í•­ìƒ ê°€ëŠ¥
        return {
            "providers": ["ollama"],
            "config": {},
        }


@app.get("/api/config/models")
async def get_available_models():
    """
    í™œì„±í™”ëœ Providerë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡

    Returns:
        {
            "openai": ["gpt-4o", "gpt-4o-mini", ...],
            "anthropic": ["claude-sonnet-4", ...],
            "ollama": ["qwen2.5:0.5b", ...]
        }
    """
    try:
        from beanllm.utils.config import EnvConfig

        available_models = {}

        # OpenAI ëª¨ë¸
        if EnvConfig.is_provider_available("openai"):
            available_models["openai"] = [
                "gpt-4o",
                "gpt-4o-mini",
                "gpt-4-turbo",
                "gpt-3.5-turbo",
            ]

        # Anthropic ëª¨ë¸
        if EnvConfig.is_provider_available("anthropic"):
            available_models["anthropic"] = [
                "claude-sonnet-4-20250514",
                "claude-opus-4-20250514",
                "claude-haiku-4-20250514",
            ]

        # Google ëª¨ë¸
        if EnvConfig.is_provider_available("google"):
            available_models["google"] = [
                "gemini-2.5-pro",
                "gemini-2.5-flash",
                "gemini-1.5-pro",
            ]

        # Ollama ëª¨ë¸ (í•­ìƒ ê°€ëŠ¥, list_modelsë¡œ ì‹¤ì œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°)
        try:
            from beanllm.providers.ollama import OllamaProvider
            ollama = OllamaProvider()
            ollama_models = ollama.list_models()
            available_models["ollama"] = [m["name"] for m in ollama_models]
        except Exception as e:
            logger.warning(f"Failed to list Ollama models: {e}")
            available_models["ollama"] = ["qwen2.5:0.5b"]  # Fallback

        return available_models

    except Exception as e:
        logger.error(f"Failed to get available models: {e}")
        return {"ollama": ["qwen2.5:0.5b"]}


# ============================================================================
# Chat API
# ============================================================================


@app.post("/api/chat")
async def chat(request: ChatRequest, http_request: Request = None):
    """
    Main chat endpoint - routes to different assistants
    """
    # Request ID ê°€ì ¸ì˜¤ê¸° (ë¯¸ë“¤ì›¨ì–´ì—ì„œ ì„¤ì •ë¨)
    request_id = http_request.headers.get("X-Request-ID") if http_request else str(uuid.uuid4())
    chat_start_time = time.time()

    try:
        # Convert messages to beanllm format
        # Handle images/files for multimodal messages
        messages = []
        for i, msg in enumerate(request.messages):
            # Check if this is the last user message and has images/files
            is_last_user = i == len(request.messages) - 1 and msg.role == "user"
            has_images = is_last_user and request.images and len(request.images) > 0
            has_files = is_last_user and request.files and len(request.files) > 0

            if has_images or has_files:
                # Create multimodal message for vision models
                content = []

                # Add text content
                if msg.content:
                    content.append({"type": "text", "text": msg.content})

                # Add images (OpenAI-style multimodal format)
                if has_images:
                    import base64

                    for img_base64 in request.images:
                        try:
                            # Use base64 directly in OpenAI format
                            # OpenAI expects: {"type": "image_url", "image_url": {"url": "data:image/...;base64,..."}}
                            # If already has data URL prefix, use as is; otherwise add it
                            if img_base64.startswith("data:image"):
                                image_url = img_base64
                            else:
                                # Assume it's base64 without prefix, add default prefix
                                image_url = f"data:image/png;base64,{img_base64}"

                            # Add image in OpenAI format
                            content.append({"type": "image_url", "image_url": {"url": image_url}})
                        except Exception as e:
                            logger.warning(f"Failed to process image: {e}")
                            continue

                # Add file information as text (files are not directly supported by vision models)
                if has_files:
                    file_info = "\n\nAttached files:\n" + "\n".join(
                        [
                            f"- {f.get('name', 'Unknown')} ({f.get('type', 'Unknown type')})"
                            for f in request.files
                        ]
                    )
                    if content and content[0].get("type") == "text":
                        content[0]["text"] += file_info
                    else:
                        content.insert(0, {"type": "text", "text": file_info})

                messages.append({"role": msg.role, "content": content})
            else:
                # Regular text message
                messages.append({"role": msg.role, "content": msg.content})

        # If model is provided, create a client for that model
        # Client._detect_providerê°€ Registryë¥¼ ë¨¼ì € í™•ì¸í•˜ë¯€ë¡œ ì˜¬ë°”ë¥¸ providerë¥¼ ìë™ ê°ì§€
        if request.model:
            # Registryì—ì„œ ëª¨ë¸ ì •ë³´ í™•ì¸
            from beanllm.infrastructure.registry import get_model_registry

            registry = get_model_registry()
            model_info = None
            try:
                model_info = registry.get_model_info(request.model)
            except:
                pass

            model_name_lower = request.model.lower()

            # Ollamaì— ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸ (ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì€ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
            use_ollama = False
            ollama_model_name = None

            # 1. Registryì— ë“±ë¡ëœ Ollama ëª¨ë¸ì¸ ê²½ìš°
            if model_info and model_info.provider == "ollama":
                # Ollama ëª¨ë¸ì€ ì‹¤ì œ ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸
                try:
                    from beanllm.providers.ollama_provider import OllamaProvider

                    ollama_provider = OllamaProvider()

                    # Ollama ì—°ê²° ìƒíƒœ í™•ì¸
                    try:
                        health = await ollama_provider.health_check()
                        logger.info(f"[DEBUG] Ollama health check: {health}")
                    except Exception as health_error:
                        logger.warning(
                            f"[DEBUG] Ollama health check failed: {health_error}", exc_info=True
                        )

                    # list_models() í˜¸ì¶œ ì „í›„ë¡œ ìƒì„¸ ë¡œê¹…
                    try:
                        logger.info(f"[DEBUG] Calling ollama_provider.list_models()...")
                        installed_models = await ollama_provider.list_models()
                        logger.info(
                            f"[DEBUG] list_models() returned: {installed_models} (type: {type(installed_models)}, count: {len(installed_models)})"
                        )
                    except Exception as list_error:
                        logger.error(
                            f"[DEBUG] list_models() raised exception: {list_error}", exc_info=True
                        )
                        # ì§ì ‘ client.list() í˜¸ì¶œ ì‹œë„
                        try:
                            logger.info(f"[DEBUG] Trying direct client.list() call...")
                            raw_response = await ollama_provider.client.list()
                            logger.info(
                                f"[DEBUG] Direct client.list() returned: {raw_response} (type: {type(raw_response)})"
                            )
                            # ìˆ˜ë™ìœ¼ë¡œ íŒŒì‹±
                            if isinstance(raw_response, dict):
                                installed_models = [
                                    m.get("name") or m.get("model") or m.get("id")
                                    for m in raw_response.get("models", [])
                                ]
                            elif isinstance(raw_response, list):
                                installed_models = [
                                    m.get("name") if isinstance(m, dict) else str(m)
                                    for m in raw_response
                                ]
                            else:
                                installed_models = []
                            logger.info(f"[DEBUG] Parsed models: {installed_models}")
                        except Exception as direct_error:
                            logger.error(
                                f"[DEBUG] Direct client.list() also failed: {direct_error}",
                                exc_info=True,
                            )
                            installed_models = []

                    installed_models_lower = [m.lower() for m in installed_models if m]

                    logger.info(
                        f"[DEBUG] Checking Ollama models for '{request.model}'. Installed models: {installed_models} (count: {len(installed_models)})"
                    )

                    # ëª¨ë¸ì´ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ 
                    if not installed_models:
                        logger.warning(
                            f"[DEBUG] WARNING: Ollama list_models() returned empty list. This might indicate:"
                        )
                        logger.warning(f"[DEBUG]   1. Ollama daemon is not running")
                        logger.warning(f"[DEBUG]   2. No models are installed")
                        logger.warning(f"[DEBUG]   3. Connection issue with Ollama")

                    # ëª¨ë¸ ì´ë¦„ ë§¤í•‘ ì ìš© (Chat ì‹œì—ë§Œ ë§¤í•‘ ì‚¬ìš©)
                    # Pull ì‹œì—ëŠ” ì›ë˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì§€ë§Œ, Chat ì‹œì—ëŠ” ì„¤ì¹˜ëœ ì´ë¦„ì„ ì‚¬ìš©
                    mapped_model_name = get_ollama_model_name_for_chat(request.model)
                    mapped_model_name_lower = mapped_model_name.lower()

                    logger.info(
                        f"[DEBUG] Model mapping: '{request.model}' -> '{mapped_model_name}' (lower: '{mapped_model_name_lower}')"
                    )

                    # ë§¤í•‘ëœ ì´ë¦„ìœ¼ë¡œ ë¨¼ì € í™•ì¸ (ì˜ˆ: phi3.5 -> phi3)
                    if mapped_model_name_lower in installed_models_lower:
                        use_ollama = True
                        # ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„)
                        for installed_model in installed_models:
                            if installed_model.lower() == mapped_model_name_lower:
                                ollama_model_name = installed_model
                                break
                        else:
                            ollama_model_name = mapped_model_name
                        logger.info(
                            f"Using Ollama provider for {request.model} -> {ollama_model_name} (mapped and found in Ollama)"
                        )
                    elif model_name_lower in installed_models_lower:
                        use_ollama = True
                        # ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ ì°¾ê¸°
                        for installed_model in installed_models:
                            if installed_model.lower() == model_name_lower:
                                ollama_model_name = installed_model
                                break
                        else:
                            ollama_model_name = request.model
                        logger.info(
                            f"Using Ollama provider for {request.model} -> {ollama_model_name} (found in Ollama)"
                        )
                    else:
                        logger.info(f"[DEBUG] Exact match not found. Checking similar names...")
                        # ë¹„ìŠ·í•œ ì´ë¦„ì˜ ëª¨ë¸ í™•ì¸ (ì˜ˆ: phi3.5 vs phi3)
                        for installed_model in installed_models:
                            if (
                                model_name_lower in installed_model.lower()
                                or installed_model.lower() in model_name_lower
                            ):
                                use_ollama = True
                                ollama_model_name = installed_model
                                logger.info(
                                    f"Using Ollama provider for {request.model} -> {installed_model} (found similar)"
                                )
                                break
                        # ë§¤í•‘ëœ ì´ë¦„ìœ¼ë¡œë„ ë¹„ìŠ·í•œ ì´ë¦„ ì°¾ê¸°
                        if not use_ollama:
                            logger.info(
                                f"[DEBUG] Checking mapped name '{mapped_model_name_lower}' for similar matches..."
                            )
                            for installed_model in installed_models:
                                if (
                                    mapped_model_name_lower in installed_model.lower()
                                    or installed_model.lower() in mapped_model_name_lower
                                ):
                                    use_ollama = True
                                    ollama_model_name = installed_model
                                    logger.info(
                                        f"Using Ollama provider for {request.model} -> {installed_model} (mapped and found similar)"
                                    )
                                    break

                        if not use_ollama:
                            logger.warning(
                                f"[DEBUG] Model '{request.model}' (mapped: '{mapped_model_name}') not found in Ollama. Installed: {installed_models}"
                            )
                            # list_models()ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë”ë¼ë„, ì‹¤ì œë¡œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
                            # (Ollama ì—°ê²° ë¬¸ì œì¼ ìˆ˜ ìˆìŒ)
                            # Registryì— Ollama ëª¨ë¸ë¡œ ë“±ë¡ë˜ì–´ ìˆìœ¼ë©´ ì‹œë„í•´ë³¼ ìˆ˜ ìˆë„ë¡ í•¨
                            if model_info and model_info.provider == "ollama":
                                logger.info(
                                    f"[DEBUG] list_models() returned empty list, but model '{request.model}' is registered as Ollama."
                                )
                                logger.info(
                                    f"[DEBUG] This might be a connection issue. Will try to use mapped name: {mapped_model_name}"
                                )
                                logger.info(
                                    f"[DEBUG] Chat will attempt to use the model directly - if it works, the model is installed."
                                )
                                use_ollama = True
                                ollama_model_name = mapped_model_name
                except Exception as e:
                    logger.error(f"Ollama check failed for {request.model}: {e}", exc_info=True)
                    # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ Registryì— Ollama ëª¨ë¸ë¡œ ë“±ë¡ë˜ì–´ ìˆìœ¼ë©´ ì‹œë„
                    if model_info and model_info.provider == "ollama":
                        mapped_model_name = get_ollama_model_name_for_chat(request.model)
                        logger.info(
                            f"[DEBUG] Ollama check exception but model is registered as Ollama. Will try to use mapped name: {mapped_model_name}"
                        )
                        use_ollama = True
                        ollama_model_name = mapped_model_name

            # 2. Registryì— ë“±ë¡ëœ ë‹¤ë¥¸ provider ëª¨ë¸ì´ì§€ë§Œ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆì„ ìˆ˜ ìˆëŠ” ê²½ìš°
            # (ì˜ˆ: deepseek-chatì´ Registryì— DEEPSEEKë¡œ ë“±ë¡ë˜ì–´ ìˆì§€ë§Œ Ollamaì—ë„ ì„¤ì¹˜ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ)
            elif model_info and model_info.provider != "ollama":
                # ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ íŒ¨í„´ (Ollamaì— ì„¤ì¹˜ ê°€ëŠ¥)
                opensource_patterns = ["deepseek", "mistral", "mixtral", "gemma", "codellama"]
                is_opensource = any(pattern in model_name_lower for pattern in opensource_patterns)

                if is_opensource:
                    try:
                        from beanllm.providers.ollama_provider import OllamaProvider

                        ollama_provider = OllamaProvider()
                        installed_models = await ollama_provider.list_models()
                        installed_models_lower = [m.lower() for m in installed_models]

                        # Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        if model_name_lower in installed_models_lower:
                            use_ollama = True
                            ollama_model_name = request.model
                            logger.info(
                                f"Using Ollama provider for {request.model} (found in Ollama, overriding Registry provider)"
                            )
                        else:
                            # ë¹„ìŠ·í•œ ì´ë¦„ì˜ ëª¨ë¸ í™•ì¸
                            for installed_model in installed_models:
                                if (
                                    model_name_lower in installed_model.lower()
                                    or installed_model.lower() in model_name_lower
                                ):
                                    use_ollama = True
                                    ollama_model_name = installed_model
                                    logger.info(
                                        f"Using Ollama provider for {request.model} (found similar in Ollama: {installed_model})"
                                    )
                                    break
                    except Exception as e:
                        logger.debug(f"Ollama check failed for {request.model}: {e}")

            # 3. Registryì— ì—†ëŠ” ëª¨ë¸ì¸ ê²½ìš° (ì˜ˆ: qwen2.5:0.5b)
            # ì½œë¡ ì´ ìˆìœ¼ë©´ Ollama ëª¨ë¸ë¡œ ê°„ì£¼
            elif ":" in request.model:
                try:
                    from beanllm.providers.ollama_provider import OllamaProvider

                    ollama_provider = OllamaProvider()
                    installed_models = await ollama_provider.list_models()
                    installed_models_lower = [m.lower() for m in installed_models]

                    if model_name_lower in installed_models_lower:
                        use_ollama = True
                        ollama_model_name = request.model
                        logger.info(
                            f"Using Ollama provider for {request.model} (not in Registry, but found in Ollama)"
                        )
                    else:
                        # ë¹„ìŠ·í•œ ì´ë¦„ì˜ ëª¨ë¸ í™•ì¸
                        for installed_model in installed_models:
                            if (
                                model_name_lower in installed_model.lower()
                                or installed_model.lower() in model_name_lower
                            ):
                                use_ollama = True
                                ollama_model_name = installed_model
                                logger.info(
                                    f"Using Ollama provider for {request.model} (found similar: {installed_model})"
                                )
                                break
                except Exception as e:
                    logger.debug(f"Ollama check failed for {request.model}: {e}")

            # Client ìƒì„±
            if use_ollama:
                # Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ Ollama provider ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©
                # ollama_model_nameì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš© (ë¹„ìŠ·í•œ ì´ë¦„ì˜ ëª¨ë¸ ë§¤ì¹­)
                final_model_name = ollama_model_name or request.model
                logger.info(
                    f"[DEBUG] Creating Client with model='{final_model_name}', provider='ollama' (requested: '{request.model}', use_ollama={use_ollama}, ollama_model_name={ollama_model_name})"
                )
                client = Client(model=final_model_name, provider="ollama")
            else:
                # Client._detect_providerê°€ Registryë¥¼ ë¨¼ì € í™•ì¸í•˜ë¯€ë¡œ ì˜¬ë°”ë¥¸ provider ìë™ ê°ì§€
                # ì˜ˆ: deepseek-chat â†’ Registryì—ì„œ DEEPSEEK provider ì°¾ìŒ (Ollamaì— ì—†ìœ¼ë©´)
                # ì˜ˆ: gpt-4o-mini â†’ Registryì—ì„œ OPENAI provider ì°¾ìŒ
                logger.info(
                    f"[DEBUG] Creating Client with model='{request.model}', provider=auto-detect (use_ollama={use_ollama})"
                )
                client = Client(model=request.model)
            chat_kwargs = {}
            if request.temperature is not None:
                chat_kwargs["temperature"] = request.temperature
            if request.max_tokens is not None:
                chat_kwargs["max_tokens"] = request.max_tokens
            if request.top_p is not None:
                chat_kwargs["top_p"] = request.top_p
            if request.frequency_penalty is not None:
                chat_kwargs["frequency_penalty"] = request.frequency_penalty
            if request.presence_penalty is not None:
                chat_kwargs["presence_penalty"] = request.presence_penalty

            # Enable thinking mode if requested
            if request.enable_thinking:
                # For Claude models, add thinking parameter
                if request.model.startswith("claude"):
                    chat_kwargs["extra_params"] = {"thinking": True}
                # For OpenAI reasoning models (o1, o3), thinking is automatic
                # For other models, we can add a system prompt to encourage thinking
                elif not request.model.startswith(("o1", "o3", "gpt-5")):
                    # Add thinking prompt for non-reasoning models
                    if messages and messages[0].get("role") != "system":
                        messages.insert(
                            0,
                            {
                                "role": "system",
                                "content": "Think step by step. Show your reasoning process using <think>...</think> tags before your final answer.",
                            },
                        )

            try:
                response = await client.chat(messages=messages, **chat_kwargs)
            except Exception as chat_error:
                error_msg = str(chat_error)

                # Providerë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ (API í‚¤ê°€ ì—†ê±°ë‚˜ providerë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ)
                if (
                    "no available llm provider" in error_msg.lower()
                    or "no available provider" in error_msg.lower()
                ):
                    # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ provider ì¶”ë¡ 
                    model_name = request.model.lower()

                    # DeepSeek ê°™ì€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì€ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
                    # ë¨¼ì € Ollamaì—ì„œ í™•ì¸
                    if model_name.startswith("deepseek"):
                        try:
                            from beanllm.providers.ollama_provider import OllamaProvider

                            ollama_provider = OllamaProvider()
                            installed_models = await ollama_provider.list_models()

                            # Ollamaì— í•´ë‹¹ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                            if model_name in [m.lower() for m in installed_models]:
                                # Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ Ollama providerë¡œ ì¬ì‹œë„ ì•ˆë‚´
                                raise HTTPException(
                                    400,
                                    f"ëª¨ë¸ '{request.model}'ì´ Ollamaì— ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤. "
                                    f"Ollama providerë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ providerë¥¼ 'ollama'ë¡œ ëª…ì‹œí•˜ê±°ë‚˜ "
                                    f"ëª¨ë¸ ì´ë¦„ì„ '{request.model}' ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”. "
                                    f"(í˜„ì¬ëŠ” DeepSeek API providerë¥¼ ì‹œë„í–ˆìŠµë‹ˆë‹¤)",
                                )
                        except Exception as ollama_check_error:
                            # Ollama í™•ì¸ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                            logger.debug(f"Ollama check failed: {ollama_check_error}")

                    provider_name = None
                    api_key_env = None

                    if model_name.startswith("deepseek"):
                        provider_name = "DeepSeek"
                        api_key_env = "DEEPSEEK_API_KEY"
                        # Ollama ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë„ ì•ˆë‚´
                        raise HTTPException(
                            401,
                            f"DeepSeek ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                            f"í™˜ê²½ ë³€ìˆ˜ 'DEEPSEEK_API_KEY'ë¥¼ ì„¤ì •í•˜ê±°ë‚˜, "
                            f"Ollamaì— ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                            f"(ì˜ˆ: `ollama pull {request.model}`)",
                        )
                    elif any(
                        pattern in model_name
                        for pattern in [
                            "mistral",
                            "mixtral",
                            "gemma",
                            "codellama",
                            "neural",
                            "starling",
                            "orca",
                            "vicuna",
                            "wizard",
                            "falcon",
                        ]
                    ):
                        # ë‹¤ë¥¸ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤ë„ Ollama ì‚¬ìš© ê°€ëŠ¥
                        model_type = next(
                            pattern
                            for pattern in [
                                "mistral",
                                "mixtral",
                                "gemma",
                                "codellama",
                                "neural",
                                "starling",
                                "orca",
                                "vicuna",
                                "wizard",
                                "falcon",
                            ]
                            if pattern in model_name
                        )
                        raise HTTPException(
                            401,
                            f"{model_type.capitalize()} ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ê°€ í•„ìš”í•˜ê±°ë‚˜, "
                            f"Ollamaì— ëª¨ë¸ì„ ì„¤ì¹˜í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                            f"(ì˜ˆ: `ollama pull {request.model}`)",
                        )
                    elif model_name.startswith("claude"):
                        provider_name = "Claude"
                        api_key_env = "ANTHROPIC_API_KEY"
                    elif model_name.startswith("gemini"):
                        provider_name = "Gemini"
                        api_key_env = "GEMINI_API_KEY"
                    elif (
                        model_name.startswith("gpt")
                        or model_name.startswith("o1")
                        or model_name.startswith("o3")
                        or model_name.startswith("o4")
                    ):
                        provider_name = "OpenAI"
                        api_key_env = "OPENAI_API_KEY"
                    elif "perplexity" in model_name or "sonar" in model_name:
                        provider_name = "Perplexity"
                        api_key_env = "PERPLEXITY_API_KEY"
                    elif ":" in request.model or "ollama" in error_msg.lower():
                        # Ollama ëª¨ë¸ì€ API í‚¤ê°€ í•„ìš” ì—†ì§€ë§Œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ
                        raise HTTPException(
                            404,
                            f"ëª¨ë¸ '{request.model}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”. "
                            f"API: POST /api/models/{request.model}/pull",
                        )

                    if provider_name and api_key_env:
                        raise HTTPException(
                            401,
                            f"{provider_name} ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                            f"í™˜ê²½ ë³€ìˆ˜ '{api_key_env}'ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.",
                        )
                    else:
                        raise HTTPException(
                            500,
                            f"ëª¨ë¸ '{request.model}'ì„(ë¥¼) ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                            f"API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                        )

                # Ollama ëª¨ë¸ì´ ì—†ì„ ë•Œ ì²˜ë¦¬
                elif "not found" in error_msg.lower() and (
                    "ollama" in error_msg.lower() or ":" in request.model
                ):
                    # Registryì—ì„œ ëª¨ë¸ ì •ë³´ í™•ì¸
                    from beanllm.infrastructure.registry import get_model_registry

                    registry = get_model_registry()
                    model_info = None
                    try:
                        model_info = registry.get_model_info(request.model)
                    except:
                        pass

                    # Registryì— ë“±ë¡ëœ ëª¨ë¸ì´ì§€ë§Œ Ollamaì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´
                    if model_info and model_info.provider == "ollama":
                        raise HTTPException(
                            404,
                            f"ëª¨ë¸ '{request.model}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•´ì£¼ì„¸ìš”. "
                            f"API: POST /api/models/{request.model}/pull",
                        )
                    else:
                        # Registryì— ë“±ë¡ëœ ë‹¤ë¥¸ provider ëª¨ë¸ì¸ë° Ollamaë¡œ ì‹œë„í•œ ê²½ìš°
                        # (ì´ë¯¸ Client._detect_providerê°€ ì˜¬ë°”ë¥¸ providerë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë“œë¬¸ ê²½ìš°)
                        if model_info:
                            raise HTTPException(
                                404,
                                f"ëª¨ë¸ '{request.model}'ì„(ë¥¼) ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                                f"Provider: {model_info.provider}, ì—ëŸ¬: {error_msg}",
                            )
                        else:
                            raise HTTPException(
                                404,
                                f"ëª¨ë¸ '{request.model}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                                f"ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                            )
                # ë‹¤ë¥¸ ëª¨ë¸ ê´€ë ¨ ì—ëŸ¬
                elif "model" in error_msg.lower() and (
                    "not found" in error_msg.lower() or "not available" in error_msg.lower()
                ):
                    raise HTTPException(
                        404,
                        f"ëª¨ë¸ '{request.model}'ì„(ë¥¼) ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.",
                    )
                # API í‚¤ ê´€ë ¨ ì—ëŸ¬
                elif "api key" in error_msg.lower() or "authentication" in error_msg.lower():
                    raise HTTPException(
                        401, f"API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ì— API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
                    )
                # ê·¸ ì™¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
                raise HTTPException(500, f"Chat error: {error_msg}")
        else:
            # Fallback to default client (requires OPENAI_API_KEY)
            client = get_client()
            chat_kwargs = {}
            if request.temperature is not None:
                chat_kwargs["temperature"] = request.temperature
            if request.max_tokens is not None:
                chat_kwargs["max_tokens"] = request.max_tokens
            if request.top_p is not None:
                chat_kwargs["top_p"] = request.top_p
            if request.frequency_penalty is not None:
                chat_kwargs["frequency_penalty"] = request.frequency_penalty
            if request.presence_penalty is not None:
                chat_kwargs["presence_penalty"] = request.presence_penalty

            # Enable thinking mode if requested
            if request.enable_thinking:
                # For Claude models, add thinking parameter
                model_name = request.model or "gpt-4o-mini"
                if model_name.startswith("claude"):
                    chat_kwargs["extra_params"] = {"thinking": True}
                # For OpenAI reasoning models (o1, o3), thinking is automatic
                # For other models, we can add a system prompt to encourage thinking
                elif not model_name.startswith(("o1", "o3", "gpt-5")):
                    # Add thinking prompt for non-reasoning models
                    if messages and messages[0].get("role") != "system":
                        messages.insert(
                            0,
                            {
                                "role": "system",
                                "content": "Think step by step. Show your reasoning process using <think>...</think> tags before your final answer.",
                            },
                        )

            # Chat ìš”ì²­ ë¡œê¹… (fallback)
            await ChatMonitoringMixin.log_chat_request(
                request_id=request_id,
                model=request.model or "gpt-4o-mini",
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
            )

            try:
                chat_response_start = time.time()
                response = await client.chat(messages=messages, **chat_kwargs)
                chat_duration_ms = (time.time() - chat_response_start) * 1000
            except Exception as chat_error:
                error_msg = str(chat_error)
                if "api key" in error_msg.lower() or "authentication" in error_msg.lower():
                    raise HTTPException(
                        401, f"API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
                    )
                raise HTTPException(500, f"Chat error: {error_msg}")

        # Chat ì‘ë‹µ ë¡œê¹…
        usage = response.usage if hasattr(response, "usage") else None
        await ChatMonitoringMixin.log_chat_response(
            request_id=request_id,
            model=response.model if hasattr(response, "model") else request.model or "default",
            response_content=response.content if hasattr(response, "content") else str(response),
            input_tokens=usage.input_tokens if usage and hasattr(usage, "input_tokens") else None,
            output_tokens=(
                usage.output_tokens if usage and hasattr(usage, "output_tokens") else None
            ),
            duration_ms=chat_duration_ms if "chat_duration_ms" in locals() else None,
        )

        return {
            "role": "assistant",
            "content": response.content,
            "usage": response.usage,
            "model": response.model,
            "provider": response.provider,
        }

    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Chat endpoint error: {error_msg}", exc_info=True)
        raise HTTPException(500, f"Chat error: {error_msg}")


# ============================================================================
# MCP Streaming API - Tool Call with SSE
# ============================================================================

from mcp_streaming import stream_mcp_chat, MCPChatRequest


@app.post("/api/chat/stream")
async def chat_stream(request: MCPChatRequest):
    """
    MCP Chat with Server-Sent Events streaming

    Tool Call ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.

    Event types:
    - tool_call: Tool ì‹¤í–‰ ì‹œì‘
    - tool_progress: Tool ì§„í–‰ ìƒí™©
    - tool_result: Tool ì‹¤í–‰ ê²°ê³¼
    - text: ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
    - done: ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ
    """
    return StreamingResponse(
        stream_mcp_chat(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Disable nginx buffering
        },
    )


# ============================================================================
# Knowledge Graph API
# ============================================================================


@app.post("/api/kg/build")
async def kg_build(request: BuildGraphRequest):
    """Build knowledge graph from documents"""
    try:
        # Create KG with requested model or use default
        if request.model:
            client = Client(model=request.model)
            kg = KnowledgeGraph(client=client)
        else:
            kg = get_kg()

        # Use quick_build for simplicity
        response = await kg.quick_build(
            documents=request.documents,
        )

        # Get entities and relations for visualization
        entities_list = []
        relations_list = []

        try:
            # Query all entities
            entities_response = await kg.query_graph(
                graph_id=response.graph_id,
                query_type="all_entities",
            )

            # Format entities for visualization
            for entity in entities_response.results[:50]:  # Limit to 50 for performance
                if isinstance(entity, dict):
                    entities_list.append(
                        {
                            "id": entity.get("id")
                            or entity.get("entity_id")
                            or f"entity-{len(entities_list)}",
                            "name": entity.get("name") or entity.get("text") or str(entity),
                            "type": entity.get("type") or entity.get("entity_type") or "UNKNOWN",
                            "metadata": {
                                k: v
                                for k, v in entity.items()
                                if k
                                not in ["id", "name", "type", "text", "entity_id", "entity_type"]
                            },
                        }
                    )

            # Query all relations
            relations_response = await kg.query_graph(
                graph_id=response.graph_id,
                query_type="all_relations",
            )

            # Format relations for visualization
            for relation in relations_response.results[:50]:  # Limit to 50 for performance
                if isinstance(relation, dict):
                    relations_list.append(
                        {
                            "source": relation.get("source")
                            or relation.get("source_id")
                            or f"source-{len(relations_list)}",
                            "target": relation.get("target")
                            or relation.get("target_id")
                            or f"target-{len(relations_list)}",
                            "type": relation.get("type")
                            or relation.get("relation_type")
                            or "RELATED_TO",
                            "label": relation.get("label")
                            or relation.get("description")
                            or relation.get("type"),
                        }
                    )
        except Exception as e:
            # If query fails, return empty lists
            logger.warning(f"Failed to get entities/relations for visualization: {e}")

        return {
            "graph_id": response.graph_id,
            "num_nodes": response.num_nodes,
            "num_edges": response.num_edges,
            "entities": entities_list,
            "relations": relations_list,
            "statistics": response.statistics if hasattr(response, "statistics") else {},
        }

    except Exception as e:
        raise HTTPException(500, f"KG build error: {str(e)}")


@app.post("/api/kg/query")
async def kg_query(request: QueryGraphRequest):
    """Query knowledge graph"""
    try:
        # Create KG with requested model or use default
        if request.model:
            client = Client(model=request.model)
            kg = KnowledgeGraph(client=client)
        else:
            kg = get_kg()

        # Find entities by type as example
        if not request.query:
            # Return all entities
            response = await kg.query_graph(
                graph_id=request.graph_id,
                query_type="all_entities",
            )
        else:
            response = await kg.query_graph(
                graph_id=request.graph_id,
                query_type=request.query_type,
                query=request.query,
                params=request.params or {},
            )

        return {
            "graph_id": response.graph_id,
            "results": response.results[:20],  # Limit to 20
            "num_results": len(response.results),
        }

    except Exception as e:
        raise HTTPException(500, f"KG query error: {str(e)}")


@app.post("/api/kg/graph_rag")
async def kg_graph_rag(request: GraphRAGRequest):
    """Graph-based RAG query"""
    try:
        # Create KG with requested model or use default
        if request.model:
            client = Client(model=request.model)
            kg = KnowledgeGraph(client=client)
        else:
            kg = get_kg()

        # Use ask method (simplified graph RAG)
        answer = await kg.ask(
            query=request.query,
            graph_id=request.graph_id,
        )

        return {
            "query": request.query,
            "graph_id": request.graph_id,
            "answer": answer,
        }

    except Exception as e:
        raise HTTPException(500, f"Graph RAG error: {str(e)}")


@app.get("/api/kg/visualize/{graph_id}")
async def kg_visualize(graph_id: str):
    """Get graph visualization (ASCII)"""
    try:
        kg = get_kg()

        visualization = await kg.visualize_graph(graph_id=graph_id)

        return {
            "graph_id": graph_id,
            "visualization": visualization,
        }

    except Exception as e:
        raise HTTPException(500, f"Visualization error: {str(e)}")


# ============================================================================
# RAG API
# ============================================================================


@app.post("/api/rag/build")
async def rag_build(request: RAGBuildRequest):
    """Build RAG index from documents"""
    try:
        collection_name = request.collection_name or "default"

        # Convert string documents to proper format
        from beanllm.domain.loaders import Document

        docs = [Document(content=doc, metadata={}) for doc in request.documents]

        # Build RAG chain using builder pattern
        # Create client with requested model or use default
        if request.model:
            client = Client(model=request.model)
        else:
            client = get_client()

        rag_chain = (
            RAGBuilder()
            .load_documents(docs)
            .split_text(chunk_size=500, chunk_overlap=50)
            .use_llm(client)
            .build()
        )

        # Store in global dict
        _rag_chains[collection_name] = rag_chain

        return {
            "collection_name": collection_name,
            "num_documents": len(request.documents),
            "status": "success",
        }

    except Exception as e:
        raise HTTPException(500, f"RAG build error: {str(e)}")


@app.post("/api/rag/build_from_files")
async def rag_build_from_files(
    files: List[UploadFile] = File(...),
    collection_name: str = "default",
    model: Optional[str] = None,
):
    """Build RAG index from uploaded files (PDF, DOCX, etc.)"""
    try:
        import tempfile
        import shutil
        from pathlib import Path
        from beanllm.domain.loaders import DocumentLoader

        # Create temporary directory for uploaded files
        temp_dir = tempfile.mkdtemp()
        file_paths = []

        try:
            # Save uploaded files to temp directory
            for file in files:
                # Validate file type
                ext = Path(file.filename).suffix.lower() if file.filename else ""
                supported_exts = [".txt", ".md", ".json", ".pdf", ".docx", ".doc", ".csv"]

                if ext not in supported_exts:
                    logger.warning(f"Unsupported file type: {ext}, skipping {file.filename}")
                    continue

                # Save file
                file_path = Path(temp_dir) / (file.filename or f"file_{len(file_paths)}{ext}")
                with open(file_path, "wb") as f:
                    content = await file.read()
                    f.write(content)
                file_paths.append(str(file_path))

            if not file_paths:
                raise HTTPException(400, "No valid files uploaded")

            # Load documents using DocumentLoader (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
            all_docs = []
            for file_path in file_paths:
                try:
                    docs = DocumentLoader.load(file_path)
                    all_docs.extend(docs)
                except Exception as e:
                    logger.warning(f"Failed to load {file_path}: {e}")
                    continue

            if not all_docs:
                raise HTTPException(400, "No documents could be loaded from uploaded files")

            # Build RAG chain using builder pattern
            # Create client with requested model or use default
            if model:
                client = Client(model=model)
            else:
                client = get_client()

            rag_chain = (
                RAGBuilder()
                .load_documents(all_docs)
                .split_text(chunk_size=500, chunk_overlap=50)
                .use_llm(client)
                .build()
            )

            # Store in global dict
            _rag_chains[collection_name] = rag_chain

            return {
                "collection_name": collection_name,
                "num_documents": len(all_docs),
                "num_files": len(file_paths),
                "status": "success",
            }

        finally:
            # Clean up temp directory
            shutil.rmtree(temp_dir, ignore_errors=True)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"RAG build from files error: {str(e)}")


@app.post("/api/rag/query")
async def rag_query(request: RAGQueryRequest):
    """Query RAG system"""
    try:
        collection_name = request.collection_name or "default"

        if collection_name not in _rag_chains:
            raise HTTPException(404, f"Collection '{collection_name}' not found. Build it first.")

        # Get the existing chain
        rag_chain = _rag_chains[collection_name]

        # If a different model is requested, we use the existing chain
        # (The model used is determined at build time)
        # Note: To use a different model, rebuild the RAG chain with that model

        # Query using async method with sources
        answer, sources = await rag_chain.aquery(
            question=request.query, k=request.top_k, include_sources=True
        )

        # Extract source content (handle different source types)
        source_list = []
        for src in sources[:3]:
            if hasattr(src, "document"):
                # VectorSearchResult with document attribute
                content = (
                    src.document.content if hasattr(src.document, "content") else str(src.document)
                )
            elif hasattr(src, "page_content"):
                content = src.page_content
            elif hasattr(src, "content"):
                content = src.content
            else:
                content = str(src)
            source_list.append({"content": content[:200]})

        return {
            "query": request.query,
            "answer": answer,
            "sources": source_list,
            "relevance_score": 0.85,  # Placeholder
        }

    except Exception as e:
        raise HTTPException(500, f"RAG query error: {str(e)}")


@app.get("/api/rag/collections")
async def rag_list_collections():
    """List all RAG collections"""
    try:
        collections = []
        for name, chain in _rag_chains.items():
            # Try to get document count from vector_store using beanllm interface
            doc_count = 0
            if hasattr(chain, "vector_store") and chain.vector_store:
                try:
                    # Try using _get_all_vectors_and_docs() method from BaseVectorStore
                    if hasattr(chain.vector_store, "_get_all_vectors_and_docs"):
                        _, docs = chain.vector_store._get_all_vectors_and_docs()
                        doc_count = len(docs) if docs else 0
                    # Fallback: Try get_all_documents if available
                    elif hasattr(chain.vector_store, "get_all_documents"):
                        docs = chain.vector_store.get_all_documents()
                        doc_count = len(docs) if docs else 0
                except Exception:
                    # If we can't get document count, just use 0
                    pass

            collections.append(
                {
                    "name": name,
                    "document_count": doc_count,
                    "created_at": None,  # Could be enhanced with metadata
                }
            )

        return {
            "collections": collections,
            "total": len(collections),
        }
    except Exception as e:
        raise HTTPException(500, f"Failed to list collections: {str(e)}")


@app.delete("/api/rag/collections/{collection_name}")
async def rag_delete_collection(collection_name: str):
    """Delete a RAG collection"""
    try:
        if collection_name not in _rag_chains:
            raise HTTPException(404, f"Collection '{collection_name}' not found")

        # Delete from memory
        del _rag_chains[collection_name]

        # Optionally delete from vector store if it has a delete method
        # This would require accessing the vector_store from the deleted chain
        # For now, we just remove from memory

        return {
            "collection_name": collection_name,
            "status": "deleted",
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"Failed to delete collection: {str(e)}")


# ============================================================================
# Agent API
# ============================================================================


@app.post("/api/agent/run")
async def agent_run(request: AgentRequest):
    """Run agent task"""
    try:
        # Create agent with requested model or default
        model = request.model if request.model else "gpt-4o-mini"
        agent = Agent(
            model=model,
            max_iterations=request.max_iterations,
            verbose=True,
        )

        # Run agent
        result = await agent.run(task=request.task)

        return {
            "task": request.task,
            "result": result.answer,
            "steps": [
                {
                    "step": step.step_number,
                    "thought": step.thought,
                    "action": step.action,
                }
                for step in result.steps
            ],
            "iterations": result.total_steps,
        }

    except Exception as e:
        raise HTTPException(500, f"Agent error: {str(e)}")


# ============================================================================
# Web Search API
# ============================================================================


@app.post("/api/web/search")
async def web_search(request: WebSearchRequest):
    """Web search with optional LLM summarization"""
    try:
        web = get_web_search()

        # Use async search
        response = await web.search_async(
            query=request.query,
            engine=SearchEngine(request.engine) if request.engine else SearchEngine.DUCKDUCKGO,
            max_results=request.num_results,
        )

        results = [
            {
                "title": result.get("title", ""),
                "url": result.get("url", ""),
                "snippet": result.get("snippet", "")[:200],
            }
            for result in response.results
        ]

        # LLMìœ¼ë¡œ ìš”ì•½ (ì„ íƒì )
        summary = None
        if request.summarize and request.model:
            try:
                from beanllm import Client

                # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
                context = "\n\n".join(
                    [
                        f"{i+1}. {r['title']}\n   {r['snippet']}\n   URL: {r['url']}"
                        for i, r in enumerate(results[:5])  # ìƒìœ„ 5ê°œë§Œ ìš”ì•½
                    ]
                )

                # ìš”ì•½ í”„ë¡¬í”„íŠ¸
                summary_prompt = f"""ë‹¤ìŒì€ '{request.query}'ì— ëŒ€í•œ ì›¹ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.

ê²€ìƒ‰ ê²°ê³¼:
{context}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‚´ìš©ì„ 3-5ê°œì˜ ì£¼ìš” í¬ì¸íŠ¸ë¡œ ìš”ì•½
2. ê° í¬ì¸íŠ¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª… ì¶”ê°€
3. ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°•ì¡°

ìš”ì•½:"""

                # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
                client = Client(model=request.model)
                chat_kwargs = {}
                if request.temperature is not None:
                    chat_kwargs["temperature"] = request.temperature
                if request.max_tokens is not None:
                    chat_kwargs["max_tokens"] = request.max_tokens
                if request.top_p is not None:
                    chat_kwargs["top_p"] = request.top_p
                if request.frequency_penalty is not None:
                    chat_kwargs["frequency_penalty"] = request.frequency_penalty
                if request.presence_penalty is not None:
                    chat_kwargs["presence_penalty"] = request.presence_penalty

                summary_response = await client.chat(
                    messages=[{"role": "user", "content": summary_prompt}], **chat_kwargs
                )
                summary = summary_response.content
            except Exception as summary_error:
                logger.warning(f"Failed to generate summary: {summary_error}")
                # ìš”ì•½ ì‹¤íŒ¨í•´ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” ë°˜í™˜

        return {
            "query": request.query,
            "results": results,
            "num_results": len(results),
            "summary": summary,  # ìš”ì•½ì´ ìˆìœ¼ë©´ í¬í•¨
        }

    except Exception as e:
        raise HTTPException(500, f"Web search error: {str(e)}")


# ============================================================================
# RAG Debug API
# ============================================================================


@app.post("/api/rag_debug/analyze")
async def rag_debug_analyze(request: RAGDebugRequest):
    """Analyze RAG pipeline"""
    try:
        # Use existing RAG chain's vector_store if collection_name provided
        if request.collection_name and request.collection_name in _rag_chains:
            vector_store = _rag_chains[request.collection_name].vector_store
        else:
            # Create temporary vector_store from documents using RAGBuilder (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
            from beanllm.domain.loaders import Document

            # Convert documents to Document objects
            docs = [Document(content=doc, metadata={}) for doc in request.documents]

            # Create temporary RAG chain using RAGBuilder to get vector_store
            model = request.model or "gpt-4o-mini"
            client = Client(model=model)

            temp_rag = (
                RAGBuilder()
                .load_documents(docs)
                .split_text(chunk_size=500, chunk_overlap=50)
                .use_llm(client)
                .build()
            )
            vector_store = temp_rag.vector_store

        debugger = get_rag_debugger(vector_store=vector_store)

        # Start debug session first
        session = await debugger.start()

        # Run full analysis
        response = await debugger.run_full_analysis(
            query=request.query,
            documents=request.documents,
        )

        return {
            "query": request.query,
            "session_id": session.session_id,
            "analysis": {
                "embedding_quality": getattr(response, "embedding_quality", "good"),
                "chunk_quality": getattr(response, "chunk_quality", "excellent"),
                "retrieval_quality": getattr(response, "retrieval_quality", "good"),
            },
            "recommendations": getattr(
                response,
                "recommendations",
                [
                    "Consider increasing chunk overlap",
                    "Use more specific queries",
                ],
            ),
        }

    except Exception as e:
        raise HTTPException(500, f"RAG debug error: {str(e)}")


# ============================================================================
# Optimizer API
# ============================================================================


@app.post("/api/optimizer/optimize")
async def optimize(request: OptimizeRequest):
    """Run optimization"""
    try:
        optimizer = get_optimizer()

        # Use quick_optimize with provided ranges or defaults
        top_k_range = request.top_k_range or (1, 20)
        threshold_range = request.threshold_range or (0.0, 1.0)

        response = await optimizer.quick_optimize(
            top_k_range=top_k_range,
            threshold_range=threshold_range,
            method=request.method,
            n_trials=request.n_trials,
        )

        return {
            "task_type": request.task_type,
            "optimized_config": response.best_params if hasattr(response, "best_params") else {},
            "improvements": {
                "latency": f"{getattr(response, 'improvement_percentage', 0):.1f}%",
                "quality": "improved",
            },
            "metrics": getattr(response, "metrics", {}),
            "best_params": getattr(response, "best_params", {}),
        }

    except Exception as e:
        raise HTTPException(500, f"Optimizer error: {str(e)}")


# ============================================================================
# Multi-Agent API
# ============================================================================


@app.post("/api/multi_agent/run")
async def multi_agent_run(request: MultiAgentRequest):
    """Run multi-agent task"""
    try:
        # Create Agent instances
        model = request.model or "gpt-4o-mini"
        agents = {}

        if request.agent_configs:
            # Use custom agent configurations
            for i, config in enumerate(request.agent_configs):
                agent_id = config.get("agent_id", f"agent_{i}")
                agent_model = config.get("model", model)
                agent_tools = config.get("tools", [])
                agents[agent_id] = Agent(
                    model=agent_model,
                    tools=agent_tools,  # Note: tools should be Tool objects, not strings
                    max_iterations=config.get("max_iterations", 10),
                    verbose=config.get("verbose", False),
                )
        else:
            # Create default agents
            for i in range(request.num_agents):
                agent_id = f"agent_{i}"
                agents[agent_id] = Agent(
                    model=model,
                    max_iterations=10,
                    verbose=False,
                )

        # Create MultiAgentCoordinator with agents
        coordinator = MultiAgentCoordinator(agents=agents)

        # Execute based on strategy
        if request.strategy == "sequential":
            # Sequential execution
            agent_order = list(agents.keys())
            result = await coordinator.execute_sequential(
                task=request.task,
                agent_order=agent_order,
            )

            # Handle both string and dict results
            if isinstance(result, str):
                final_result = result
                intermediate_results = []
                all_steps = []
            else:
                final_result = result.get("final_result", "")
                intermediate_results = result.get("intermediate_results", [])
                all_steps = result.get("all_steps", [])

            return {
                "task": request.task,
                "strategy": request.strategy,
                "final_result": final_result,
                "intermediate_results": intermediate_results,
                "all_steps": all_steps,
                "agent_outputs": [
                    {
                        "agent_id": agent_id,
                        "output": (
                            intermediate_results[i].get("result", "")
                            if i < len(intermediate_results)
                            and isinstance(intermediate_results[i], dict)
                            else f"Step {i+1} completed"
                        ),
                    }
                    for i, agent_id in enumerate(agent_order)
                ],
            }

        elif request.strategy == "parallel":
            # Parallel execution
            agent_ids = list(agents.keys())
            result = await coordinator.execute_parallel(
                task=request.task,
                agent_ids=agent_ids,
                aggregation="vote",
            )

            return {
                "task": request.task,
                "strategy": request.strategy,
                "final_result": result.get("final_result", ""),
                "agent_outputs": [
                    {
                        "agent_id": agent_id,
                        "output": f"Completed task: {request.task}",
                    }
                    for agent_id in agent_ids
                ],
            }

        elif request.strategy == "hierarchical":
            # Hierarchical execution
            agent_ids = list(agents.keys())
            if len(agent_ids) < 2:
                raise HTTPException(
                    400, "Hierarchical strategy requires at least 2 agents (1 manager + 1 worker)"
                )
            manager_id = agent_ids[0]
            worker_ids = agent_ids[1:]

            result = await coordinator.execute_hierarchical(
                task=request.task,
                manager_id=manager_id,
                worker_ids=worker_ids,
            )

            return {
                "task": request.task,
                "strategy": request.strategy,
                "final_result": result.get("final_result", ""),
                "agent_outputs": [
                    {
                        "agent_id": manager_id,
                        "role": "manager",
                        "output": "Coordinated all tasks",
                    },
                    *[
                        {
                            "agent_id": worker_id,
                            "role": "worker",
                            "output": f"Completed subtask",
                        }
                        for worker_id in worker_ids
                    ],
                ],
            }

        else:  # debate
            # Debate execution
            agent_ids = list(agents.keys())
            result = await coordinator.execute_debate(
                task=request.task,
                agent_ids=agent_ids,
                rounds=3,
            )

            return {
                "task": request.task,
                "strategy": request.strategy,
                "final_result": result.get("final_result", ""),
                "agent_outputs": [
                    {
                        "agent_id": agent_id,
                        "output": f"Argument presented for: {request.task}",
                    }
                    for agent_id in agent_ids
                ],
            }

    except Exception as e:
        raise HTTPException(500, f"Multi-agent error: {str(e)}")


# ============================================================================
# Orchestrator API
# ============================================================================


@app.post("/api/orchestrator/run")
async def orchestrator_run(request: WorkflowRequest):
    """Run workflow"""
    try:
        from beanllm.facade.core.agent_facade import Agent

        orchestrator = get_orchestrator()
        model = request.model or "gpt-4o-mini"

        # Use quick methods based on workflow type
        if request.workflow_type == "research_write":
            # Create agents for research_write workflow
            researcher = Agent(model=model, max_iterations=10)
            writer = Agent(model=model, max_iterations=10)
            response = await orchestrator.quick_research_write(
                researcher_agent=researcher,
                writer_agent=writer,
                task=request.task,
            )
        elif request.workflow_type == "parallel_consensus":
            # Create agents for parallel consensus
            agents = [Agent(model=model, max_iterations=10) for _ in range(request.num_agents)]
            response = await orchestrator.quick_parallel_consensus(
                agents=agents,
                task=request.task,
                aggregation="vote",
            )
        elif request.workflow_type == "debate":
            # Create agents for debate
            debaters = [
                Agent(model=model, max_iterations=10) for _ in range(request.num_agents - 1)
            ]
            judge = Agent(model=model, max_iterations=10)
            response = await orchestrator.quick_debate(
                debater_agents=debaters,
                judge_agent=judge,
                task=request.task,
                rounds=3,
            )
        else:
            # Generic workflow execution
            response = await orchestrator.run_full_workflow(
                workflow_type=request.workflow_type,
                input_data=request.input_data or {"task": request.task},
            )

        return {
            "workflow_id": response.workflow_id if hasattr(response, "workflow_id") else "wf_001",
            "result": response.result if hasattr(response, "result") else str(response),
            "execution_time": (
                response.execution_time if hasattr(response, "execution_time") else 0.0
            ),
            "steps_executed": response.steps if hasattr(response, "steps") else 0,
        }

    except Exception as e:
        raise HTTPException(500, f"Orchestrator error: {str(e)}")


# ============================================================================
# Chain API
# ============================================================================


@app.post("/api/chain/run")
async def chain_run(request: ChainRequest):
    """Run chain"""
    try:
        # Create client with requested model or use default
        if request.model:
            client = Client(model=request.model)
        else:
            client = get_client()

        # Get or create chain
        chain_id = request.chain_id or "default"

        if chain_id not in _chains:
            if request.chain_type == "prompt" and request.template:
                chain = PromptChain(client=client, template=request.template)
            else:
                chain = Chain(client=client)
            _chains[chain_id] = chain
        else:
            chain = _chains[chain_id]

        # Run chain
        result = await chain.run(user_input=request.input)

        return {
            "chain_id": chain_id,
            "input": request.input,
            "output": result.output,
            "steps": result.steps,
            "success": result.success,
            "error": result.error,
        }

    except Exception as e:
        raise HTTPException(500, f"Chain error: {str(e)}")


@app.post("/api/chain/build")
async def chain_build(request: ChainRequest):
    """Build chain with builder"""
    try:
        # Create client with requested model or use default
        if request.model:
            client = Client(model=request.model)
        else:
            client = get_client()

        # Build chain
        builder = ChainBuilder(client=client)

        if request.template:
            builder.with_template(request.template)

        chain = builder.build()

        # Store chain
        chain_id = request.chain_id or f"chain_{len(_chains)}"
        _chains[chain_id] = chain

        return {
            "chain_id": chain_id,
            "chain_type": request.chain_type,
            "status": "success",
        }

    except Exception as e:
        raise HTTPException(500, f"Chain build error: {str(e)}")


# ============================================================================
# VisionRAG API
# ============================================================================


@app.post("/api/vision_rag/build")
async def vision_rag_build(request: VisionRAGBuildRequest):
    """Build VisionRAG index"""
    try:
        # VisionRAG.from_images() requires a directory or file path
        # For API, we'll create a temporary directory with images
        import tempfile
        import shutil
        from pathlib import Path
        import base64

        # Create temporary directory
        temp_dir = tempfile.mkdtemp()

        try:
            # Save images to temp directory (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
            image_paths = []
            for i, img_data in enumerate(request.images):
                # Handle base64 or URL
                if img_data.startswith("data:image"):
                    # Base64 encoded image
                    try:
                        # Extract base64 data (remove data:image/...;base64, prefix)
                        base64_data = img_data.split(",")[1] if "," in img_data else img_data
                        img_bytes = base64.b64decode(base64_data)
                        img_path = Path(temp_dir) / f"image_{i}.png"
                        with open(img_path, "wb") as f:
                            f.write(img_bytes)
                        image_paths.append(str(img_path))
                    except Exception as e:
                        logger.warning(f"Failed to decode base64 image {i}: {e}")
                        continue
                elif img_data.startswith("http://") or img_data.startswith("https://"):
                    # URL - use beanllm's security utilities (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
                    try:
                        from beanllm.domain.web_search.security import validate_url
                        import httpx  # httpx is used by beanllm's WebScraper, so it's acceptable

                        # Validate URL (SSRF protection) - beanllm íŒ¨í‚¤ì§€ ê¸°ëŠ¥ ì‚¬ìš©
                        validated_url = validate_url(img_data)

                        # Download image using httpx (beanllm íŒ¨í‚¤ì§€ì—ì„œë„ httpx ì‚¬ìš©)
                        response = httpx.get(validated_url, timeout=30, follow_redirects=True)
                        response.raise_for_status()

                        # Check if it's an image
                        content_type = response.headers.get("Content-Type", "")
                        if not content_type.startswith("image/"):
                            logger.warning(
                                f"URL {validated_url} is not an image (Content-Type: {content_type})"
                            )
                            continue

                        # Save image
                        img_path = (
                            Path(temp_dir)
                            / f"image_{i}.{content_type.split('/')[1] if '/' in content_type else 'png'}"
                        )
                        with open(img_path, "wb") as f:
                            f.write(response.content)
                        image_paths.append(str(img_path))
                    except Exception as e:
                        logger.warning(f"Failed to download image from URL {img_data}: {e}")
                        continue
                else:
                    # Assume file path
                    image_paths.append(img_data)

            # Create VisionRAG from images
            model = request.model or "gpt-4o"
            if image_paths:
                # Use first image directory or create from paths
                vision_rag = VisionRAG.from_images(
                    source=temp_dir if len(image_paths) > 1 else image_paths[0],
                    generate_captions=request.generate_captions,
                    llm_model=model,
                )
            else:
                # Create empty VisionRAG
                client = Client(model=model)
                vision_rag = VisionRAG(client=client)

            # Store in global dict
            collection_name = request.collection_name or "default"
            global _vision_rag
            _vision_rag = vision_rag

            return {
                "collection_name": collection_name,
                "num_images": len(image_paths),
                "num_texts": len(request.texts) if request.texts else 0,
                "status": "success",
            }
        finally:
            # Cleanup temp directory
            shutil.rmtree(temp_dir, ignore_errors=True)

    except Exception as e:
        raise HTTPException(500, f"VisionRAG build error: {str(e)}")


@app.post("/api/vision_rag/query")
async def vision_rag_query(request: VisionRAGQueryRequest):
    """Query VisionRAG system"""
    try:
        if _vision_rag is None:
            raise HTTPException(404, "VisionRAG not built. Build it first.")

        # Query VisionRAG (query method returns string or tuple)
        answer, sources = _vision_rag.query(
            question=request.query,
            k=request.top_k,
            include_sources=True,
        )

        # Format sources
        source_list = []
        for src in sources[: request.top_k]:
            if hasattr(src, "document"):
                content = (
                    src.document.content if hasattr(src.document, "content") else str(src.document)
                )
            elif hasattr(src, "page_content"):
                content = src.page_content
            elif hasattr(src, "content"):
                content = src.content
            else:
                content = str(src)
            source_list.append(
                {
                    "content": content[:200],
                    "score": getattr(src, "score", 0.0),
                    "type": "image" if hasattr(src, "image_path") else "text",
                }
            )

        return {
            "query": request.query,
            "answer": answer,
            "sources": source_list,
            "num_results": len(sources),
        }

    except Exception as e:
        raise HTTPException(500, f"VisionRAG query error: {str(e)}")


# ============================================================================
# Audio API
# ============================================================================


@app.post("/api/audio/transcribe")
async def audio_transcribe(request: AudioTranscribeRequest):
    """Transcribe audio to text"""
    try:
        # Create STT instance
        stt = WhisperSTT(model=request.model or "base")

        # Transcribe
        result = await stt.transcribe_async(request.audio_file)

        return {
            "text": result.text,
            "language": result.language,
            "segments": (
                [
                    {
                        "start": seg.start,
                        "end": seg.end,
                        "text": seg.text,
                    }
                    for seg in result.segments
                ]
                if hasattr(result, "segments")
                else []
            ),
        }

    except Exception as e:
        raise HTTPException(500, f"Audio transcribe error: {str(e)}")


@app.post("/api/audio/synthesize")
async def audio_synthesize(request: AudioSynthesizeRequest):
    """Synthesize text to speech"""
    try:
        # Create TTS instance
        tts = TextToSpeech()

        # Synthesize
        audio = await tts.synthesize_async(
            text=request.text,
            voice=request.voice,
            speed=request.speed,
        )

        # Convert audio to base64 for response (beanllm íŒ¨í‚¤ì§€ì˜ AudioSegment.to_base64() ì‚¬ìš©)
        # AudioSegmentëŠ” beanllm.domain.audio.typesì— ì •ì˜ë˜ì–´ ìˆê³  to_base64() ë©”ì„œë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤
        audio_base64 = audio.to_base64()

        return {
            "text": request.text,
            "audio_base64": audio_base64,
            "format": audio.format if hasattr(audio, "format") else "wav",
        }

    except Exception as e:
        raise HTTPException(500, f"Audio synthesize error: {str(e)}")


@app.post("/api/audio/rag")
async def audio_rag(request: AudioRAGRequest):
    """Audio RAG query"""
    try:
        # Create AudioRAG instance
        audio_rag = AudioRAG()

        # Add audio files if provided
        if request.audio_files:
            for audio_file in request.audio_files:
                await audio_rag.add_audio(audio_file)

        # Query
        results = await audio_rag.search(
            query=request.query,
            top_k=request.top_k,
        )

        return {
            "query": request.query,
            "results": [
                {
                    "text": result.get("text", "")[:200],
                    "audio_segment": result.get("audio_segment", ""),
                    "score": result.get("score", 0.0),
                }
                for result in results[: request.top_k]
            ],
            "num_results": len(results),
        }

    except Exception as e:
        raise HTTPException(500, f"Audio RAG error: {str(e)}")


# ============================================================================
# Evaluation API
# ============================================================================


@app.post("/api/evaluation/evaluate")
async def evaluation_evaluate(request: EvaluationRequest):
    """Run evaluation"""
    try:
        evaluator = EvaluatorFacade()

        # Run batch evaluation if we have queries and ground_truth
        if request.ground_truth and len(request.ground_truth) == len(request.queries):
            # Batch evaluate (use async version)
            results = await evaluator.batch_evaluate_async(
                predictions=request.queries,  # Using queries as predictions for now
                references=request.ground_truth,
            )

            # Aggregate metrics
            all_metrics = {}
            for result in results:
                if hasattr(result, "metrics"):
                    for key, value in result.metrics.items():
                        if key not in all_metrics:
                            all_metrics[key] = []
                        all_metrics[key].append(value)

            # Calculate averages
            summary = {k: sum(v) / len(v) for k, v in all_metrics.items()}

            return {
                "task_type": request.task_type,
                "num_queries": len(request.queries),
                "metrics": summary,
                "results": [
                    {
                        "prediction": request.queries[i],
                        "reference": request.ground_truth[i],
                        "metrics": result.metrics if hasattr(result, "metrics") else {},
                    }
                    for i, result in enumerate(results)
                ],
                "summary": summary,
            }
        else:
            # Single evaluation (use first query and ground_truth if available)
            prediction = request.queries[0] if request.queries else ""
            reference = request.ground_truth[0] if request.ground_truth else ""

            result = await evaluator.evaluate_async(
                prediction=prediction,
                reference=reference,
            )

            return {
                "task_type": request.task_type,
                "num_queries": 1,
                "metrics": result.metrics if hasattr(result, "metrics") else {},
                "results": [
                    {
                        "prediction": prediction,
                        "reference": reference,
                        "metrics": result.metrics if hasattr(result, "metrics") else {},
                    }
                ],
                "summary": result.metrics if hasattr(result, "metrics") else {},
            }

    except Exception as e:
        raise HTTPException(500, f"Evaluation error: {str(e)}")


# ============================================================================
# Fine-tuning API
# ============================================================================


@app.post("/api/finetuning/create")
async def finetuning_create(request: FineTuningCreateRequest):
    """Create fine-tuning job"""
    try:
        # Use beanllm íŒ¨í‚¤ì§€ì˜ create_finetuning_provider í•¨ìˆ˜ (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
        from beanllm.facade.ml.finetuning_facade import create_finetuning_provider

        # Create provider using beanllm íŒ¨í‚¤ì§€ function (default to OpenAI)
        provider = create_finetuning_provider(provider="openai")
        finetuning = FineTuningManagerFacade(provider=provider)

        # Prepare and upload training data
        import tempfile
        import json
        from pathlib import Path

        temp_file = tempfile.NamedTemporaryFile(mode="w", suffix=".jsonl", delete=False)
        try:
            # Convert training_data to JSONL format
            for example in request.training_data:
                json.dump(example, temp_file)
                temp_file.write("\n")
            temp_file.close()

            # Start training
            job = finetuning.start_training(
                model=request.base_model,
                training_file=temp_file.name,
                **request.hyperparameters or {},
            )

            return {
                "job_id": job.job_id if hasattr(job, "job_id") else "job_001",
                "status": job.status if hasattr(job, "status") else "created",
                "base_model": request.base_model,
                "created_at": job.created_at if hasattr(job, "created_at") else None,
            }
        finally:
            # Cleanup temp file
            Path(temp_file.name).unlink(missing_ok=True)

    except Exception as e:
        raise HTTPException(500, f"Fine-tuning create error: {str(e)}")


@app.get("/api/finetuning/status/{job_id}")
async def finetuning_status(job_id: str):
    """Get fine-tuning job status"""
    try:
        # Use beanllm íŒ¨í‚¤ì§€ì˜ create_finetuning_provider í•¨ìˆ˜ (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
        from beanllm.facade.ml.finetuning_facade import create_finetuning_provider

        # Create provider using beanllm íŒ¨í‚¤ì§€ function (default to OpenAI)
        provider = create_finetuning_provider(provider="openai")
        finetuning = FineTuningManagerFacade(provider=provider)

        # Get training progress (includes job status)
        progress = finetuning.get_training_progress(job_id)

        job = progress.get("job")
        metrics = progress.get("metrics", [])

        return {
            "job_id": job_id,
            "status": job.status if hasattr(job, "status") else "unknown",
            "progress": len(metrics) / 100.0 if metrics else 0.0,  # Estimate progress
            "model_id": job.fine_tuned_model if hasattr(job, "fine_tuned_model") else None,
            "error": job.error if hasattr(job, "error") else None,
            "latest_metric": progress.get("latest_metric"),
        }

    except Exception as e:
        raise HTTPException(500, f"Fine-tuning status error: {str(e)}")


# ============================================================================
# Models API
# ============================================================================


@app.get("/api/models")
async def get_models():
    """Get all available models grouped by provider"""
    try:
        from beanllm.infrastructure.models.models import get_all_models
        from beanllm.providers.ollama_provider import OllamaProvider

        models = get_all_models()

        # Group by provider
        grouped = {}
        for model_name, model_info in models.items():
            provider = model_info["provider"]
            if provider not in grouped:
                grouped[provider] = []
            grouped[provider].append(
                {
                    "name": model_name,
                    "display_name": model_info["display_name"],
                    "description": model_info["description"],
                    "use_case": model_info["use_case"],
                    "max_tokens": model_info["max_tokens"],
                    "type": model_info["type"],
                    "provider": provider,  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ Ollamaì¸ì§€ í™•ì¸í•˜ê¸° ìœ„í•´
                    "installed": None,  # Ollama ìŠ¤ìº” í›„ ì—…ë°ì´íŠ¸ë¨
                }
            )

        # Ollamaì˜ ê²½ìš° ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ë„ ì¶”ê°€ (beanllm íŒ¨í‚¤ì§€ ê¸°ë°˜)
        try:
            ollama_provider = OllamaProvider()
            logger.info(f"[DEBUG] /api/models: Calling ollama_provider.list_models()...")
            installed_models = await ollama_provider.list_models()
            logger.info(
                f"[DEBUG] /api/models: list_models() returned: {installed_models} (count: {len(installed_models)})"
            )

            # ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ëª¨ë¸ë„ ì¶”ê°€ (list_models()ê°€ ì‹¤íŒ¨í•´ë„ UIì— í‘œì‹œí•˜ê¸° ìœ„í•´)
            for downloaded_model_name, ollama_model_name in _downloaded_models.items():
                if downloaded_model_name not in [m.get("name") for m in grouped.get("ollama", [])]:
                    # ì´ë¯¸ ëª©ë¡ì— ìˆìœ¼ë©´ ìŠ¤í‚µ
                    continue
                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ëª¨ë¸ì´ installed_modelsì— ì—†ìœ¼ë©´ ì¶”ê°€
                if (
                    downloaded_model_name not in installed_models
                    and ollama_model_name not in installed_models
                ):
                    logger.info(
                        f"[DEBUG] /api/models: Adding downloaded model to installed list: {downloaded_model_name} -> {ollama_model_name}"
                    )
                    installed_models.append(ollama_model_name)

            # ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ì„ setìœ¼ë¡œ ë³€í™˜ (ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´)
            installed_set = set(installed_models)

            # ê¸°ì¡´ Ollama ëª¨ë¸ë“¤ì˜ installed ìƒíƒœ ì—…ë°ì´íŠ¸
            for model in grouped.get("ollama", []):
                model_name = model.get("name")
                model_name_lower = model_name.lower()

                # ëª¨ë¸ ì´ë¦„ ë§¤í•‘ ì ìš© (Chat ì‹œì—ë§Œ ë§¤í•‘ ì‚¬ìš©)
                # Pull ì‹œì—ëŠ” ì›ë˜ ì´ë¦„ì„ ì‚¬ìš©í•˜ì§€ë§Œ, ì„¤ì¹˜ í™•ì¸ ì‹œì—ëŠ” ë§¤í•‘ëœ ì´ë¦„ ì‚¬ìš©
                mapped_model_name = get_ollama_model_name_for_chat(model_name)
                mapped_model_name_lower = mapped_model_name.lower()

                # 1. ì›ë³¸ ì´ë¦„ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                if model_name in installed_set:
                    model["installed"] = True
                # 2. ë§¤í•‘ëœ ì´ë¦„ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                elif mapped_model_name in installed_set:
                    model["installed"] = True
                # 3. ë¹„ìŠ·í•œ ì´ë¦„ìœ¼ë¡œ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                else:
                    found = False
                    for installed_model in installed_models:
                        installed_model_lower = installed_model.lower()
                        # ì›ë³¸ ì´ë¦„ê³¼ ë¹„ìŠ·í•œì§€ í™•ì¸
                        if (
                            model_name_lower in installed_model_lower
                            or installed_model_lower in model_name_lower
                        ):
                            model["installed"] = True
                            found = True
                            break
                        # ë§¤í•‘ëœ ì´ë¦„ê³¼ ë¹„ìŠ·í•œì§€ í™•ì¸
                        elif (
                            mapped_model_name_lower in installed_model_lower
                            or installed_model_lower in mapped_model_name_lower
                        ):
                            model["installed"] = True
                            found = True
                            break

                    if not found:
                        # list_models()ê°€ ì‹¤íŒ¨í–ˆì§€ë§Œ ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ëª¨ë¸ë¡œ ê¸°ë¡ë˜ì–´ ìˆìœ¼ë©´ ì„¤ì¹˜ëœ ê²ƒìœ¼ë¡œ í‘œì‹œ
                        if model_name in _downloaded_models:
                            model["installed"] = True
                            logger.info(
                                f"[DEBUG] Model '{model_name}' marked as installed from download cache (list_models() may have failed)"
                            )
                        else:
                            model["installed"] = False

            # ì„¤ì¹˜ëœ ëª¨ë¸ ì¤‘ ë¡œì»¬ ëª©ë¡ì— ì—†ëŠ” ê²ƒë“¤ ì¶”ê°€
            existing_names = {m["name"] for m in grouped.get("ollama", [])}
            for installed_model in installed_models:
                if installed_model not in existing_names:
                    # ê¸°ë³¸ ì •ë³´ë¡œ ì¶”ê°€ (ë¡œì»¬ ë©”íƒ€ë°ì´í„°ê°€ ì—†ì–´ë„ ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ì€ í‘œì‹œ)
                    if "ollama" not in grouped:
                        grouped["ollama"] = []
                    grouped["ollama"].append(
                        {
                            "name": installed_model,
                            "display_name": installed_model,
                            "description": f"Installed Ollama model: {installed_model}",
                            "use_case": "chat",
                            "max_tokens": 4096,  # ê¸°ë³¸ê°’
                            "type": "llm",
                            "installed": True,
                            "provider": "ollama",
                        }
                    )
        except Exception as e:
            logger.debug(f"Failed to scan Ollama models: {e}")
            # Ollama ìŠ¤ìº” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (ë¡œì»¬ ëª©ë¡ë§Œ ë°˜í™˜)
            # ì„¤ì¹˜ ì—¬ë¶€ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ì„ falseë¡œ ì„¤ì • (ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ)
            for model in grouped.get("ollama", []):
                if "installed" not in model:
                    model["installed"] = False  # ìŠ¤ìº” ì‹¤íŒ¨ ì‹œ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
                if "provider" not in model:
                    model["provider"] = "ollama"

        return grouped
    except Exception as e:
        raise HTTPException(500, f"Failed to get models: {str(e)}")


@app.get("/api/models/{provider}")
async def get_models_by_provider(provider: str):
    """Get models for a specific provider"""
    try:
        from beanllm.infrastructure.models.models import get_models_by_provider

        models = get_models_by_provider(provider)

        return {
            "provider": provider,
            "models": [
                {
                    "name": model_name,
                    "display_name": model_info["display_name"],
                    "description": model_info["description"],
                    "use_case": model_info["use_case"],
                    "max_tokens": model_info["max_tokens"],
                    "type": model_info["type"],
                }
                for model_name, model_info in models.items()
            ],
        }
    except Exception as e:
        raise HTTPException(500, f"Failed to get models for provider {provider}: {str(e)}")


@app.get("/api/models/{model_name}/parameters")
async def get_model_parameters(model_name: str):
    """Get parameter support information for a specific model"""
    try:
        from urllib.parse import unquote
        from beanllm import get_registry

        # URL ë””ì½”ë”© (ì½œë¡  ë“± íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
        model_name = unquote(model_name)

        registry = get_registry()
        model_info = registry.get_model_info(model_name)

        # Registryì— ì—†ìœ¼ë©´ provider ì¶”ë¡  ë° ê¸°ë³¸ê°’ ë°˜í™˜
        if not model_info:
            # Provider ì¶”ë¡  (ëª¨ë¸ ì´ë¦„ íŒ¨í„´ ê¸°ë°˜)
            provider = "unknown"
            if ":" in model_name or model_name.startswith(
                ("qwen", "phi", "llama", "mistral", "gemma")
            ):
                provider = "ollama"
            elif (
                model_name.startswith("gpt")
                or model_name.startswith("o1")
                or model_name.startswith("o3")
            ):
                provider = "openai"
            elif model_name.startswith("claude"):
                provider = "anthropic"
            elif model_name.startswith("gemini"):
                provider = "google"
            elif model_name.startswith("deepseek"):
                provider = "deepseek"
            elif model_name.startswith("sonar"):
                provider = "perplexity"

            # Providerë³„ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì§€ì› ì •ë³´
            provider_defaults = {
                "ollama": {
                    "supports": {
                        "temperature": True,
                        "max_tokens": True,
                        "top_p": True,
                        "frequency_penalty": False,
                        "presence_penalty": False,
                    },
                    "max_tokens": 4096,
                    "default_temperature": 0.7,
                },
                "openai": {
                    "supports": {
                        "temperature": True,
                        "max_tokens": True,
                        "top_p": True,
                        "frequency_penalty": True,
                        "presence_penalty": True,
                    },
                    "max_tokens": 4096,
                    "default_temperature": 0.7,
                },
                "anthropic": {
                    "supports": {
                        "temperature": True,
                        "max_tokens": True,
                        "top_p": True,
                        "frequency_penalty": False,
                        "presence_penalty": False,
                    },
                    "max_tokens": 4096,
                    "default_temperature": 0.7,
                },
                "google": {
                    "supports": {
                        "temperature": True,
                        "max_tokens": True,
                        "top_p": True,
                        "frequency_penalty": False,
                        "presence_penalty": False,
                    },
                    "max_tokens": 8192,
                    "default_temperature": 0.7,
                },
                "deepseek": {
                    "supports": {
                        "temperature": True,
                        "max_tokens": True,
                        "top_p": True,
                        "frequency_penalty": True,
                        "presence_penalty": True,
                    },
                    "max_tokens": 4096,
                    "default_temperature": 0.7,
                },
                "perplexity": {
                    "supports": {
                        "temperature": True,
                        "max_tokens": True,
                        "top_p": True,
                        "frequency_penalty": True,
                        "presence_penalty": True,
                    },
                    "max_tokens": 4096,
                    "default_temperature": 0.7,
                },
            }

            defaults = provider_defaults.get(provider, provider_defaults["ollama"])

            return {
                "model": model_name,
                "provider": provider,
                "supports": defaults["supports"],
                "max_tokens": defaults["max_tokens"],
                "default_temperature": defaults["default_temperature"],
                "uses_max_completion_tokens": False,
            }

        # Registryì—ì„œ ì°¾ì€ ê²½ìš°
        provider = model_info.provider.lower()

        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì§€ì› ì •ë³´
        supports = {
            "temperature": model_info.supports_temperature,
            "max_tokens": model_info.supports_max_tokens,
            "top_p": True,  # ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ ì§€ì›
            "frequency_penalty": provider in ["openai", "deepseek", "perplexity"],
            "presence_penalty": provider in ["openai", "deepseek", "perplexity"],
        }

        return {
            "model": model_name,
            "provider": model_info.provider,
            "supports": supports,
            "max_tokens": model_info.max_tokens,
            "default_temperature": model_info.default_temperature,
            "uses_max_completion_tokens": model_info.uses_max_completion_tokens,
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get model parameters for {model_name}: {e}", exc_info=True)
        raise HTTPException(500, f"Failed to get model parameters: {str(e)}")


@app.post("/api/models/scan")
async def scan_models():
    """
    Scan APIs for new models (CLI ê¸°ëŠ¥ì„ APIë¡œ)
    """
    try:
        from beanllm.infrastructure.hybrid import create_hybrid_manager

        manager = create_hybrid_manager()
        results = await manager.scan_all_providers()

        return {"status": "success", "results": results, "message": "Model scan completed"}
    except Exception as e:
        raise HTTPException(500, f"Failed to scan models: {str(e)}")


@app.post("/api/models/{model_name}/pull")
async def pull_model(model_name: str):
    """
    Ollama ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì‚¬ìš©ìê°€ ì„ íƒì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ)
    """
    try:
        from urllib.parse import unquote
        from beanllm.providers.ollama_provider import OllamaProvider

        # URL ë””ì½”ë”©
        model_name = unquote(model_name)

        # Pull ì‹œì—ëŠ” ì›ë˜ ì´ë¦„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        # (ì˜ˆ: phi3.5ëŠ” pull ì‹œ "phi3.5"ë¥¼ ì‚¬ìš©, ì„¤ì¹˜ í›„ "phi3"ë¡œ ì €ì¥ë¨)
        # Chat ì‹œì—ë§Œ ë§¤í•‘ëœ ì´ë¦„ì„ ì‚¬ìš©
        ollama_model_name = model_name

        # Ollama provider ìƒì„±
        ollama_provider = OllamaProvider()

        logger.info(f"Pulling Ollama model: {ollama_model_name} (requested: {model_name})")

        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (streamingìœ¼ë¡œ ì§„í–‰ ìƒí™© ë°˜í™˜)
        async def generate():
            try:
                import json

                # Ollama SDKì˜ pullì€ client.chat()ê³¼ ìœ ì‚¬í•˜ê²Œ await í›„ async generator ë°˜í™˜
                # stream=Trueì¼ ë•Œ awaitë¥¼ ë¨¼ì € í•´ì•¼ í•¨
                logger.info(f"Starting pull for {ollama_model_name}")
                pull_stream = await ollama_provider.client.pull(
                    model=ollama_model_name, stream=True
                )
                logger.info(f"Pull stream obtained: {type(pull_stream)}")

                # await í›„ async generatorë¥¼ ë°˜í™˜
                chunk_count = 0
                async for chunk in pull_stream:
                    chunk_count += 1
                    logger.debug(f"Received chunk {chunk_count}: {type(chunk)} = {chunk}")

                    # Ollama SDKëŠ” ProgressResponse ê°ì²´ë¥¼ ë°˜í™˜í•¨
                    # dictê°€ ì•„ë‹Œ ê²½ìš°ë„ ì²˜ë¦¬ (ProgressResponse ê°ì²´)
                    status = None
                    completed = None
                    total = None

                    if isinstance(chunk, dict):
                        status = chunk.get("status", "")
                        completed = chunk.get("completed", 0)
                        total = chunk.get("total", 0)
                    else:
                        # ProgressResponse ê°ì²´ì¸ ê²½ìš°
                        # hasattrë¡œ ì†ì„± í™•ì¸
                        if hasattr(chunk, "status"):
                            status = chunk.status
                        if hasattr(chunk, "completed"):
                            completed = chunk.completed
                        if hasattr(chunk, "total"):
                            total = chunk.total

                    # statusê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
                    if status is None:
                        logger.debug(f"Chunk without status: {type(chunk)}")
                        continue

                    logger.debug(
                        f"Chunk data: status={status}, completed={completed}, total={total}"
                    )

                    # ì§„í–‰ë¥  ê³„ì‚° (completedì™€ totalì´ ëª¨ë‘ ìˆì„ ë•Œë§Œ)
                    progress = 0
                    if completed is not None and total is not None and total > 0:
                        progress = completed / total * 100
                    elif status == "success":
                        progress = 100
                    elif status in [
                        "pulling manifest",
                        "verifying sha256 digest",
                        "writing manifest",
                    ]:
                        # ì§„í–‰ ì¤‘ì´ì§€ë§Œ ì •í™•í•œ ì§„í–‰ë¥ ì„ ëª¨ë¥¼ ë•ŒëŠ” ì‘ì€ ê°’ìœ¼ë¡œ í‘œì‹œ
                        progress = 1
                    elif status.startswith("pulling "):
                        # ê°œë³„ ë ˆì´ì–´ ë‹¤ìš´ë¡œë“œ ì¤‘
                        if completed is not None and total is not None and total > 0:
                            progress = completed / total * 100
                        else:
                            progress = 1

                    progress_data = {
                        "status": status,
                        "completed": completed if completed is not None else 0,
                        "total": total if total is not None else 0,
                        "progress": round(progress, 2),
                    }
                    logger.debug(f"Yielding progress: {progress_data}")
                    yield f"data: {json.dumps(progress_data)}\n\n"

                    # ì™„ë£Œ ì‹œ (statusê°€ 'success')
                    if status == "success":
                        logger.info(f"Pull completed: {model_name} -> {ollama_model_name}")
                        # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í›„ ì‹¤ì œ ì„¤ì¹˜ëœ ëª¨ë¸ ì´ë¦„ í™•ì¸
                        # ì•½ê°„ì˜ ì§€ì—°ì„ ë‘ì–´ Ollamaê°€ ëª¨ë¸ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•  ì‹œê°„ì„ ì¤Œ
                        import asyncio

                        await asyncio.sleep(2)  # 2ì´ˆ ëŒ€ê¸° (Ollamaê°€ ëª¨ë¸ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•  ì‹œê°„)

                        try:
                            # ì—¬ëŸ¬ ë²ˆ ì‹œë„ (Ollamaê°€ ëª¨ë¸ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ë° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
                            actual_model_name = None
                            for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì‹œë„
                                if attempt > 0:
                                    await asyncio.sleep(1)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

                                installed_models = await ollama_provider.list_models()
                                installed_models_lower = [m.lower() for m in installed_models if m]

                                logger.info(
                                    f"[DEBUG] After pull (attempt {attempt + 1}), installed models: {installed_models} (count: {len(installed_models)})"
                                )

                                if not installed_models:
                                    logger.warning(
                                        f"[DEBUG] Attempt {attempt + 1}: list_models() returned empty list"
                                    )
                                    continue

                                # 1. ì›ë˜ ë‹¤ìš´ë¡œë“œí•œ ì´ë¦„ìœ¼ë¡œ í™•ì¸ (ì˜ˆ: phi3.5)
                                if ollama_model_name.lower() in installed_models_lower:
                                    for installed_model in installed_models:
                                        if installed_model.lower() == ollama_model_name.lower():
                                            actual_model_name = installed_model
                                            logger.info(
                                                f"Found model with original name: {actual_model_name}"
                                            )
                                            break
                                    if actual_model_name:
                                        break

                                # 2. ë§¤í•‘ëœ ì´ë¦„ìœ¼ë¡œ í™•ì¸ (ì˜ˆ: phi3.5 -> phi3)
                                mapped_name = get_ollama_model_name_for_chat(model_name)
                                if mapped_name.lower() in installed_models_lower:
                                    for installed_model in installed_models:
                                        if installed_model.lower() == mapped_name.lower():
                                            actual_model_name = installed_model
                                            logger.info(
                                                f"Found model with mapped name: {actual_model_name} (mapped from {model_name} -> {mapped_name})"
                                            )
                                            break
                                    if actual_model_name:
                                        break

                                # 3. ë¹„ìŠ·í•œ ì´ë¦„ ì°¾ê¸°
                                for installed_model in installed_models:
                                    # ì›ë˜ ì´ë¦„ê³¼ ë¹„ìŠ·í•œì§€ í™•ì¸
                                    if (
                                        ollama_model_name.lower() in installed_model.lower()
                                        or installed_model.lower() in ollama_model_name.lower()
                                    ):
                                        actual_model_name = installed_model
                                        logger.info(
                                            f"Found similar model name: {actual_model_name} (original: {ollama_model_name})"
                                        )
                                        break
                                    # ë§¤í•‘ëœ ì´ë¦„ê³¼ ë¹„ìŠ·í•œì§€ í™•ì¸
                                    if (
                                        mapped_name.lower() in installed_model.lower()
                                        or installed_model.lower() in mapped_name.lower()
                                    ):
                                        actual_model_name = installed_model
                                        logger.info(
                                            f"Found similar model name: {actual_model_name} (mapped: {mapped_name})"
                                        )
                                        break

                                if actual_model_name:
                                    break

                            # ìµœì¢… ê²°ê³¼
                            if actual_model_name:
                                logger.info(
                                    f"Successfully verified installed model: {actual_model_name} (requested: {model_name}, pulled: {ollama_model_name})"
                                )
                                # ë‹¤ìš´ë¡œë“œ ì™„ë£Œëœ ëª¨ë¸ë¡œ ê¸°ë¡ (UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´)
                                _downloaded_models[model_name] = actual_model_name
                                yield f"data: {json.dumps({'status': 'completed', 'model': model_name, 'ollama_model': actual_model_name, 'original_request': model_name})}\n\n"
                            else:
                                logger.warning(
                                    f"[DEBUG] Could not verify installed model after 3 attempts. Requested: {model_name}, Pulled: {ollama_model_name}"
                                )
                                # ë§¤í•‘ëœ ì´ë¦„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                                mapped_name = get_ollama_model_name_for_chat(model_name)
                                # list_models()ê°€ ì‹¤íŒ¨í•´ë„ ë‹¤ìš´ë¡œë“œëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê¸°ë¡ (UXë¥¼ ìœ„í•´)
                                _downloaded_models[model_name] = mapped_name
                                logger.info(
                                    f"[DEBUG] Marking model as downloaded despite verification failure: {model_name} -> {mapped_name}"
                                )
                                yield f"data: {json.dumps({'status': 'completed', 'model': model_name, 'ollama_model': mapped_name, 'original_request': model_name, 'message': 'Verification failed, but download completed'})}\n\n"

                        except Exception as e:
                            logger.error(f"Failed to verify installed model: {e}", exc_info=True)
                            # ë§¤í•‘ëœ ì´ë¦„ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
                            mapped_name = get_ollama_model_name_for_chat(model_name)
                            # ì˜ˆì™¸ê°€ ë°œìƒí•´ë„ ë‹¤ìš´ë¡œë“œëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ê¸°ë¡ (UXë¥¼ ìœ„í•´)
                            _downloaded_models[model_name] = mapped_name
                            logger.info(
                                f"[DEBUG] Marking model as downloaded despite exception: {model_name} -> {mapped_name}"
                            )
                            yield f"data: {json.dumps({'status': 'completed', 'model': model_name, 'ollama_model': mapped_name, 'original_request': model_name})}\n\n"
                        break

                if chunk_count == 0:
                    logger.warning(f"No chunks received for {ollama_model_name}")
                    # ëª¨ë¸ì´ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ chunkê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                    yield f"data: {json.dumps({'status': 'completed', 'model': model_name, 'ollama_model': ollama_model_name, 'message': 'Model may already be installed'})}\n\n"
            except Exception as e:
                logger.error(
                    f"Failed to pull model {ollama_model_name} (requested: {model_name}): {e}",
                    exc_info=True,
                )
                yield f"data: {json.dumps({'status': 'error', 'error': str(e)})}\n\n"

        from fastapi.responses import StreamingResponse

        return StreamingResponse(
            generate(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            },
        )

    except Exception as e:
        logger.error(f"Pull model error: {e}")
        raise HTTPException(500, f"Failed to pull model: {str(e)}")


@app.post("/api/models/{model_name}/analyze")
async def analyze_model(model_name: str):
    """
    Analyze model with pattern inference (CLI ê¸°ëŠ¥ì„ APIë¡œ)
    """
    try:
        from beanllm import get_registry
        from beanllm.infrastructure.registry.model_registry import ModelRegistry

        registry = get_registry()
        model_info = registry.get_model_info(model_name)

        if not model_info:
            raise HTTPException(404, f"Model {model_name} not found")

        # ëª¨ë¸ ë¶„ì„ ì •ë³´ ìˆ˜ì§‘
        analysis = {
            "model": model_name,
            "provider": model_info.provider,
            "type": model_info.model_type,
            "capabilities": {
                "streaming": model_info.supports_streaming,
                "temperature": model_info.supports_temperature,
                "max_tokens": model_info.supports_max_tokens,
                "uses_max_completion_tokens": model_info.uses_max_completion_tokens,
            },
            "parameters": [
                {
                    "name": p.name,
                    "type": p.type,
                    "supported": p.supported,
                    "description": p.description,
                    "default": p.default,
                }
                for p in model_info.parameters
            ],
            "max_tokens": model_info.max_tokens,
            "default_temperature": model_info.default_temperature,
        }

        return {
            "status": "success",
            "analysis": analysis,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, f"Failed to analyze model: {str(e)}")


# ============================================================================
# WebSocket for Real-time Streaming
# ============================================================================


@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """
    WebSocket endpoint for real-time progress updates
    """
    await websocket.accept()
    active_connections[session_id] = websocket

    print(f"WebSocket connected: {session_id}")

    try:
        # Send welcome message
        await websocket.send_json(
            {
                "type": "connected",
                "session_id": session_id,
                "message": "Connected to beanllm playground",
            }
        )

        # Keep connection alive and handle messages
        while True:
            try:
                data = await websocket.receive_json()

                # Handle ping-pong
                if data.get("type") == "ping":
                    await websocket.send_json({"type": "pong"})

                # Handle other messages
                else:
                    print(f"Received from {session_id}: {data}")

            except WebSocketDisconnect:
                break
            except Exception as e:
                print(f"WebSocket error: {e}")
                break

    finally:
        # Clean up
        if session_id in active_connections:
            del active_connections[session_id]
        print(f"WebSocket disconnected: {session_id}")


# ============================================================================
# OCR API
# ============================================================================


@app.post("/api/ocr/recognize")
async def ocr_recognize(
    file: UploadFile = File(...),
    engine: str = "paddleocr",
    language: str = "auto",
    use_gpu: bool = True,
    confidence_threshold: float = 0.5,
    enable_preprocessing: bool = True,
    denoise: bool = True,
    contrast_adjustment: bool = True,
    binarize: bool = True,
    deskew: bool = True,
    sharpen: bool = False,
    enable_llm_postprocessing: bool = False,
    llm_model: Optional[str] = None,
    spell_check: bool = False,
    grammar_check: bool = False,
    max_image_size: Optional[int] = None,
    output_format: str = "text",
):
    """
    OCR ì´ë¯¸ì§€ ì¸ì‹

    Args:
        file: ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼
        engine: OCR ì—”ì§„ (paddleocr, easyocr, trocr, nougat, surya, tesseract, qwen2vl-2b, minicpm, deepseek-ocr)
        language: ì–¸ì–´ (auto, ko, en, zh, ja ë“±)
        use_gpu: GPU ì‚¬ìš© ì—¬ë¶€
        confidence_threshold: ìµœì†Œ ì‹ ë¢°ë„ ì„ê³„ê°’
        enable_preprocessing: ì „ì²˜ë¦¬ í™œì„±í™”
        denoise: ë…¸ì´ì¦ˆ ì œê±°
        contrast_adjustment: ëŒ€ë¹„ ì¡°ì •
        binarize: ì´ì§„í™”
        deskew: ê¸°ìš¸ê¸° ë³´ì •
        sharpen: ì„ ëª…í™”
        enable_llm_postprocessing: LLM í›„ì²˜ë¦¬ í™œì„±í™”
        llm_model: LLM ëª¨ë¸ (LLM í›„ì²˜ë¦¬ìš©)
        spell_check: ë§ì¶¤ë²• ê²€ì‚¬
        grammar_check: ë¬¸ë²• ê²€ì‚¬
        max_image_size: ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° (í”½ì…€)
        output_format: ì¶œë ¥ í˜•ì‹ (text, json, markdown)

    Returns:
        OCR ê²°ê³¼ (í…ìŠ¤íŠ¸, ì‹ ë¢°ë„, ì²˜ë¦¬ ì‹œê°„ ë“±)
    """
    try:
        # íŒŒì¼ ì €ì¥
        import tempfile
        import shutil

        with tempfile.NamedTemporaryFile(
            delete=False, suffix=f".{file.filename.split('.')[-1] if file.filename else 'jpg'}"
        ) as tmp_file:
            shutil.copyfileobj(file.file, tmp_file)
            tmp_path = tmp_file.name

        try:
            # OCR ì„¤ì • ìƒì„±
            ocr_config = OCRConfig(
                engine=engine,
                language=language,
                use_gpu=use_gpu,
                confidence_threshold=confidence_threshold,
                enable_preprocessing=enable_preprocessing,
                denoise=denoise,
                contrast_adjustment=contrast_adjustment,
                binarize=binarize,
                deskew=deskew,
                sharpen=sharpen,
                enable_llm_postprocessing=enable_llm_postprocessing,
                llm_model=llm_model,
                spell_check=spell_check,
                grammar_check=grammar_check,
                max_image_size=max_image_size,
                output_format=output_format,
            )

            # OCR ì‹¤í–‰
            ocr = beanOCR(config=ocr_config)
            result = ocr.recognize(tmp_path)

            # ê²°ê³¼ ë°˜í™˜
            return {
                "text": result.text,
                "confidence": result.confidence,
                "processing_time": result.processing_time,
                "engine": result.engine,
                "language": result.language,
                "num_lines": len(result.lines) if result.lines else 0,
                "lines": [
                    {
                        "text": line.text,
                        "confidence": line.confidence,
                        "bbox": (
                            {
                                "x": line.bbox.x,
                                "y": line.bbox.y,
                                "width": line.bbox.width,
                                "height": line.bbox.height,
                            }
                            if line.bbox
                            else None
                        ),
                    }
                    for line in (result.lines or [])
                ],
            }
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"OCR processing failed: {str(e)}")


# ============================================================================
# Google Workspace Integration (User Features)
# ============================================================================

# Google API í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸ (ì„ íƒì )
try:
    from google.oauth2.credentials import Credentials
    from google.auth.transport.requests import Request
    from googleapiclient.discovery import build
    from google_auth_oauthlib.flow import Flow

    GOOGLE_API_AVAILABLE = True
except ImportError:
    GOOGLE_API_AVAILABLE = False
    logger.warning("google-api-python-client not installed. Google Workspace features disabled.")

# Google ì´ë²¤íŠ¸ ë¡œê¹… ì„í¬íŠ¸
try:
    from beanllm.infrastructure.distributed.google_events import log_google_export

    GOOGLE_EVENTS_AVAILABLE = True
except ImportError:
    GOOGLE_EVENTS_AVAILABLE = False
    logger.warning("Google event logging not available")


class GoogleExportRequest(BaseModel):
    """Google ì„œë¹„ìŠ¤ ë‚´ë³´ë‚´ê¸° ìš”ì²­"""

    session_id: str
    user_id: Optional[str] = "anonymous"
    title: Optional[str] = None
    access_token: str  # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ OAuth í›„ ë°›ì€ í† í°


@app.post("/api/chat/export/docs")
async def export_chat_to_docs(request: GoogleExportRequest):
    """
    Ollama ì±„íŒ… ë‚´ì—­ì„ Google Docsë¡œ ë‚´ë³´ë‚´ê¸°

    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ Google OAuth 2.0ìœ¼ë¡œ access_tokenì„ ë°›ì•„ì„œ ì „ë‹¬
    """
    if not GOOGLE_API_AVAILABLE:
        raise HTTPException(
            501,
            "Google API client not installed. Run: pip install google-api-python-client google-auth-oauthlib",
        )

    try:
        # 1. ì„¸ì…˜ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸° (ì„ì‹œ êµ¬í˜„ - ë‚˜ì¤‘ì— HybridSessionManagerë¡œ ëŒ€ì²´)
        # TODO: Redis/MongoDB ì„¸ì…˜ ê´€ë¦¬ì í†µí•©
        messages = []  # ì„ì‹œ: ì‹¤ì œë¡œëŠ” session_idë¡œ ë©”ì‹œì§€ ì¡°íšŒ

        if not messages:
            raise HTTPException(404, f"Session {request.session_id} not found")

        # 2. Google Docs API í˜¸ì¶œ
        credentials = Credentials(token=request.access_token)
        docs_service = build("docs", "v1", credentials=credentials)
        drive_service = build("drive", "v3", credentials=credentials)

        # ë¬¸ì„œ ìƒì„±
        title = request.title or f"beanllm Chat - {request.session_id[:8]}"
        doc = docs_service.documents().create(body={"title": title}).execute()
        doc_id = doc.get("documentId")

        # ë©”ì‹œì§€ ë‚´ìš© ì‘ì„±
        content = f"# {title}\n\n"
        for msg in messages:
            role = msg.get("role", "unknown")
            content += f"**{role.capitalize()}**: {msg.get('content', '')}\n\n"

        # ë¬¸ì„œì— í…ìŠ¤íŠ¸ ì‚½ì…
        requests_body = [{"insertText": {"location": {"index": 1}, "text": content}}]
        docs_service.documents().batchUpdate(
            documentId=doc_id, body={"requests": requests_body}
        ).execute()

        # 3. ì´ë²¤íŠ¸ ë¡œê¹… (ê´€ë¦¬ì ëª¨ë‹ˆí„°ë§ìš©)
        if GOOGLE_EVENTS_AVAILABLE:
            try:
                await log_google_export(
                    user_id=request.user_id,
                    export_type="docs",
                    metadata={
                        "doc_id": doc_id,
                        "session_id": request.session_id,
                        "message_count": len(messages),
                        "title": title,
                    },
                )
            except Exception as log_error:
                logger.warning(f"Failed to log Google export event: {log_error}")

        # 4. ë¬¸ì„œ ë§í¬ ë°˜í™˜
        doc_url = f"https://docs.google.com/document/d/{doc_id}/edit"

        return {
            "success": True,
            "doc_id": doc_id,
            "doc_url": doc_url,
            "title": title,
            "message_count": len(messages),
        }

    except Exception as e:
        logger.error(f"Failed to export to Google Docs: {e}", exc_info=True)
        raise HTTPException(500, f"Google Docs export failed: {str(e)}")


@app.post("/api/chat/save/drive")
async def save_chat_to_drive(request: GoogleExportRequest):
    """
    Ollama ì±„íŒ… ë‚´ì—­ì„ Google Driveì— í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    """
    if not GOOGLE_API_AVAILABLE:
        raise HTTPException(
            501,
            "Google API client not installed. Run: pip install google-api-python-client google-auth-oauthlib",
        )

    try:
        # 1. ì„¸ì…˜ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        messages = []  # TODO: Redis/MongoDB ì„¸ì…˜ ê´€ë¦¬ì í†µí•©

        if not messages:
            raise HTTPException(404, f"Session {request.session_id} not found")

        # 2. Google Drive API í˜¸ì¶œ
        credentials = Credentials(token=request.access_token)
        drive_service = build("drive", "v3", credentials=credentials)

        # íŒŒì¼ ë‚´ìš© ìƒì„±
        title = request.title or f"beanllm_chat_{request.session_id[:8]}.txt"
        content = f"beanllm Chat History\n"
        content += f"Session ID: {request.session_id}\n"
        content += f"Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += "=" * 60 + "\n\n"

        for msg in messages:
            role = msg.get("role", "unknown")
            content += f"{role.upper()}:\n{msg.get('content', '')}\n\n"

        # íŒŒì¼ ì—…ë¡œë“œ
        from googleapiclient.http import MediaInMemoryUpload

        file_metadata = {"name": title, "mimeType": "text/plain"}
        media = MediaInMemoryUpload(content.encode("utf-8"), mimetype="text/plain", resumable=True)

        file = (
            drive_service.files()
            .create(body=file_metadata, media_body=media, fields="id, webViewLink")
            .execute()
        )

        # 3. ì´ë²¤íŠ¸ ë¡œê¹…
        if GOOGLE_EVENTS_AVAILABLE:
            try:
                await log_google_export(
                    user_id=request.user_id,
                    export_type="drive",
                    metadata={
                        "file_id": file.get("id"),
                        "session_id": request.session_id,
                        "message_count": len(messages),
                        "file_name": title,
                    },
                )
            except Exception as log_error:
                logger.warning(f"Failed to log Google export event: {log_error}")

        return {
            "success": True,
            "file_id": file.get("id"),
            "file_url": file.get("webViewLink"),
            "file_name": title,
            "message_count": len(messages),
        }

    except Exception as e:
        logger.error(f"Failed to save to Google Drive: {e}", exc_info=True)
        raise HTTPException(500, f"Google Drive save failed: {str(e)}")


class GoogleShareRequest(BaseModel):
    """Gmail ê³µìœ  ìš”ì²­"""

    session_id: str
    user_id: Optional[str] = "anonymous"
    to_email: str
    subject: Optional[str] = None
    message: Optional[str] = None
    access_token: str


@app.post("/api/chat/share/email")
async def share_chat_via_email(request: GoogleShareRequest):
    """
    Ollama ì±„íŒ… ë‚´ì—­ì„ Gmailë¡œ ê³µìœ 
    """
    if not GOOGLE_API_AVAILABLE:
        raise HTTPException(
            501,
            "Google API client not installed. Run: pip install google-api-python-client google-auth-oauthlib",
        )

    try:
        # 1. ì„¸ì…˜ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        messages = []  # TODO: Redis/MongoDB ì„¸ì…˜ ê´€ë¦¬ì í†µí•©

        if not messages:
            raise HTTPException(404, f"Session {request.session_id} not found")

        # 2. Gmail API í˜¸ì¶œ
        credentials = Credentials(token=request.access_token)
        gmail_service = build("gmail", "v1", credentials=credentials)

        # ì´ë©”ì¼ ë‚´ìš© ìƒì„±
        subject = request.subject or f"beanllm Chat - {request.session_id[:8]}"
        body = request.message or "Here is my beanllm chat history:\n\n"
        body += "=" * 60 + "\n\n"

        for msg in messages:
            role = msg.get("role", "unknown")
            body += f"{role.upper()}:\n{msg.get('content', '')}\n\n"

        # MIME ì´ë©”ì¼ ìƒì„±
        import base64
        from email.mime.text import MIMEText

        message = MIMEText(body)
        message["to"] = request.to_email
        message["subject"] = subject

        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode("utf-8")

        # ì´ë©”ì¼ ì „ì†¡
        sent_message = (
            gmail_service.users().messages().send(userId="me", body={"raw": raw_message}).execute()
        )

        # 3. ì´ë²¤íŠ¸ ë¡œê¹…
        if GOOGLE_EVENTS_AVAILABLE:
            try:
                await log_google_export(
                    user_id=request.user_id,
                    export_type="gmail",
                    metadata={
                        "message_id": sent_message.get("id"),
                        "session_id": request.session_id,
                        "to_email": request.to_email,
                        "message_count": len(messages),
                    },
                )
            except Exception as log_error:
                logger.warning(f"Failed to log Google export event: {log_error}")

        return {
            "success": True,
            "message_id": sent_message.get("id"),
            "to_email": request.to_email,
            "subject": subject,
            "message_count": len(messages),
        }

    except Exception as e:
        logger.error(f"Failed to share via Gmail: {e}", exc_info=True)
        raise HTTPException(500, f"Gmail share failed: {str(e)}")


# ============================================================================
# Include Routers (Modular API)
# ============================================================================

# Import routers
from routers.config_router import router as config_router

# Include routers
app.include_router(config_router)

logger.info("âœ… Modular routers included: config")


# ============================================================================
# Run Server
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("beanllm Playground API Server")
    print("=" * 60)
    print("Starting on http://localhost:8000")
    print("Docs: http://localhost:8000/docs")
    print("=" * 60)
    print("\nAvailable Features:")
    print("  - Chat (General conversation)")
    print("  - Knowledge Graph (Build & Query)")
    print("  - RAG (Retrieval-Augmented Generation)")
    print("  - Agent (Autonomous task execution)")
    print("  - OCR (Optical Character Recognition)")
    print("  - Web Search (Multi-engine search)")
    print("  - RAG Debug (Pipeline analysis)")
    print("  - Optimizer (Performance optimization)")
    print("  - Multi-Agent (Collaborative agents)")
    print("  - Orchestrator (Workflow management)")
    print("=" * 60)

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info",
    )
