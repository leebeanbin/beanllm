# ğŸ«˜ beanllm Playground

Complete web interface for beanllm framework with **all AI features**.

## âœ¨ Features

### Core Features

1. **ğŸ’¬ Chat** - General LLM conversation with Think Mode support
2. **ğŸ” RAG** - Retrieval-Augmented Generation
3. **ğŸ¤– Agent** - Autonomous task execution with tools
4. **ğŸ‘¥ Multi-Agent** - Collaborative multi-agent systems
5. **ğŸ”€ Chain** - LLM chains and workflows
6. **ğŸ•¸ï¸ Knowledge Graph** - Entity/relation extraction and graph reasoning
7. **ğŸ–¼ï¸ Vision RAG** - Image-based RAG
8. **ğŸµ Audio** - Audio transcription and synthesis
9. **ğŸ“Š Evaluation** - Model evaluation tools
10. **ğŸ”§ Fine-tuning** - Model fine-tuning
11. **ğŸ“„ OCR** - Optical Character Recognition
12. **ğŸŒ Web Search** - Multi-engine web search

### UI Features

- ğŸ¨ KRDS design system with pastel colors
- ğŸŒ“ Dark mode support
- ğŸ“± Fully responsive
- âš¡ Real-time streaming
- ğŸ§  Think Mode visualization
- ğŸ“¥ Model download with progress tracking
- ğŸ¯ Interactive onboarding guide

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.11+
python --version

# Node.js 18+ & pnpm
node --version
pnpm --version
```

### 1. Backend Setup (FastAPI)

```bash
# Navigate to backend directory
cd playground/backend

# Install dependencies
pip install -r requirements.txt

# Set up environment variables (optional for open-source models)
export OPENAI_API_KEY=sk-...
export ANTHROPIC_API_KEY=sk-ant-...
export DEEPSEEK_API_KEY=...
export GEMINI_API_KEY=...
export PERPLEXITY_API_KEY=...

# Start the server
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**Backend will start on**: http://localhost:8000

**API Documentation**: http://localhost:8000/docs

### 2. Frontend Setup (Next.js)

```bash
# Navigate to frontend directory
cd playground/frontend

# Install dependencies
pnpm install

# Start development server
pnpm dev
```

**Frontend will start on**: http://localhost:3000

### 3. Start Using

1. Open http://localhost:3000
2. Select a model from the dropdown
3. Start chatting or using any feature!

---

## ğŸ“ Project Structure

```
playground/
â”œâ”€â”€ backend/                 # FastAPI backend
â”‚   â”œâ”€â”€ main.py             # All-in-one server
â”‚   â””â”€â”€ requirements.txt    # Python dependencies
â”‚
â”œâ”€â”€ frontend/               # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # Next.js app directory (pages)
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ lib/           # API client & utilities
â”‚   â”‚   â””â”€â”€ providers/     # Context providers
â”‚   â”œâ”€â”€ package.json       # Node dependencies
â”‚   â””â”€â”€ tailwind.config.js # Tailwind configuration
â”‚
â””â”€â”€ README.md              # This file
```

---

## ğŸ”Œ API Endpoints

### Health Check âœ…
- `GET /` - Service status
- `GET /health` - Detailed health check

### Chat âœ…
- `POST /api/chat` - General conversation
  - Supports text and multimodal (image + text)
  - Think Mode support (`enable_thinking: true`)

### Models âœ…
- `GET /api/models` - List all available models grouped by provider
- `GET /api/models/{model_name}/parameters` - Get model parameters
- `POST /api/models/{model_name}/pull` - Download Ollama model (SSE streaming)

### RAG âœ…
- `POST /api/rag/build` - Build RAG index from documents
- `POST /api/rag/query` - Query RAG system
- `GET /api/rag/collections` - List RAG collections
- `DELETE /api/rag/collections/{name}` - Delete collection
- `POST /api/rag/build_from_files` - Build from uploaded files

### Knowledge Graph âœ…
- `POST /api/kg/build` - Build knowledge graph from documents
- `POST /api/kg/query` - Query graph (Cypher)
- `POST /api/kg/graph_rag` - Graph RAG query
- `GET /api/kg/visualize/{graph_id}` - Visualize graph

### Agent âœ…
- `POST /api/agent/run` - Run autonomous agent task

### Multi-Agent âœ…
- `POST /api/multi_agent/run` - Run multi-agent system
  - `mode`: "sequential", "parallel", "hierarchical", "debate"

### Chain âœ…
- `POST /api/chain/build` - Build chain
- `POST /api/chain/run` - Run chain

### Web Search âœ…
- `POST /api/web/search` - Search the web
  - `summarize`: Use LLM to summarize results
  - `model`: Model for summarization

### Evaluation âœ…
- `POST /api/evaluation/evaluate` - Evaluate model responses

### Vision RAG ğŸ”§
- `POST /api/vision_rag/build` - Build VisionRAG index from images
- `POST /api/vision_rag/query` - Query VisionRAG
- Note: Requires additional dependencies

### Audio ğŸ”§
- `POST /api/audio/transcribe` - Transcribe audio (requires audio file)
- `POST /api/audio/synthesize` - Synthesize speech (requires OpenAI API key)

### OCR ğŸ”§
- `POST /api/ocr/recognize` - Recognize text from images (requires PaddleOCR)

### Fine-tuning ğŸ”§
- `POST /api/finetuning/create` - Create fine-tuning job (requires OpenAI API key)
- `GET /api/finetuning/status/{job_id}` - Get job status

**Legend**: âœ… Fully working | ğŸ”§ Requires dependencies or API keys

---

## ğŸ¯ Model Support

### Supported Providers

- **OpenAI**: GPT-5, GPT-4o, GPT-4.1, O1, O3 series
- **Anthropic**: Claude 3.5, Claude 4, Claude 4.5 series
- **Google**: Gemini 1.5, Gemini 2.0, Gemini 2.5, Gemini 3.0 series
- **DeepSeek**: DeepSeek Chat, DeepSeek V3, DeepSeek R1 (API or Ollama)
- **Perplexity**: Sonar, Sonar Pro, Sonar Reasoning Pro
- **Ollama**: Local open-source models (Qwen, Llama, Phi, etc.)

### Smart Provider Detection

The backend automatically detects the best provider for each model:

1. **Registry Check**: Checks if model is registered in the model registry
2. **Ollama Check**: For open-source models, checks if installed locally in Ollama
3. **Pattern Detection**: Falls back to pattern-based detection

**Open-source models** (DeepSeek, Mistral, Gemma, etc.) are automatically checked in Ollama first. If found, they use the local Ollama provider (no API key needed). Otherwise, they fall back to API providers.

### Model Download

Ollama models can be downloaded directly from the UI:
- Click the download button next to any Ollama model
- Track download progress in real-time
- Cancel downloads if needed
- Downloads persist across page navigation

---

## ğŸ§  Think Mode

The backend supports thinking/reasoning mode for different model types:

### Native Thinking Models
- **Claude models**: Uses native `thinking` parameter
- **OpenAI reasoning models** (o1, o3, gpt-5): Automatic thinking mode
- **DeepSeek R1**: Native reasoning mode

### Prompt-Based Thinking
- **Other models** (including open-source like Qwen, Llama, Phi): Uses system prompt to encourage step-by-step reasoning

Enable thinking mode by toggling the Think Mode button in the UI or setting `enable_thinking: true` in the API request.

---

## ğŸ› ï¸ Development

### Backend Development

```bash
cd playground/backend

# Run with auto-reload
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Or simply
python main.py
```

### Frontend Development

```bash
cd playground/frontend

# Development server
pnpm dev

# Production build
pnpm build

# Start production server
pnpm start
```

### Environment Variables

**Backend** (environment variables or `.env`):
```bash
# Required for API providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DEEPSEEK_API_KEY=...
GEMINI_API_KEY=...
PERPLEXITY_API_KEY=...
```

**Frontend** (`.env.local`):
```bash
# Optional - defaults to http://localhost:8000
NEXT_PUBLIC_API_URL=http://localhost:8000
```

---

## ğŸ¨ UI Features

### Design System
- **KRDS Design System**: Government UI/UX design system
- **Pastel Colors**: Soft, modern color palette
- **Pretendard GOV Font**: Recommended Korean font
- **Accessibility**: ARIA attributes, keyboard navigation

### Components
- **Model Selector**: Easy model selection with download support
- **Settings Panel**: Dynamic model parameters with tooltips
- **Think Mode Toggle**: Visual reasoning process display
- **Onboarding Guide**: Interactive step-by-step guide
- **File Upload**: Drag-and-drop file upload for RAG

---

## ğŸ› Troubleshooting

### Backend Issues

**Problem**: `ModuleNotFoundError: No module named 'fastapi'`
**Solution**:
```bash
cd playground/backend
pip install -r requirements.txt
```

**Problem**: `OPENAI_API_KEY not set`
**Solution**: Set environment variables or use Ollama models (no API key needed)

**Problem**: Ollama models not found
**Solution**: 
1. Install Ollama: https://ollama.ai
2. Download models using the UI or `ollama pull <model-name>`

### Frontend Issues

**Problem**: `Cannot find module 'next'`
**Solution**:
```bash
cd playground/frontend
pnpm install
```

**Problem**: Build fails with type errors
**Solution**: Clear cache and rebuild
```bash
rm -rf .next
pnpm build
```

### Connection Issues

**Problem**: Frontend can't connect to backend
**Solution**:
1. Ensure backend is running on http://localhost:8000
2. Check CORS settings in `main.py`
3. Verify `NEXT_PUBLIC_API_URL` in `.env.local`

---

## ğŸ“š Documentation

### Interactive API Docs
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

### API Testing

**ì¢…í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼**: 7/11 í…ŒìŠ¤íŠ¸ í†µê³¼ (64%)

âœ… **ì™„ë²½ ì‘ë™** (7ê°œ):

- Document Loaders: Text, CSV, HTML, Markdown, JSON, PDF (6/6)
- PDF Multi-page ì§€ì›

âš ï¸ **ì˜ì¡´ì„± í•„ìš”** (4ê°œ):

- RAG File Upload (Ollama embedding ì„¤ì •)
- Vision RAG (ì•„í‚¤í…ì²˜ ìˆ˜ì • í•„ìš”)
- OCR (PaddleOCR ì„¤ì¹˜)
- Knowledge Graph Query (Ollama ëª¨ë¸ ì‚¬ìš©)

**í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**:

```bash
cd backend

# ìƒ˜í”Œ íŒŒì¼ ìƒì„±
python create_sample_files.py

# ë¬¸ì„œ ë¡œë” í…ŒìŠ¤íŠ¸
python test_document_loaders.py

# ì „ì²´ ê¸°ëŠ¥ ì¢…í•© í…ŒìŠ¤íŠ¸
python test_all_features_comprehensive.py
```

ìƒì„¸ ê²°ê³¼: `backend/TEST_SUMMARY.md`

---

## ğŸ”§ Technology Stack

### Backend
- **FastAPI** - Modern async Python web framework
- **Uvicorn** - ASGI server
- **Pydantic** - Request/response validation
- **WebSockets** - Real-time communication
- **beanllm** - Core LLM framework

### Frontend
- **Next.js 15** - React framework
- **TypeScript** - Type safety
- **Tailwind CSS** - Styling
- **shadcn/ui** - UI components
- **Radix UI** - Accessible components
- **Lucide React** - Icons
- **sonner** - Toast notifications

---

## ğŸ“ License

Same as beanllm framework.

---

## ğŸ¯ Next Steps

### Try It Out
1. Set up backend and frontend (see Quick Start above)
2. Open http://localhost:3000
3. Try all features
4. Download and test Ollama models

### Customize
1. Modify `backend/main.py` to add custom endpoints
2. Update frontend components in `frontend/src/components`
3. Enhance UI in `frontend/src/app`

### Deploy
1. Set up production environment variables
2. Build frontend: `cd frontend && pnpm build`
3. Deploy backend (Docker, Cloud Run, etc.)
4. Deploy frontend (Vercel, Netlify, etc.)

---

**ğŸ«˜ Built with beanllm - The unified LLM framework**
